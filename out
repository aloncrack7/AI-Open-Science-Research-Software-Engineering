GROBID server is up and running
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Juyoung Yun', given_name='Juyoung', middle_name=None, surname='Yun', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Byungkon Kang', given_name='Byungkon', middle_name=None, surname='Kang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Zhoulai Fu', given_name='Zhoulai', middle_name=None, surname='Fu', email=None, orcid=None, affiliation=None)], index=None, id=None, unstructured=None, date=None, title='The Hidden Power of Pure 16-bit Floating-Point Neural Networks', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), pdf_md5='1FE8B9B2AB3A6E109E5291F09127DA59', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='J Bjorck', given_name='J', middle_name=None, surname='Bjorck', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Chen', given_name='X', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C D Sa', given_name='C', middle_name='D', surname='Sa', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C P Gomes', given_name='C', middle_name='P', surname='Gomes', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Q Weinberger', given_name='K', middle_name='Q', surname='Weinberger', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date='2021', title='Low-precision reinforcement learning: Running soft actor-critic in half precision', book_title='Proceedings of The Hidden Power of Pure 16-bit Floating-Point Neural Networks International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='X Chen', given_name='X', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Hu', given_name='X', middle_name=None, surname='Hu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Zhou', given_name='H', middle_name=None, surname='Zhou', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Xu', given_name='N', middle_name=None, surname='Xu', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='2017', title='FxpNet: Training a deep convolutional neural network in fixed-point representation', book_title='Proceedings of the International Joint Conference on Neural Networks', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Courbariaux', given_name='M', middle_name=None, surname='Courbariaux', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J.-P David', given_name='J.-P', middle_name=None, surname='David', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date='2015', title='BinaryConnect: Training deep neural networks with binary weights during propagations', book_title='Proceedings of Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Das', given_name='D', middle_name=None, surname='Das', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Mellempudi', given_name='N', middle_name=None, surname='Mellempudi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Mudigere', given_name='D', middle_name=None, surname='Mudigere', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Kalamkar', given_name='D', middle_name=None, surname='Kalamkar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Avancha', given_name='S', middle_name=None, surname='Avancha', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Banerjee', given_name='K', middle_name=None, surname='Banerjee', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Sridharan', given_name='S', middle_name=None, surname='Sridharan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Vaidyanathan', given_name='K', middle_name=None, surname='Vaidyanathan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Kaul', given_name='B', middle_name=None, surname='Kaul', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Georganas', given_name='E', middle_name=None, surname='Georganas', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Heinecke', given_name='A', middle_name=None, surname='Heinecke', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Dubey', given_name='P', middle_name=None, surname='Dubey', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Corbal', given_name='J', middle_name=None, surname='Corbal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Shustrov', given_name='N', middle_name=None, surname='Shustrov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Dubtsov', given_name='R', middle_name=None, surname='Dubtsov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Fomenko', given_name='E', middle_name=None, surname='Fomenko', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Pirogov', given_name='V', middle_name=None, surname='Pirogov', email=None, orcid=None, affiliation=None)], index=3, id='b3', unstructured=None, date='2018', title='Mixed precision training of convolutional neural networks using integer operations', book_title='Proceedings of International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C De Sa', given_name='C', middle_name=None, surname='De Sa', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Feldman', given_name='M', middle_name=None, surname='Feldman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Ré', given_name='C', middle_name=None, surname='Ré', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Olukotun', given_name='K', middle_name=None, surname='Olukotun', email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='2017', title='Understanding and optimizing asynchronous low-precision stochastic gradient descent', book_title='Proceedings of International Symposium on Computer Architecture', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Dean', given_name='J', middle_name=None, surname='Dean', email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date='2020', title='The deep learning revolution and its implications for computer architecture and chip design', book_title='Proceedings of IEEE International Solid State Circuits Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='I Goodfellow', given_name='I', middle_name=None, surname='Goodfellow', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Courville', given_name='A', middle_name=None, surname='Courville', email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date='2016', title='Deep Learning', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='MIT Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='http://www.deeplearningbook.org'), GrobidBiblio(authors=[GrobidAuthor(full_name='S Gopinath', given_name='S', middle_name=None, surname='Gopinath', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Ghanathe', given_name='N', middle_name=None, surname='Ghanathe', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Seshadri', given_name='V', middle_name=None, surname='Seshadri', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Sharma', given_name='R', middle_name=None, surname='Sharma', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date='2019', title='Compiling KB-sized machine learning models to tiny IoT devices', book_title='Proceedings of Programming Language Design and Implementation', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Gupta', given_name='S', middle_name=None, surname='Gupta', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Agrawal', given_name='A', middle_name=None, surname='Agrawal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Gopalakrishnan', given_name='K', middle_name=None, surname='Gopalakrishnan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Narayanan', given_name='P', middle_name=None, surname='Narayanan', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date='2015', title='Deep learning with limited numerical precision', book_title='Proceedings of International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K He', given_name='K', middle_name=None, surname='He', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Zhang', given_name='X', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Ren', given_name='S', middle_name=None, surname='Ren', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Sun', given_name='J', middle_name=None, surname='Sun', email=None, orcid=None, affiliation=None)], index=9, id='b9', unstructured=None, date='2016', title='Identity mappings in deep residual networks', book_title='Proceedings of the European Conference on Computer Vision', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G Hinton', given_name='G', middle_name=None, surname='Hinton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Srivastava', given_name='N', middle_name=None, surname='Srivastava', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Swersky', given_name='K', middle_name=None, surname='Swersky', email=None, orcid=None, affiliation=None)], index=10, id='b10', unstructured=None, date=None, title='Neural networks for machine learning', book_title='lecture 6e', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='http://www.cs.toronto.edu/˜tijmen/csc321/slides/lecture_slides_lec6.pdf'), GrobidBiblio(authors=[], index=11, id='b11', unstructured=None, date='2008', title='Ieee standard for floating-point arithmetic', book_title=None, series_title=None, editors=None, journal='IEEE Std', journal_abbrev=None, publisher=None, institution='IEEE Computer Society', issn=None, eissn=None, volume='754', issue=None, pages='1-70', first_page='1', last_page='70', note=None, doi='10.1109/IEEESTD.2008.4610935', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='B Jacob', given_name='B', middle_name=None, surname='Jacob', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Kligys', given_name='S', middle_name=None, surname='Kligys', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Chen', given_name='B', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Zhu', given_name='M', middle_name=None, surname='Zhu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Tang', given_name='M', middle_name=None, surname='Tang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Howard', given_name='A', middle_name=None, surname='Howard', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Adam', given_name='H', middle_name=None, surname='Adam', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Kalenichenko', given_name='D', middle_name=None, surname='Kalenichenko', email=None, orcid=None, affiliation=None)], index=12, id='b12', unstructured=None, date='2017', title='Quantization and training of neural networks for efficient integerarithmetic-only inference', book_title='Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Kalamkar', given_name='D', middle_name=None, surname='Kalamkar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Mudigere', given_name='D', middle_name=None, surname='Mudigere', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Mellempudi', given_name='N', middle_name=None, surname='Mellempudi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Das', given_name='D', middle_name=None, surname='Das', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Banerjee', given_name='K', middle_name=None, surname='Banerjee', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Avancha', given_name='S', middle_name=None, surname='Avancha', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D T Vooturi', given_name='D', middle_name='T', surname='Vooturi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Jammalamadaka', given_name='N', middle_name=None, surname='Jammalamadaka', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Huang', given_name='J', middle_name=None, surname='Huang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Yuen', given_name='H', middle_name=None, surname='Yuen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Yang', given_name='J', middle_name=None, surname='Yang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Park', given_name='J', middle_name=None, surname='Park', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Heinecke', given_name='A', middle_name=None, surname='Heinecke', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Georganas', given_name='E', middle_name=None, surname='Georganas', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Srinivasan', given_name='S', middle_name=None, surname='Srinivasan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Kundu', given_name='A', middle_name=None, surname='Kundu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Smelyanskiy', given_name='M', middle_name=None, surname='Smelyanskiy', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Kaul', given_name='B', middle_name=None, surname='Kaul', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Dubey', given_name='P', middle_name=None, surname='Dubey', email=None, orcid=None, affiliation=None)], index=13, id='b13', unstructured=None, date='2019', title='A study of BFLOAT16 for deep learning training', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='https://arxiv.org/abs/1905.12322'), GrobidBiblio(authors=[GrobidAuthor(full_name='N S Keskar', given_name='N', middle_name='S', surname='Keskar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Mudigere', given_name='D', middle_name=None, surname='Mudigere', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Nocedal', given_name='J', middle_name=None, surname='Nocedal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Smelyanskiy', given_name='M', middle_name=None, surname='Smelyanskiy', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P T P Tang', given_name='P', middle_name='T P', surname='Tang', email=None, orcid=None, affiliation=None)], index=14, id='b14', unstructured=None, date='2017', title='On large-batch training for deep learning: Generalization gap and sharp minima', book_title='Proceedings of International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D P Kingma', given_name='D', middle_name='P', surname='Kingma', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J L Ba', given_name='J', middle_name='L', surname='Ba', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Adam', given_name=None, middle_name=None, surname='Adam', email=None, orcid=None, affiliation=None)], index=15, id='b15', unstructured=None, date='2015', title='A method for stochastic optimization', book_title='Proceedings of International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Krizhevsky', given_name='A', middle_name=None, surname='Krizhevsky', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Sutskever', given_name='I', middle_name=None, surname='Sutskever', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G E Hinton', given_name='G', middle_name='E', surname='Hinton', email=None, orcid=None, affiliation=None)], index=16, id='b16', unstructured=None, date='2012', title='ImageNet classification with deep convolutional neural networks', book_title='Proceedings of Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Kumar', given_name='A', middle_name=None, surname='Kumar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Seshadri', given_name='V', middle_name=None, surname='Seshadri', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Sharma', given_name='R', middle_name=None, surname='Sharma', email=None, orcid=None, affiliation=None)], index=17, id='b17', unstructured=None, date='2020', title='Shiftry: RNN inference in 2kb of RAM', book_title='Proc. ACM Program. Lang', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='4', issue=None, pages='30', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='U Köster', given_name='U', middle_name=None, surname='Köster', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T J Webb', given_name='T', middle_name='J', surname='Webb', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Wang', given_name='X', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Nassar', given_name='M', middle_name=None, surname='Nassar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A K Bansal', given_name='A', middle_name='K', surname='Bansal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W H Constable', given_name='W', middle_name='H', surname='Constable', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O H Elibol', given_name='O', middle_name='H', surname='Elibol', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Gray', given_name='S', middle_name=None, surname='Gray', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Hall', given_name='S', middle_name=None, surname='Hall', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Hornof', given_name='L', middle_name=None, surname='Hornof', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Khosrowshahi', given_name='A', middle_name=None, surname='Khosrowshahi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Kloss', given_name='C', middle_name=None, surname='Kloss', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R J Pai', given_name='R', middle_name='J', surname='Pai', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Rao', given_name='N', middle_name=None, surname='Rao', email=None, orcid=None, affiliation=None)], index=18, id='b18', unstructured=None, date='2017', title='Flexpoint: An adaptive numerical format for efficient training of deep neural networks', book_title='Proceedings of Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D D Lin', given_name='D', middle_name='D', surname='Lin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S S Talathi', given_name='S', middle_name='S', surname='Talathi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V S Annapureddy', given_name='V', middle_name='S', surname='Annapureddy', email=None, orcid=None, affiliation=None)], index=19, id='b19', unstructured=None, date='2016', title='Fixed point quantization of deep convolutional networks', book_title='Proceedings of the International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Micikevicius', given_name='P', middle_name=None, surname='Micikevicius', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Narang', given_name='S', middle_name=None, surname='Narang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Alben', given_name='J', middle_name=None, surname='Alben', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Diamos', given_name='G', middle_name=None, surname='Diamos', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Elsen', given_name='E', middle_name=None, surname='Elsen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Garcia', given_name='D', middle_name=None, surname='Garcia', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Ginsburg', given_name='B', middle_name=None, surname='Ginsburg', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Houston', given_name='M', middle_name=None, surname='Houston', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Kuchaiev', given_name='O', middle_name=None, surname='Kuchaiev', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Venkatesh', given_name='G', middle_name=None, surname='Venkatesh', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Wu', given_name='H', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None)], index=20, id='b20', unstructured=None, date='2018', title='Mixed precision training', book_title='Proceedings of International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Sharma', given_name='H', middle_name=None, surname='Sharma', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Park', given_name='J', middle_name=None, surname='Park', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Suda', given_name='N', middle_name=None, surname='Suda', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Lai', given_name='L', middle_name=None, surname='Lai', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Chau', given_name='B', middle_name=None, surname='Chau', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J K Kim', given_name='J', middle_name='K', surname='Kim', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Chandra', given_name='V', middle_name=None, surname='Chandra', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Esmaeilzadeh', given_name='H', middle_name=None, surname='Esmaeilzadeh', email=None, orcid=None, affiliation=None)], index=21, id='b21', unstructured=None, date='2017', title='Bit Fusion: Bitlevel dynamically composable architecture for accelerating deep neural networks', book_title='Proceedings of International Symposium on Computer Architecture', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Simonyan', given_name='K', middle_name=None, surname='Simonyan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Zisserman', given_name='A', middle_name=None, surname='Zisserman', email=None, orcid=None, affiliation=None)], index=22, id='b22', unstructured=None, date='2015', title='Very deep convolutional networks for large-scale image recognition', book_title='Proceedings of International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='The Hidden Power of Pure 16-bit Floating-Point Neural Networks', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Skeel', given_name='R', middle_name=None, surname='Skeel', email=None, orcid=None, affiliation=None)], index=23, id='b23', unstructured=None, date='1992', title='Roundoff error and the patriot missile', book_title=None, series_title=None, editors=None, journal='SIAM News', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue='4', pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Wang', given_name='N', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C.-Y Chen', given_name='C.-Y', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Gopalakrishnan', given_name='K', middle_name=None, surname='Gopalakrishnan', email=None, orcid=None, affiliation=None)], index=24, id='b24', unstructured=None, date='2019', title='Ultra-lowprecision training of deep neural networks', book_title='Proceedings of Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Wu', given_name='S', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Li', given_name='G', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Chen', given_name='F', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Shi', given_name='L', middle_name=None, surname='Shi', email=None, orcid=None, affiliation=None)], index=25, id='b25', unstructured=None, date='2018', title='Training and inference with integers in deep neural networks', book_title='Proceedings of International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G Xiao', given_name='G', middle_name=None, surname='Xiao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Lin', given_name='J', middle_name=None, surname='Lin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Seznec', given_name='M', middle_name=None, surname='Seznec', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Demouth', given_name='J', middle_name=None, surname='Demouth', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Han', given_name='S', middle_name=None, surname='Han', email=None, orcid=None, affiliation=None)], index=26, id='b26', unstructured=None, date='2022', title='SmoothQuant: Accurate and efficient post-training quantization for large language models', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='https://arxiv.org/abs/2211.10438')], abstract="Lowering the precision of neural networks from the prevalent 32-bit precision has long been considered harmful to performance, despite the gain in space and time. Many works propose various techniques to implement half-precision neural networks, but none study pure 16-bit settings. This paper investigates the unexpected performance gain of pure 16-bit neural networks over the 32bit networks in classification tasks. We present extensive experimental results that favorably compare various 16-bit neural networks' performance to those of the 32-bit models. In addition, a theoretical analysis of the efficiency of 16-bit models is provided, which is coupled with empirical evidence to back it up. Finally, we discuss situations in which low-precision training is indeed detrimental.", body='Introduction Today\'s ubiquitous need for neural network techniquesfrom autonomous vehicle driving, healthcare, and finance to general artificial intelligence and engineering -has become a faith of fact for many. Significant computing power can be necessary for training neural networks on real-world data, which has stimulated the semiconductor industry to pursue cutting-edge chips and GPU solutions for reduced-precision floating-point arithmetic. Reduced precision provides remarkable performance gain in speed, memory usage, and energy consumption over traditional CPU-based single and double-precision computing. Many GPUs are now powered with reduced-precision floating-point. In particular, widely accessible NVIDIA GPUs support the IEEE-standardized half-precision, namely, the 16-bit floating-point format. However, reduced precision alone is also known to cause 1 Department of Computer Science, Stony Brook University, NY, USA 2 Department of Computer Science, State University of New York Korea, Incheon, Republic of Korea. Correspondence to: Byungkon Kang <byungkon.kang@sunykorea.ac.kr>, Zhoulai Fu <zhoulai.fu@sunykorea.ac.kr>. accuracy loss. In the IEEE 16-bit format, positive numbers except subnormal ones lie between 5.96E-8 and 6.5E04, and thus a straightforward rounding or truncation of single or double precision data into half precision can cause overflow, underflow, or subnormal numbers, all of which affects numerical accuracy. Consider the sigmoid function, sigmoid (x) = 1/(1 + e −x ). It overflows if x is small and underflows if x is large, where both overflow and underflow occur in e −x , and the error will then be propagated to the function\'s output. As a quick experiment, one can set a vector x to be all the 16-bit floating-point numbers (there are 63487 in total) excluding ±∞ and NaN, and y be all the upcast 32bit float (y= np.float32(x)), and element-wise compare sigmoid (x) and sigmoid (y). They have a relative error of 4.85E-02 and an absolute error of 7.09E-05 on average. Errors of such scale can be consequential for general scientific computing, e.g., causing the failure of the Patriot missile system  (Skeel, 1992) . The question that would naturally arise for an ML practitioner would be: Are floating-point errors of such scale too significant for a reduced precision model in machine learning to make the right prediction? In the machine learning community, many believe that "deep learning models ... are "very tolerant of reduced-precision computations"  (Dean, 2020) . ML researchers have actively investigated a wide range of techniques that lower the precision of the floating point numbers used in neural networks but still maintain accuracy. For example,  Micikevicius et al. (2018)  propose a mixed precision technique, where weight, activation, and gradients are stored in 16 bits, but weight updates are carried out on 32 bits.  Kalamkar et al. (2019)  uses another mixed precision technique with 32-bit floatingpoint and BFloat, Google Brain\'s half-precision format, in which a tensor modification method is used to zero out the lower 16 bits of the 32-bit data flow. All these algorithms use mixed precision, where two or more precision are chosen from a small number of available precisions, typically half (16-bit) and single (32-bit). To the best of our knowledge, the pure 16-bit neural network is rarely used as a standalone solution in the machine learning community. This work targets the 16-bit pure neural network and studies whether we can use it out of the box, without parameter tuning or techniques commonly used in arXiv:2301.12809v1 [cs.LG] 30 Jan 2023 mixed precision algorithms like loss scaling. Our finding is positive. That is, pure 16-bit neural networks, without any floating-point 32 components, despite being imprecise by nature, can be precise enough to handle a major application of machine learning -the classification problem. Our work first formalizes the intuitive concept of error tolerance and proposes a lemma that theoretically guarantees 16-bit "locally" achieves the same classification result as 32-bit under certain conditions. Combined with our preliminary observations, we conjecture that the 16-bit and 32-bit have close results in handling classification problems. We then validate our conjecture through extensive experiments on Deep neural network (DNN) and Constitutional Neural Network (CNN) problems. Our contributions follow: • We aim to debunk the myth that plain 16-bit models do not work well. We demonstrate that training neural network models in pure 16 bits with no additional measures to "compensate" results in competitive, if not superior, accuracy. • We offer theoretical insights on why half precision models work well, as well as empirical evidence that supports our analysis. • We perform extensive experiments comparing the performance of various pure 16-bit neural networks against that of 32-bit and mixed precision networks and find that the 16-bit neural networks can perform as well as their 32-bit counterparts. We also identify factors that could negatively influence the success of 16-bit models. \n Related Work Several techniques have been proposed to reduce the precision of machine learning models while maintaining their accuracy to some degree. The approach that best aligns with our work is that of lowering the precision of the floating point numbers used in those models, but other approaches involve algorithm modification, fixed point formats, and hardware acceleration. Algorithm modification. These approaches aim to modify certain components of the main algorithm to allow lowprecision work. The work by  (De Sa et al., 2017 ) offers a way to address the issues associated with low precision in gradient descent by assigning different precisions to the weights and the gradients. Such a scheme is assisted by special hardware implementation to speed up this mixedprecision process. A similar approach is taken by  (Bjorck et al., 2021)  in reinforcement learning. This work proposes various mechanisms to perform reinforcement learning on low precision reliably. For example, adopting numerical methods to improve the Adam optimization algorithm so that underflow can be prevented. Many of such techniques proposed are somewhat ad-hoc to the target problem but might also be useful in a more general setting. Fixed point formats. A fixed point format is another number format often used to represent real numbers, although not standard. Several other works have adopted fixed point formats to allow low precision due to the intuitive representation and high extensibility to other configurations. To verify the issues with low precision in deep learning, the authors of  (Gupta et al., 2015)  investigate the behavior of deep neural networks when the precision is lowered. In this work, the empirical results show that reducing the precision of the real numbers comprising the weights of the network shows a graceful degradation of accuracy.  (Chen et al., 2017)  performs neural network training in fixed point numbers by quantizing a floating point number into a low-precision fixed point number during training. The idea of quantizing floating point is also used in  (Lin et al., 2016) , where the authors approach the conversion as an optimization problem. The objective of the optimization is to reduce the network\'s size by adopting different bit-width for each layer. On a more system-based approach,  (Kumar et al., 2020)  proposes compiler support for converting floating point numbers to low-precision fixed point numbers.  (Gopinath et al., 2019)  also takes compiler-and language-based support for achieving low-precision numbers. Although not entirely the same as real number fixed point, integer quantization can also be considered a special fixed point format and has become a viable choice in lowprecision neural network design. In  (Wu et al., 2018) , floating point weights are quantized to convert them into signed integer representations. The authors discover that adopting an integral quantization technique results in a regularizationlike behavior, leading to increased accuracy. Similar to this approach, work by  (Das et al., 2018 ) also proposes to use integer operations to achieve high accuracy. Such integer operations effectively convert the floating point numbers into what is known as dynamic fixed point (DFP) format. The work in  (Jacob et al., 2017 ) also aims to quantize the weights to integers. However, the weights remain integers only during the forward pass and become floating point numbers for back-propagation to account for minute updates. Another work  (Courbariaux et al., 2015)  takes an extreme quantization approach to binarize the weights to -1 and 1. The weights remain binary during the forward and backward pass but become floating points during the weight update phase.  (Xiao et al., 2022)  adopts a post-training quantization approach to reduce the memory footprint of large-scale language models.  (Köster et al., 2017)  proposes an adaptive numerical format that retains the advantages of floating and fixed point numbers. This is achieved by having a shared exponent that gets updated during training. Mixed precision. On the other hand, mixed precision ap-proaches maintain standard floating point numbers in two different precision. The first practically successful reducedprecision floating point mechanism was proposed by  (Micikevicius et al., 2018) . In this work, the authors devise a scheme to perform mixed precision training by maintaining a set of master full-precision floating point weights that serves as the \'original copy\' of the half-precision counterparts. While this technique does result in reduced running time, the mixed-precision nature of it limits the performance gain achieved.  (Wang et al., 2019)  propose a 4-bit floating point architecture mixed with a small amount of 8-bit precision. In addition to the format, the authors devise a two-phase rounding procedure to counter the low accuracy induced by the low-precision format. Hardware support. Lastly, we would like to point out that many of these works either implicitly or explicitly require hardware support due to the individual floating/fixed point formats. Most approaches typically use FPGAs and FPUs to implement these formats, but other works such as  (Sharma et al., 2017)  propose novel architectures tailored to addressing the bit formats of the floating point numbers. Some previously mentioned works, such as  (Wang et al., 2019) , also hint at the possibility of leveraging hardware assistance. Unlike these previous works, our work focuses explicitly on the IEEE floating point format, which is the de-facto standard for general computing machinery. More precisely, we investigate the pros and cons of using a pure 16-bit IEEE floating point format in training neural networks without external support. \n Background General Notation The real numbers and integers are denoted by R and Z, respectively. Given a vector x = (x 0 , ..., x n−1 ) ∈ R n , its infinity norm or maximum norm:, denoted by |x| ∞ , is the maximum element in the vector, namely |x| ∞ def = max i |x i |. \n Floating-Point Representation We write F 16 to denote the set of 16-bit floating-point numbers excluding ±∞ and NaN (Not-a-Number). Following IEEE-754 standard (IEEE Computer Society, 2008), each x ∈ F 16 can be written as x = (−1) s × g 0 .g 1 ...g 10 (2) × 2 e (1) where s ∈ {0, 1}, g i ∈ {0, 1} (0 ≤ i ≤ 10), and e ∈ Z. We call s, g 0 .g 1 ....g 10 and e the sign, the significand, and the exponent, respectively. They satisfy g 0 = 0 and −14 ≤ e ≤ 15. The case where g 0 = 0 and e = −14 is called a subnormal number. We write F 32 for the set of 32-bit (single-precision) floating-point numbers excluding ±∞ and NaN. The following property holds: F 16 ⊂ F 32 ⊂ R (2) Namely, a 16-bit or 32-bit floating-point number is a real, and a 16-bit can be exactly represented as a 32-bit (by padding with zeros). Tab. 3.1 lists the range and the precision of 16-bit and 32-bit floating-point numbers. = argmin y∈Fp |x − y|. 1  Rounding error is usually small, on the order of machine epsilon (Tab. 3.1), but it can be propagated and become more significant. For example, the floating-point code sin(0.1) goes through three approximations. First, 0.1 is rounded to the floating-point • p (0.1) for some precision p. Then, the rounding error is propagated by the floating-point code sin. Lastly, the calculation output is rounded again if an exact representation is not possible. Floating-point errors are usually measured in absolute error or relative error. This paper focuses on classification problems where output numbers are probabilities between 0 and 1. Thus, we use the absolute error |x − y| for quantifying the difference between two floating point numbers x and y. \n Theory Suppose M 16 and M 32 are 16-bit and 32-bit deep learning models trained by the same neural network architecture and hyper-parameters. By abuse of notation, we consider M 16 and M 32 as classifiers or functions that return the probability vector from the last layer given an input x (e.g., an image). Let x be an arbitrarily chosen input. Suppose M 32 (x) returns (p 0 , • • • , p N −1 ), and M 16 (x) returns (p 0 , • • • , p N −1 ). Clearly, the classification result of an input x made by a clas-  def = argmax i {p i |p i ∈ M (x)}. (3) Due to the floating-point error, the classification results of M 32 and M 16 on x can be different. To quantify this difference, we define the floating point error as follows. Definition 4.1. Given the 16-bit classifer M 16 , the 32-bit M 32 , and an input x, the floating point error between the classifiers is given as δ(M 32 , M 16 , x) def = |M 32 (x) − M 16 (x)| ∞ (4) The degree to which this difference affects the outcome is an important question we investigate in this work. In fact, we can show a sufficient condition (denoted by C) that guarantees the absence of difference between two classifiers. (C.) If the difference between the largest of p i and the second largest is greater than twice the floating point error, then the two classifiers M 16 and M 32 have the same classification result on x. Illustration for condition (C): suppose M 32 (x) = (0.8, 0.1, 0.05, 0.05) for a classification problem of four labels. Let the largest error between this probability vector and M 16 (x) be δ. Then in the worst case, 0.8 can drop to 0.8 − δ for the 16-bit, and the second largest probability becomes 0.1 + δ. If 0.8 − δ > 0.1 + δ, then M 16 and M 32 must have the same classification result on x, e.g., M 16 (x) = (0.7, 0.15, 0.1, 0.05). Below, we formalize condition (C) following introduction of the notion of error tolerance. Definition 4.2. The error tolerance of a classifier M with respect to an input x is defined as the gap between the largest probability and the second-largest one: Γ(M, x) def = p 0 − p 1 , (5) where p 0 = |M (x))| ∞ , and p 1 = |M (x)\\p 0 | ∞ . Here, M 32 (x)\\p 0 refers to a vector of elements in M 32 (x) but with p 0 removed. The error tolerance can be thought of as quantifying the stability of the prediction. We have the following lemma corresponding to condition C. Lemma 4.3. Consider a classification problem characterized by a pair (X, Y ) where X is the space of input data, and Y = {0, ..N − 1} is the labels of classification. Suppose a learning algorithm trains a 32-bit model M 32 : X → F N 32 and a 16-bit model M 16 : X → F N 16 on a dataset D ⊆ X × Y . We have: If Γ(M 32 , x) ≥ 2δ(M 32 , M 16 , x) (6) then pred(M 32 , x) = pred(M 16 , x). Proof. Let M 32 (x) be (p 0 , ....p N −1 ). Without loss of generality we assume p 0 is the largest one in {p i } (0 ≤ i ≤ N − 1). We denote δ(M 32 , M 16 , x) by δ hereafter. Following Eq. 6, we have ∀i ∈ {1, ..., N − 1}, p 0 − p i ≥ 2δ. Let M 16 (x) be (p 0 , ....p N −1 ). Then for each i ∈ {1, ..., N − 1}, we have p 0 ≥ p 0 − δ By Eq. 4 and Def. of p 0 , p 0 ≥ p i + δ By Eq. 5 and Eq. 7 ≥ p i By Eq. 4 Thus p 0 remains the largest in the elements of M 16 (x). Below we illustrate Lemma 4.3 through a simple neural network trained on MNIST. Our 32-bit implementation has three Dense layers followed by a softmax layer at the end. Our 16-bit implementation uses the same architecture, except all floating-point operations are performed on 16-bit. Table  4  shows our results of error tolerance Γ and floatingpoint error δ. Observe that the mean floating-point error is of the magnitude of 1E-3 with a variance of 1E-5 or 1E-4; the error tolerance is 1E-1 with a variance of 1E-2. Thus, one can argue that Eq. 6, namely, Γ > 2δ, holds for most data in MNIST. The table also shows that floating-point errors can be larger than the tolerance in some corner cases. Thus, we expect our 16-bit and 32-bit implementations to have close but different accuracy results. Results at training and testing are presented. Table  4  shows the accuracy and loss results in MNIST comparing 16-bit and 32-bit implementations. We can see consistently that the 16-bit results have accuracy close to those of the 32-bit models, sometimes even better. This result motivates us to study whether the 16-bit model is similar to the 32-bit model for more complex neural networks. In theory, if 80% of data in a dataset satisfy Eq. 6, Lemma 4.3 tells us that the 32-bit and 16-bit models will have at least 80% of classification results being the same. The main challenge here is that we cannot determine if Eq. 6 always holds or the percentage of data that satisfies it. We believe that for complex neural networks, most data meet Eq. 6. This is because the loss function for the classification problem is a cross-entropy in the form of −Σ log(p i ), which should guide p i toward 1 during training and in turn, causes a large error tolerance Γ compared to relatively small δ. In fact, Table  4  shows that the floating point errors (δ\'s) are nearly two orders of magnitude smaller than the Γ\'s. Although the gap might close over the epochs, the difference remains sufficiently large to satisfy the condition of the lemma. With this theoretical development and observations, we pro-pose the following conjecture. The accuracy of a 16-bit neural network for classification problems, in the absence of significant errors involving floating-point overflow/underflow, will be close to that of a 32-bit neural network. We anticipate a situation where floating-point errors can become significant due to overflow or underflow since 16bit floating-point is known to have a smaller range (Table  3 .1). The conjecture may be surprising, so we devote our next section to in-depth validation. \n Experiments We aim to compare the performance of 16-bit operations to 32-bit operations in deep neural network (DNN) 2  and convolutional neural network (CNN) models. we experiment over three CNN models, including AlexNet  (Krizhevsky et al., 2012) , VGG16  (Simonyan & Zisserman, 2015) , and ResNet-34  (He et al., 2016) . We will see how the different precision settings affect the computational time and accuracy of the models. Unlike case studies in the previous section, we use 100 epochs for all experiments and gradually increase the batch size from 64 to 384 to see how it affects 16-bit training. All random seeds used in this study\'s experiments are fixed to facilitate the comparison. The experiments were conducted on NVIDIA\'s RTX3080 Laptop GPU. Table  4  gives the result most representative of our work. A more detailed description and analysis of these results will follow in the subsequent subsections.   \n DNN Experiments In the DNN experiments, we train a DNN with three hidden layers to perform the MNIST classification task and compare the performances between those of the 16-bit model and the 32-bit model. Each of the three layers in the DNN has 4096 neurons, whose outputs are fed to the next layer after passing through ReLU activation. Other works, such as  (Micikevicius et al., 2018) , use a 32-bit softmax layer regardless of the overall precision settings to prevent potential numerical instability, but we leave stick with a 16-bit softmax to see how a "pure" 16-bit model fares against 32-bit ones. We also vary the types of optimizers among RMSProp, Adam, and SGD to examine their effects on performance. In addition, we confirm from this experiment that not only SGD but also other optimizers, such as RMSProp and Adam, can be used in 16-bit if is properly set. In all experiments in this section, we use the learning rate of 10 −3 and fix in RMSProp and Adam to 10 −3 as well. We direct readers to the Appendix for experiments in other settings. Finally, in addition to the 32-bit baseline, we also compare the mixed-precision training algorithm proposed by  (Micikevicius et al., 2018)  (as provided by TensorFlow). Figure  1  shows that 16-bit deep neural networks are better than 32-bit and mixed precision in terms of computational time while maintaining similar test accuracy. In detail, (see Figure  3  in the Appendix) 16-bit SGD\'s computational time is decreased by 40.9%, and the test accuracy was increased 0.6%, respectively, compared to 32-bit while 16-bit computational time was decreased by 59.8% and accuracy was increased 0.59% compared to mixed precision when the batch size was the smallest at 64. Other batch sizes yielded consistent trends, albeit with smaller magnitude  (Keskar et al., 2017) . Optimization-wise, 16-bit RMSProp reduced the runtime by 40.4% and 59.6% compared to 32-bit and mixed, respectively, and accuracy is increased by 0.3% and 0.4%. 16-bit Adam improved the runtime by 41.6% and 58.1%, and 6.4% and 7.5% in terms of accuracy compared to 32bit and mixed precision, respectively. Of the total of 33 experimental groups calculated with three optimizers, 31 decreased running time by more than 40% compared to 32 bits while maintaining the accuracy to a similar level. Every computational time and test accuracy in 16-bit deep neural networks is better than those of 32-bit and mixed precision. \n CNN Experiments 16-bit CNN experiments were conducted to determine whether 16-bit is enough for training a more complex image classification problem and how numerically different it is from 32-bit. We used the CIFAR10 dataset and three convolutional neural networks (CNN) models: AlexNet, VGG16, and ResNet-34. All of these experiments were carried out using Adam in a 16-bit environment. Since the batch normalization (BN) layer is not implemented as an off-the-shelf module in 16 bits, the experiment was conducted focusing on CNN models that could be used without the batch normalization layer. See the Appendix for treatment on 16-bit BN implementations. \n TRAINING TIME AND ACCUARCY Figure  1  shows that 16-bit CNNs are better than 32-bit and mixed precision in terms of computational time and test accuracy. Figure  2  shows that 16-bit operations can also be applied to CNN models for image classification. We found that 16-bit CNNs maintain similar accuracy to 32-bit. This experiment shows that by using 16-bit operations in image classification with CNN models, the running time (whether training or testing) can be greatly reduced while maintaining similar or higher accuracy compared to 32-bit operations. Our results demonstrate the efficiency of neural networks in training CNN models in a low-precision setting. \n MODEL SIZE The preservation of the trained model weights is an important aspect of contemporary deep learning.   5  shows the stored model has about half the size of the 32-bit size, faithfully reflecting the half-precision size reduction. 16-bit neural network allows for reducing the size of the model by half while maintaining similar accuracy. This opens up possibilities for 16-bit models to afford more complex and accurate architectures to attain better results. \n Limitation and Discussion This subsection reports the limitations we find while using the 16-bit neural network. Light hyperparameter-tuning. During our experiments, we do not need to tune hyperparameters of 16-bit neural network training, e.g. learning rates, except the epsilon in the settings of the optimizers. The SGD  (Goodfellow et al., 2016)  optimizer does not have epsilon, so it can be used directly. For the other optimizers we have tested in our experiments, RMSProp  (Hinton et al.)  and Adam  (Kingma & Ba., 2015) , we will need to change epsilon, whose default value (in Tensorflow), 1E-7, can easily trigger significant inaccuracy for 16-bit training. As a detail, the parameter epsilon corresponds to in the weight updates below: RMSProp : w t =w t−1 − η g t √ v t + ADAM : w t =w t−1 − η mt √ vt + These optimizers introduced in to enhance numerical stability, but = 1E-7 in the denominator causes floating-point overflow when v t in RMSProp or vt in Adam are close to 0. As mentioned previously, we set = 1E-3 for 16-bit training. Missing 16-bit Batch normalization. Tensorflow\'s current batch normalization layer does not directly support pure 16-bit operations. The batch normalization layer presumably originates from mixed precision works, which would cast 16-bit input values to 32-bit for calculation and then downcast to 16 bits. This type conversion only results in runtime overhead due to the intermediate 32-bit computation. Since this work aims to report results on pure 16-bit neural networks, we have to implement a 16-bit batch normalization layer on our own. Our implementation of the 16bit batch normalization can be found in Appendix. Batch size. Our CNN experiments show that the accuracy of 16-bit neural networks decreases in larger batch sizes. This can be due to the precision loss incurred when averaging the cost over a larger number of samples in the mini-batches. This is probably a minor limitation since neural networks in memory-constrained environments usually do not use large batch sizes. Despite these limitations, we have confirmed that 16-bit neural networks learn as well as 32-bit with only minor fine-tuning ( in the optimizers). Thus, we believe ML practitioners can readily benefit from 16NN when it comes to solving op optimization problems since it is faster, less memory-consuming, yet achieves similar accuracy as the 32-bit. Instead of spending the same amount of time and space as 32-bit, we can form a network with an enhanced cost-to-benefit ratio. \n Conclusion In this work, we have shown that for classification problems, the 16-bit floating-point neural network can have an accuracy close to the 32-bit. We have proposed a conjecture on their accuracy closeness and have validated it theoretically and empirically. Our experiments also suggest a small amount of accuracy gain, possibly due to the regularizing effect of lowering the precision. These findings show that it is much safer and more efficient to use 16-bit precision than what is commonly perceived: the runtime and memory consumption is lowered significantly with little or no loss in accuracy. In the future, we plan to expand our work to verify similar characteristics in other types of architectures and problems such as generative models and regression. \n A. Extra experiments A.1. Top-1 and time results As stated in the main text, the CNN results we provide in Section 5 are based on architectures without the batch normalization (BN) layers. Figure  4  shows that 16-bit and 32-bit CNN models\' performances without BN layers. The main reason for excluding those layers is that there are no off-the-shelf 16-bit implementations for BN layers. As a remedy, we implemented those manually and present the results. We chose to move this result from the main text to here because we were not able to fully verify that the implementation is truly a pure 16-bit one. That is, although we take every possible precaution to adhere to 16-bit computation on the Python level, we do not know if any unprecedented upcasting occurs in the library level (e.g., libcuda or cublas). Hence, the results in Figure  5  are gathered from 16-bit neural networks \'to the best of our knowledge\'. From the figures, we can see that incorporating 16-bit BN layers still results in a similar pattern as the one given in the main text. With the exception of AlexNet, 16-bit networks give similar to superior performance compared to the 32-bit models.  Figure 1 . 1 Figure 1. Top-1 accuracy (top row) and computational time (bottom row) on MNIST Classification using DNN \n Figure 3 . 3 Figure 3. MNIST classification top-1 accuracy and computational time \n Figure 4 .Figure 5 . 45 Figure 4. CIFAR-10 classification top-1 and top-2 accuracy and computational time without BN Layers \n \n \n Table 1 . 1 Some characteristics of the 16-bit and 32-bit floating-point formats. Type Size Range Machine-epsilon Half 16 bits 6.55E±4 4.88E-04 Single 32 bits 3.4E±38 5.96E-08 3.2. Floating-point ErrorsRounding is necessary when representing real numbers that cannot be written as Eq. 1. Rounding error of a real x at precision p refers to |x − • p (x)| where the rounding operation, • p : R → F p , defines the nearest floating-point number of x. Namely, • p (x) def \n Table 2 . 2 The columns of "Floating-point errors" and "Error tolerance" refer to statistics of δ(M32, M16, x) and Γ(M 32, x) respectively, where x ranges over the images of the MNIST dataset. Floating-point error Error tolerance Epochs Min Max Mean Variance Min Max Mean Variance 10 0.00E+00 1.68E-01 2.76E-03 5.62E-05 9.29E-05 1.00E+00 7.66E-01 7.58E-02 20 7.84E-15 2.07E-01 2.88E-03 8.53E-05 3.65E-05 1.00E+00 8.24E-01 6.31E-02 50 0.00E+00 3.82E-01 3.69E-03 1.97E-04 2.74E-06 1.00E+00 8.79E-01 4.72E-02 100 0.00E+00 5.64E-01 4.12E-03 3.27E-04 3.24E-04 1.00E+00 9.16E-01 3.42E-02 200 0.00E+00 6.75E-01 4.23E-03 4.76E-04 1.93E-04 1.00E+00 9.47E-01 2.19E-02 500 0.00E+00 9.35E-01 3.76E-03 6.18E-04 1.44E-03 1.00E+00 9.79E-01 7.72E-03 1000 0.00E+00 9.95E-01 3.14E-03 5.77E-04 1.91E-03 1.00E+00 9.91E-01 2.98E-03 sifier M is given as pred(M, x) \n Table 3 . 3 Comparing accuracy and loss results between 32-bit and 16-bit neural networks on the MNIST dataset. Train accuracy Test accuracy Train loss Test loss Epochs 32-bit 16-bit 32-bit 16-bit 32-bit 16-bit 32-bit 16-bit 10 90.3% 90.0% 90.8% 91.0% 3.49E-01 3.69E-01 3.23E-01 3.39E-01 20 92.2% 92.3% 92.8% 92.8% 2.71E-01 2.94E-01 2.57E-01 2.74E-01 50 94.9% 95.3% 94.9% 94.7% 1.81E-01 2.10E-01 1.80E-01 2.04E-01 100 96.7% 96.4% 96.3% 95.8% 1.16E-01 1.49E-01 1.28E-01 1.52E-01 200 98.4% 97.3% 97.4% 97.3% 6.05E-02 9.32E-02 8.97E-02 1.10E-01 500 99.8% 99.1% 97.7% 98.1% 1.38E-02 4.00E-02 7.87E-02 8.54E-02 1000 100.0% 99.8% 97.8% 98.1% 2.97E-03 2.04E-02 9.02E-02 8.29E-02 \n Table 4 . 4 Summary of the time and accuracy performances of the three CNNs. Model Results FP32 FP16 AlexNet Time 381s 270s Accuracy 68.9% 69.5% VGG16 Time 1445s 812s Accuracy 82.9% 84.3% ResNet-34 Time 1914s 1058s Accuracy 76.6% 76.1% \n Table 5 . 5 Sizes of saved CNN models in 16-and 32-bit. Model FP16 FP32 AlexNet 41.99 MB 83.94 MB VGG16 67.34 MB 134.61 MB ResNet-34 171.99 MB 343.87 MB It is useful for model training and storage if a model with similar accuracy has less storage size. Table \n\t\t\t For simplicity, this definition of the rounding operation ignores the case where a tie needs to be broken. \n\t\t\t While DNNs subsume CNNs, we use the term DNN to refer to fully-connected, non-convolutional neural networks.', acknowledgement=None, annex=None)
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Maxime Bouton', given_name='Maxime', middle_name=None, surname='Bouton', email='maxime.bouton@ericsson.com', orcid=None, affiliation=None), GrobidAuthor(full_name='Jaeseong Jeong', given_name='Jaeseong', middle_name=None, surname='Jeong', email='jaeseong.jeong@ericsson.com', orcid=None, affiliation=None), GrobidAuthor(full_name='Jose Outes', given_name='Jose', middle_name=None, surname='Outes', email='jose.outes@ericsson.com', orcid=None, affiliation=None), GrobidAuthor(full_name='Adriano Mendo', given_name='Adriano', middle_name=None, surname='Mendo', email='adriano.mendo@ericsson.com', orcid=None, affiliation=None), GrobidAuthor(full_name='Alexandros Nikou', given_name='Alexandros', middle_name=None, surname='Nikou', email='alexandros.nikou@ericsson.com', orcid=None, affiliation=None)], index=None, id=None, unstructured=None, date=None, title='Multi-agent Reinforcement Learning with Graph Q-Networks for Antenna Tuning', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), pdf_md5='6759706962860387DE44D2D59F7F979A', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='S S Mwanje', given_name='S', middle_name='S', surname='Mwanje', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Decarreau', given_name='G', middle_name=None, surname='Decarreau', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Mannweiler', given_name='C', middle_name=None, surname='Mannweiler', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M N Islam', given_name='M', middle_name='N', surname='Islam', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Schmelz', given_name='L', middle_name=None, surname='Schmelz', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date='2016', title='Network management automation in 5g: Challenges and opportunities', book_title='IEEE International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='F Vannella', given_name='F', middle_name=None, surname='Vannella', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Jeong', given_name='J', middle_name=None, surname='Jeong', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Proutiere', given_name='A', middle_name=None, surname='Proutiere', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='2022', title='Off-policy learning in contextual bandits for remote electrical tilt optimization', book_title=None, series_title=None, editors=None, journal='IEEE Transactions on Vehicular Technology', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V Yajnanarayana', given_name='V', middle_name=None, surname='Yajnanarayana', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Rydén', given_name='H', middle_name=None, surname='Rydén', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Hévizi', given_name='L', middle_name=None, surname='Hévizi', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date='2020', title='5g handover using reinforcement learning', book_title='IEEE 5G World Forum', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='E Ghadimi', given_name='E', middle_name=None, surname='Ghadimi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F D Calabrese', given_name='F', middle_name='D', surname='Calabrese', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Peters', given_name='G', middle_name=None, surname='Peters', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Soldati', given_name='P', middle_name=None, surname='Soldati', email=None, orcid=None, affiliation=None)], index=3, id='b3', unstructured=None, date='2017', title='A reinforcement learning approach to power control and rate adaptation in cellular networks', book_title='IEEE International Conference on Communications (ICC)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Sunehag', given_name='P', middle_name=None, surname='Sunehag', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Lever', given_name='G', middle_name=None, surname='Lever', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Gruslys', given_name='A', middle_name=None, surname='Gruslys', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W M Czarnecki', given_name='W', middle_name='M', surname='Czarnecki', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V F Zambaldi', given_name='V', middle_name='F', surname='Zambaldi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Jaderberg', given_name='M', middle_name=None, surname='Jaderberg', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Lanctot', given_name='M', middle_name=None, surname='Lanctot', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Sonnerat', given_name='N', middle_name=None, surname='Sonnerat', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Z Leibo', given_name='J', middle_name='Z', surname='Leibo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Tuyls', given_name='K', middle_name=None, surname='Tuyls', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Graepel', given_name='T', middle_name=None, surname='Graepel', email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='2018', title='Value-decomposition networks for cooperative multi-agent learning based on team reward', book_title='Autonomous Agents and Multiagent Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Rashid', given_name='T', middle_name=None, surname='Rashid', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Samvelyan', given_name='M', middle_name=None, surname='Samvelyan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C S De Witt', given_name='C', middle_name='S', surname='De Witt', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Farquhar', given_name='G', middle_name=None, surname='Farquhar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J N Foerster', given_name='J', middle_name='N', surname='Foerster', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Whiteson', given_name='S', middle_name=None, surname='Whiteson', email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date='2018', title='QMIX: monotonic value function factorisation for deep multi-agent reinforcement learning', book_title='International Conference on Machine Learning (ICML)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='E Balevi', given_name='E', middle_name=None, surname='Balevi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J G Andrews', given_name='J', middle_name='G', surname='Andrews', email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date='2019', title='Online antenna tuning in heterogeneous cellular networks with deep reinforcement learning', book_title=None, series_title=None, editors=None, journal='IEEE Transactions on Cognitive Communications and Networking', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='5', issue='4', pages='1113-1124', first_page='1113', last_page='1124', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Farooq', given_name='H', middle_name=None, surname='Farooq', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Imran', given_name='A', middle_name=None, surname='Imran', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Jaber', given_name='M', middle_name=None, surname='Jaber', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date='2019', title='AI empowered smart user association in LTE relays hetnets', book_title='IEEE International Conference on Communications Workshops', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='F Vannella', given_name='F', middle_name=None, surname='Vannella', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Iakovidis', given_name='G', middle_name=None, surname='Iakovidis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E A Hakim', given_name='E', middle_name='A', surname='Hakim', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Aumayr', given_name='E', middle_name=None, surname='Aumayr', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Feghhi', given_name='S', middle_name=None, surname='Feghhi', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date='2021', title='Remote electrical tilt optimization via safe reinforcement learning', book_title='IEEE Wireless Communications and Networking Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Y Jin', given_name='Y', middle_name=None, surname='Jin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Vannella', given_name='F', middle_name=None, surname='Vannella', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Bouton', given_name='M', middle_name=None, surname='Bouton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Jeong', given_name='J', middle_name=None, surname='Jeong', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E A Hakim', given_name='E', middle_name='A', surname='Hakim', email=None, orcid=None, affiliation=None)], index=9, id='b9', unstructured=None, date='2022', title='A graph attention learning approach to antenna tilt optimization', book_title='International Conference on 6G Networking (6GNet)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Bouton', given_name='M', middle_name=None, surname='Bouton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Farooq', given_name='H', middle_name=None, surname='Farooq', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Forgeat', given_name='J', middle_name=None, surname='Forgeat', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Bothe', given_name='S', middle_name=None, surname='Bothe', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Shirazipour', given_name='M', middle_name=None, surname='Shirazipour', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Karlsson', given_name='P', middle_name=None, surname='Karlsson', email=None, orcid=None, affiliation=None)], index=10, id='b10', unstructured=None, date='2021', title='Coordinated reinforcement learning for optimizing mobile networks', book_title='NeurIPS Workshop on cooperative AI', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2109.15175', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M J Kochenderfer', given_name='M', middle_name='J', surname='Kochenderfer', email=None, orcid=None, affiliation=None)], index=11, id='b11', unstructured=None, date='2015', title='Decision making under uncertainty: Theory and application', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='MIT Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V Mnih', given_name='V', middle_name=None, surname='Mnih', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Kavukcuoglu', given_name='K', middle_name=None, surname='Kavukcuoglu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Silver', given_name='D', middle_name=None, surname='Silver', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A A Rusu', given_name='A', middle_name='A', surname='Rusu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Veness', given_name='J', middle_name=None, surname='Veness', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M G Bellemare', given_name='M', middle_name='G', surname='Bellemare', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Graves', given_name='A', middle_name=None, surname='Graves', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M A Riedmiller', given_name='M', middle_name='A', surname='Riedmiller', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Fidjeland', given_name='A', middle_name=None, surname='Fidjeland', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Ostrovski', given_name='G', middle_name=None, surname='Ostrovski', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Petersen', given_name='S', middle_name=None, surname='Petersen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Beattie', given_name='C', middle_name=None, surname='Beattie', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Sadik', given_name='A', middle_name=None, surname='Sadik', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Antonoglou', given_name='I', middle_name=None, surname='Antonoglou', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H King', given_name='H', middle_name=None, surname='King', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Kumaran', given_name='D', middle_name=None, surname='Kumaran', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Wierstra', given_name='D', middle_name=None, surname='Wierstra', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Legg', given_name='S', middle_name=None, surname='Legg', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Hassabis', given_name='D', middle_name=None, surname='Hassabis', email=None, orcid=None, affiliation=None)], index=12, id='b12', unstructured=None, date='2015', title='Human-level control through deep reinforcement learning', book_title=None, series_title=None, editors=None, journal='Nature', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='518', issue='7540', pages='529-533', first_page='529', last_page='533', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C Morris', given_name='C', middle_name=None, surname='Morris', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Ritzert', given_name='M', middle_name=None, surname='Ritzert', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Fey', given_name='M', middle_name=None, surname='Fey', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W L Hamilton', given_name='W', middle_name='L', surname='Hamilton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J E Lenssen', given_name='J', middle_name='E', surname='Lenssen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Rattan', given_name='G', middle_name=None, surname='Rattan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Grohe', given_name='M', middle_name=None, surname='Grohe', email=None, orcid=None, affiliation=None)], index=13, id='b13', unstructured=None, date='2019', title='Weisfeiler and leman go neural: Higher-order graph neural networks', book_title='AAAI Conference on Artificial Intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T N Kipf', given_name='T', middle_name='N', surname='Kipf', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Welling', given_name='M', middle_name=None, surname='Welling', email=None, orcid=None, affiliation=None)], index=14, id='b14', unstructured=None, date='2017', title='Semi-supervised classification with graph convolutional networks', book_title='International Conference on Learning Representations (ICLR)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Velickovic', given_name='P', middle_name=None, surname='Velickovic', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Cucurull', given_name='G', middle_name=None, surname='Cucurull', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Casanova', given_name='A', middle_name=None, surname='Casanova', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Romero', given_name='A', middle_name=None, surname='Romero', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Liò', given_name='P', middle_name=None, surname='Liò', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None)], index=15, id='b15', unstructured=None, date='2018-05-03', title='Graph attention networks', book_title='6th International Conference on Learning Representations, ICLR 2018', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='Conference Track Proceedings', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='3gpp', given_name=None, middle_name=None, surname='3gpp', email=None, orcid=None, affiliation=None)], index=16, id='b16', unstructured=None, date=None, title='Evolved Universal Terrestrial Radio Access (E-UTRA); Further advancements for E-UTRA physical layer aspects', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='3', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[], index=17, id='b17', unstructured=None, date='2017', title='Generation Partnership Project (3GPP)', book_title=None, series_title=None, editors=None, journal='Technical Specification (TS)', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='36', issue=None, pages=None, first_page=None, last_page=None, note='Version 9.2.0', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Tan', given_name='M', middle_name=None, surname='Tan', email=None, orcid=None, affiliation=None)], index=18, id='b18', unstructured=None, date='1993', title='Multi-agent reinforcement learning: Independent versus cooperative agents', book_title='International Conference on Machine Learning (ICML)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Hessel', given_name='M', middle_name=None, surname='Hessel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Modayil', given_name='J', middle_name=None, surname='Modayil', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Van Hasselt', given_name='H', middle_name=None, surname='Van Hasselt', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Schaul', given_name='T', middle_name=None, surname='Schaul', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Ostrovski', given_name='G', middle_name=None, surname='Ostrovski', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Dabney', given_name='W', middle_name=None, surname='Dabney', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Horgan', given_name='D', middle_name=None, surname='Horgan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Piot', given_name='B', middle_name=None, surname='Piot', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M G Azar', given_name='M', middle_name='G', surname='Azar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Silver', given_name='D', middle_name=None, surname='Silver', email=None, orcid=None, affiliation=None)], index=19, id='b19', unstructured=None, date='2018', title='Rainbow: Combining improvements in deep reinforcement learning', book_title='AAAI Conference on Artificial Intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Son', given_name='K', middle_name=None, surname='Son', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Kim', given_name='D', middle_name=None, surname='Kim', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W J Kang', given_name='W', middle_name='J', surname='Kang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Hostallero', given_name='D', middle_name=None, surname='Hostallero', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Yi', given_name='Y', middle_name=None, surname='Yi', email=None, orcid=None, affiliation=None)], index=20, id='b20', unstructured=None, date='2019', title='QTRAN: learning to factorize with transformation for cooperative multiagent reinforcement learning', book_title='International Conference on Machine Learning (ICML)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Asplund', given_name='H', middle_name=None, surname='Asplund', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Johansson', given_name='M', middle_name=None, surname='Johansson', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Lundevall', given_name='M', middle_name=None, surname='Lundevall', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Jaldén', given_name='N', middle_name=None, surname='Jaldén', email=None, orcid=None, affiliation=None)], index=21, id='b21', unstructured=None, date='2018', title='A set of propagation models for site-specific predictions', book_title='12th European Conference on Antennas and Propagation', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='EuCAP', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='E Liang', given_name='E', middle_name=None, surname='Liang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Liaw', given_name='R', middle_name=None, surname='Liaw', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Nishihara', given_name='R', middle_name=None, surname='Nishihara', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Moritz', given_name='P', middle_name=None, surname='Moritz', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Fox', given_name='R', middle_name=None, surname='Fox', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Goldberg', given_name='K', middle_name=None, surname='Goldberg', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Gonzalez', given_name='J', middle_name=None, surname='Gonzalez', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M I Jordan', given_name='M', middle_name='I', surname='Jordan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Stoica', given_name='I', middle_name=None, surname='Stoica', email=None, orcid=None, affiliation=None)], index=22, id='b22', unstructured=None, date='2018', title='Rllib: Abstractions for distributed reinforcement learning', book_title='International Conference on Machine Learning (ICML)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Fey', given_name='M', middle_name=None, surname='Fey', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J E Lenssen', given_name='J', middle_name='E', surname='Lenssen', email=None, orcid=None, affiliation=None)], index=23, id='b23', unstructured=None, date='2019', title='Fast graph representation learning with PyTorch Geometric', book_title='ICLR Workshop on Representation Learning on Graphs and Manifolds', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None)], abstract='Future generations of mobile networks are expected to contain more and more antennas with growing complexity and more parameters. Optimizing these parameters is necessary for ensuring the good performance of the network. The scale of mobile networks makes it challenging to optimize antenna parameters using manual intervention or hand-engineered strategies. Reinforcement learning is a promising technique to address this challenge but existing methods often use local optimizations to scale to large network deployments. We propose a new multi-agent reinforcement learning algorithm to optimize mobile network configurations globally. By using a value decomposition approach, our algorithm can be trained from a global reward function instead of relying on an ad-hoc decomposition of the network performance across the different cells. The algorithm uses a graph neural network architecture which generalizes to different network topologies and learns coordination behaviors. We empirically demonstrate the performance of the algorithm on an antenna tilt tuning problem and a joint tilt and power control problem in a simulated environment.', body='I. INTRODUCTION Mobile networks can be composed of thousands of base station antennas, and each of them comprises many parameters to be configured. The configuration of these parameters such as tilt or power usually has a great impact on the overall network performance. With the growing complexity of networks in 5G and beyond, the problem of dynamically configuring those parameters becomes increasingly challenging  [1] . In addition to being costly, human monitoring and intervention on the network are not scalable. Multiple parameters can be used to affect the same performance metric, and changes in the configuration of one base station are likely to influence neighboring base stations and degrade their performance. In addition, the optimal choice of parameters is highly dependent on environmental factors as well as the spatial distribution of the traffic and the mobility of the connected user equipment. There is a need to design algorithms that can automatically learn tuning strategies to optimize the network performance by adapting to changes in the environment while considering all the possible interactions across neighboring base stations and across parameters. In contrast to hand-engineered or manual tuning strategies, reinforcement learning (RL) provides a flexible framework to learn a control strategy from data. Previous works have demonstrated its application to a variety of radio access network optimization use cases including remote electrical tilt control, optimization of handovers in 5G or power control, including industrial solutions  [2] -  [4] . Although they are promising, existing RL methods often fail to capture the needed coordination across neighboring base stations. For scalability reasons, they often resort to limiting assumptions and consider the control of one entity independently of the others, or rely on specific feature engineering to incorporate neighbor information as input to the reinforcement learning agents. In addition, they focus on optimizing local Key Performance Indicators (KPIs) involving one base station and its closest neighbors instead of considering the network as a whole in the reward formulation. In this work, we derive a multi-agent reinforcement learning algorithm capable to optimize network performance globally and control many base stations in a coordinated manner without relying on local reward approximation and feature engineering. Our approach extends state-of-the-art cooperative multi-agent reinforcement learning algorithms by proposing a novel offpolicy algorithm for cooperative learning when a graph structure is available. Inspired by value decomposition methods  [5] ,  [6] , we propose a new Q-network architecture, graph Qnetwork (GQN), that is particularly well suited to address mobile network optimization problems. GQN uses graph neural networks to generalize to different network topologies and share knowledge across neighboring base stations and different parameters to control. Our GQN training algorithm uses a reward signal characterizing the performance of the network in a global area as opposed to using ad-hoc credit assignment techniques, and thereby, the agents learn to coordinate to improve the global network performance. We demonstrate the performance of the proposed algorithm on two mobile network optimization problems: tilt control and joint tilt and power control. In the first problem, we show that GQN outperforms all the baselines and can generalize to different network topologies. In the second problem, we show the ability to control different antenna parameters in a scalable way by considering each parameter as an agent. A technical report with additional details and experiments in appendix can be found at preprinturl. \n II. RELATED WORK Existing works applying reinforcement learning for antenna tuning consider only local information of a cell and its closest neighbors  [7] ,  [8] . Several methods have been proposed to do so. The first one consists in hand-engineering an input feature and reward function to accommodate for the effect one cell can have on its neighbors. The KPIs of the neighboring cells are aggregated and the RL agent tries to optimize a combination of its own KPIs and the neighbor\'s KPIs  [9] . Other techniques have proposed to use graph neural networks to process neighbor information. However, they also require an ad-hoc engineering of the reward and can only control one base station at a time  [10] . Other algorithms attempting to address the global network optimization problem have been proposed in previous works using coordination graphs  [11] . This solution also required a heuristic to handle credit assignment between base stations by splitting individual rewards across neighbors. The inference cost of the message passing algorithm is larger than in our proposed method as it requires storing a neural network for every connected base stations in the graph. Our proposed algorithm can train a model controlling multiple antennas from a single global reward signal. In addition, previous works focus on controlling one antenna parameter, in this work we demonstrate the ability to control multiple parameters simultaneously (tilt and power). Multi-agent RL algorithms have been proposed for problems where multiple agents cooperate for a common goal. The closest algorithms to our methods are value decomposition networks (VDN) and QMIX  [5] ,  [6] . They rely on factorizing the problem to only learn one value function per agent contrary to using individual reward signals. They train these individual value functions using one global reward such that the sum of individual values (or a weighted sum in the case of QMIX) matches the global reward. We take inspiration from these factorization methods but add a graph neural network component. It can exploit the topology of the telecommunications network to learn a more efficient decomposition of the joint action value function. Algorithms in the literature have not been applied to network optimization problems and have rarely been scaled to more than dozens of agents. Our algorithm scales to hundred of agents and can cope with a varying number of agents both at training and deployment time. \n III. BACKGROUND \n A. Multi-agent Reinforcement Learning The problem of network optimization can be formulated as a multi-agent cooperative reinforcement learning problem where each network entity is an agent. The problem is modeled as a multi-agent Markov decision process  [12] . Formally, it is described by the tuple (S, A, T, R, γ), where S is a joint state space, A a joint action space, T an unknown transition model, R : S × A → R a global reward function, and γ ∈ [0, 1) a discount factor. A joint state s ∈ S is equal to (s 1 , . . . , s n ) where s i is the state of agent i and n is the number of agents. Similarly, a joint action a ∈ A is equal to (a 1 , . . . , a n ) where a i is the action of agent i. The agents do not need to be homogeneous and could have different action spaces. Our goal is to find a joint policy π : S → A that maximizes the discounted accumulated global reward over time. A standard approach in RL is to represent π by a value function Q : S × A → R such that π(s) = arg max a Q(s, a) . In singleagent cases, the Q-learning algorithm can be used to learn Q. For problems with continuous, or large state spaces, Q can be represented by a neural network and learned using deep Q-learning  [13] . It minimizes the Bellman error: min θ E (s,a,r,s ) [(Q(s, a; θ) − (r + γ max a Q(s , a ; θ)) 2 ] (1) where (s, a, r, s ) is a multi-agent experience sample collected by interacting with the environment and θ are the weights parameterizing the value function. The reward is a global reward for the whole system. Representing and estimating Q in multi-agent systems is hard because of the dimensionality of the joint state and action spaces increasing with the number of agents. In addition, solving the maximization problem to find π(s) is also challenging due to the large size of the joint action space. A common approach is to approximate the value function by factorizing it into individual value functions as follows  [5] : Q(s, a) ≈ n i=1 Q i (s i , a i ) where Q i is the individual value function associated to agent i. \n B. Graph Neural Networks Graph neural networks (GNNs) are a family of neural network architectures designed to process graph structured data. Generally, a GNN is a differentiable parametric function that takes as input a graph with node and edge attributes. Consider a graph G with vertices V and edges E where each node i ∈ V is associated to an attribute vector x i ∈ R d . A simple graph neural network layer processing the input graph G can be described by the following equation  [14] : h i = σ W 1 x i + W 2 j∈N (i) x j where h i is the latent features of node i, σ is an activation function, W 1 and W 2 are learnable weight matrices, and N (i) represents the set of neighbors of vertex i in the graph. Other types of GNNs have been considered with different ways of aggregating neighbor features and considering edge features such as using convolution or attention like operators  [14] -  [16] . \n IV. PROPOSED APPROACH In this section we describe how to model network optimization problems using cooperative multi-agent reinforcement learning. We then introduce a new algorithm that can efficiently exploit the inherent graph structure of the network to automatically coordinate different agents even in a scenario where they control different types of base station parameters. \n A. System Model Mobile networks are composed of several base stations equipped with antennas responsible for serving users in certain areas. An area of coverage and its associated antenna in a radio access network is referred to as a cell. In 5G and 6G networks, the number of cells in a network is expected to increase drastically by combining macro cells responsible for covering a large area and small cells providing increased capacity in targeted zones. Each of the antennas broadcasting a signal to the cell can be tuned to maximize the signal quality which results in improved quality of experience for the users of that cell. In this work we focus on two different antenna tuning scenarios: tuning the remote electrical tilt, and jointly tuning the tilt and the downlink transmission power. Our goal is to demonstrate that our proposed algorithm can handle environments with heterogeneous parameters to be controlled. Several metrics can be considered to maximize the user quality of experience such as throughput, coverage or signal quality. In this work, we focus on maximizing the downlink signal quality of all the users in the network as measured by the reference signal to interference and noise ratio (SINR) in dB. The goal of our algorithm is to find a policy mapping the global state of the network to a joint cell configuration in order to maximize the SINR of the whole network. The SINR, ρ u of a user u in a cell c depends on the tilt angle α and the downlink power φ as follows ρ u (φ, α) = Rc u (φc,αc) N cells i=1,i =c Ri,u(φi,αi)+µ where R c,u is the reference signal received power (RSRP). The relation between RSRP and antenna parameters is derived through antenna models defined in standards  [17] . More detailed on the RSRP calculation can be found in the appendix C of our technical report. We consider the global SINR of the network as the average of the SINR of all the users in the network: SINR G (α α α, φ φ φ, s) = 1/N users • u ρ u . Considering a geometric mean would have also been possible to model fairness among the users. The global SINR is a function of the joint configuration of all the cells in the network, along with some other exogenous factor such as the environment and the traffic distribution, all abstracted in the variable s. In constrast to the global SINR, we define a local SINR for a given cell c: SINR L,c = 1/N c • u∈c ρ u which is the average SINR of all the users connected to that cell, where N c is the number of users connected to cell c. \n B. Global Network Optimization as a Multi-agent MDP Maximizing the global network performance can be formulated as a multi-agent Markov decision process where each cell in the network is an agent. We assume that a graph modelling the relation between the different cells is available. Each node is a cell, and a vertex exists between two cells if they can influence each other. Automatic neighbor relations methods defined in the standards can be used to identify these edges  [17] . In our simulation, we use a criterion based on the geographical location and the azimuth angle of each cell along with the interference ratio between the cells. Namely, if cells are close to each other and have high mutual interference they are connected in the graph. An example of such a graph is illustrated in Fig.  1 . Similarly to previous work  [10] , each agent i is assumed to observe the following quantities: the position of the antenna (x i , y i ), the direction of the antenna (x ∆ i , y ∆ i ), the 10th, 50th, and 90th percentile of the SINR of the connected users, the antenna tilt angle α i , and the antenna maximum downlink power φ i . The joint state space S is represented by the Cartesian product of the individual state of each cell. We now consider two different problems: a tilt control problem and a joint tilt and power control problem. Each problem has a different action space and a different reward function. The true objective of these antenna tuning problems is to maximize the global SINR of the network to improve  the quality of service and minimize the transmitted power to reduce the energy consumption (in the case where power is controllable). Existing multi-agent RL algorithms often must rely on ad-hoc decompositions of the reward signal in order to scale to a large number of agents. In order to compare with these baselines, we define both a global reward function that can be used by our method and a local reward function that will be used by the proposed baseline algorithm in our experimental section. The global SINR is defined as the arithmetic mean of the SINR of all the users in the network. However, other definitions such as geometric mean could be used in order to promote fairness. Our method could be applied in a straightforward way to these other definitions. Tilt control: The tilt action space is defined by a set of remote electrical tilt changes of {−1°, 0°, +1°}. The tilt is always bounded within the range [0°, 15°]. In this scenario, the maximum transmission power of each cell is fixed to 40 W. The local and global reward functions are defined as follows: • Global SINR reward: R(s, a, s ) = SINR G (α α α, φ φ φ, s). • Local SINR reward for cell i: R i (s, a, s ) = SINR L,i + 1/N i • j∈N (i) SINR L,j . This local formula considers the performance of a single cell and the average performance of its neighbors. We consider all the neighbors in the graph and each of them are equally weighted. A disadvantage of this approach is that we need to choose how to weigh the neighbor manually, while our proposed approach will learn automatically the reward decomposition. Similarly to the formula above, we only consider tilt when using this formula. Joint tilt power control: The joint action space is defined by the Cartesian product of the tilt action space described above, and a set of maximum downlink power changes {−5 W, 0 W, +5 W} resulting in a total of 9 actions per cell. The power is bounded to the range [10 W, 60 W]. The reward functions are defined by: • Global SINR and power reward R(s, a, s ) = (1 − w)SINR G (α α α, φ φ φ, s)−w•1/N cells • c φ c . This reward definition adds a penalty based on the average transmitted power of the cells. The penalty is weighted by a hyperparameter w. Setting w to 0 would yield to maximizing the power (since it leads to maximum SINR) and optimizing tilt, setting w to 1 would lead to completely minimizing the power and disregarding any effect of the tilt. In practice, the choice of w should be driven by business intents from the network operators. • Local SINR and power reward R i (s, a, s ) = (1 − w)SINR L,i −wφ i +1/N i • j∈N (i) [(1−w)SINR L,j −wφ j ] Given a joint network state s, we are interested in a policy giving a joint configuration of tilt and power to maximize the expected accumulated reward using the formalism of multiagent reinforcement learning described in Section III-A. The number of actions per cell grows exponentially with the number of parameters considered. Since we consider only tilt and power, the size of the action space is still reasonably small and we can learn a single joint controller for tilt and power. In our experiments, we are also going to consider the possibility of making two separate agents for each cell, one controlling the tilt and one controlling the power. Maximizing the global reward is generally challenging due to the complex interactions between the cells in the networks. Instead one can rely on value decomposition techniques such as independent Q-learning  [18] . Each cell would be learning using their local observation and the proposed local reward definition considering only direct neighbors. The local reward ignores the fact that some neighbors might be more important than other and that indirect interactions with neighbors further away in the graph could affect performance. To consider the global effect of all the cell in the network, we propose the graph Q-network algorithm. \n C. Graph Q-Network We describe a multi-agent reinforcement learning algorithm to control multiple cells in the network optimizing a single global objective. The algorithm relies on graph neural networks to process information from all the agents. It then uses a factorization technique to learn individual value functions for each cell from a single global reward signal, rather than relying on hand-engineered reward decomposition across the cells. At deployment time, the trained model can be evaluated with an arbitrary number of cells allowing to train at small scale but deploy at large scale. These properties are mostly enabled by an original representation of the joint state action value function which we name graph Q-network (GQN). To train such a model using a single global reward signal for all the agents, we rely on a factorization technique. Factorization generally consists of decomposing a very large function into a combination of smaller components. In this problem, we wish to learn an additive decomposition of the joint state action value function such that: Q(s, a) = i Q i (h i , a i ) where (h 1 , . . . , h n ) = GQN(s, A). The GQN function represents our proposed model and A denotes the adjacency matrix of the network graph. Intuitively, the GQN is going to learn a hidden state representation for each agent, such that the global value function is linearly factorizable into individual value functions in that hidden space. To simplify the notation, we note each output node of the GQN as Q i (s, A, a i ), which represents the value of taking action a i for node i. The GQN architecture is illustrated in Fig.  2 . It is parameterized by weights and is end to end differentiable. The model takes as input a feature vector for each agent to control along with a graph representation of the mobile network. The construction of such graph is described in the previous section and has been also demonstrated in previous work  [10] ,  [11] . Each agent is represented by a node in the graph. The feature vector s i corresponds to the local state of a cell as described in the previous section. It is processed by an encoding layer which consists of multilayer perceptrons (MLPs) applied individually to each node feature vector. To improve sample efficiency during training and the generalization of the model to an arbitrary number of nodes, the MLPs have shared weights. The encoded features, h e i , are then processed by several GNN layers. We experimented with the GNN architectures from Morris et al.  [14]  and the graph attention neural networks from Velickovic et al.  [16] . The output of the GNN layers consists of another set of hidden features, h d i , one for each node, passed through a decoding layer. The model is trained such that the output node embeddings represent the individual value function of each agent which we note as: Q i (h d i , a i ). Training a GQN model follows an off-policy training procedure similar to DQN and can benefit from existing innovations like prioritized experience replay, target networks and double Q learning  [19] . The convergence guarantees of GQN are similar to the one provided by DQN. The training procedure of GQN is described in our technical report. The GQN architecture has the sufficient property (additivity) to satisfy the individual global maximum principle  [20]  which states that the jointly optimal actions are given by taking the individual maximum of each individual value function at the output of the GQN model: a * = arg max Q(s, a) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 arg max Q 1 (h d 1 , a 1 ) arg max Q 2 (h d 2 , a 2 ) . . . arg max Q n (h d n , a n ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb (2) Contrary to previous work  [10] , the model outputs state-action value functions for all the agents at the same time. \n V. EXPERIMENTS We empirically demonstrate the performance of our proposed algorithm in simulated environments. Using the scenarios in Section IV-B, we demonstrate three different experiments illustrating respectively: the training performance for tilt environment, the generalization performance for tilt control, the training performance for the joint control problem with an additional agent decomposition scheme. \n A. Experiment Setup Our experiments rely on a proprietary system level simulator which implements a ray tracing propagation model for computing the path gains at various user locations  [21] . Details  M L P M L P G N N G N N M L P M L P M L P 𝑠 " 𝑠 # ℎ # $ ℎ " $ ℎ ! $ ℎ # % ℎ " % ℎ ! % 𝑞 " (𝑎 ! ) 𝑞 " (𝑎 " ) ⋮ Fig. 2. Graph Q-network architecture about the simulation parameters and training parameters can be found in our technical report in appendix 1 . We compare the following algorithms: • DQN: The DQN algorithm with a target network, double Q-learning, prioritized replay, and distributed training  [19] . • Neighbor DQN (N-DQN): it is a simple extension to DQN where the observation is augmented with the observation of the neighboring cells, proposed in previous work  [10] . • GAQ  [10] : the graph attention Q-network algorithm processes neighbor features using a graph attention layer. Contrary to our proposed approach, GAQ is trained using a local reward and is not able to perform joint control. • GQN: our proposed method. We write GQN(GAT) when we used a graph attention layer  [16] , otherwise we used a graph convolutional layer from Morris et al.  [14] . • Heuristic (H): This method is a rule-based method that sets the tilt angle such that the beam points to the middle of the cell. It observes the height of the antenna and the distance to its closest neighboring cell and calculates a desired tilt angle. We did not add QMIX as part of the baselines as it is already shown to be outperformed by GAQ in previous work  [10] . On the joint tilt and power control problem, QMIX was at most as good as DQN for some seeds, but the training was too unstable to report meaningful results in this paper. In the figures, the solid lines represent the mean over 3 random seeds and the shaded area is the 95th percentile confidence interval. The line for the heuristic represents the average performance and confidence interval evaluated on 300 episodes. We compare our approach (GQN, GQN(GAT)) to a standard DQN approach, N-DQN, and GAQ, a state-of-the-art algorithm developed for antenna tilt control  [10] . \n B. Training performance We can see that all the methods except from N-DQN have similar convergence properties. In these experiments, all the methods follow DQN-like procedure and the convergence was mostly affected by the choice of the exploration schedule which is the same for all the baseline for a fair comparison. For the heuristic, we plot the average performance, as it is not a learning based algorithm and the performance is constant. We can see that all the RL algorithms outperform the baseline except for N-DQN. While almost all the methods converge to an improved average SINR compared to the initial performance, GQN provides the best solution. We notice a difference of about 1 dB in the average performance. The convergence rate is roughly similar for all the methods with a slight disadvantage for GQN with the graph convolutional layer. The neighbor DQN approach performed poorly in general. We notice that the poor performance is accompanied by a high standard deviation. This is explained when looking to each of the three random seeds, only one performed at a level comparable to GAQ and DQN while the other seeds did not converge. We blame this instability on the complexity of the observation space and the difficulty to process this information with a simple fully connected neural network. The complexity of adding the graph neural network layers does not seem to affect the convergence for both GQN and GAQ. This result confirms the initial intuition that being able to train on the global objective rather than manually decomposing the global reward into individual cell reward (for DQN and GAQ) leads to better global performance. \n C. Generalization to New Deployments In this experiment, we save the trained models from Fig.  3  and evaluate them on network deployments unseen at training time. In addition, we evaluate another batch of models that were trained on random deployments rather than hexagonal deployments. We have two possible training configurations, hexagonal or random, and two evaluation configurations, hexagonal or random. Examples of a random training configuration and a hexagonal evaluation configuration are given in Fig.  1 . We evaluate the models with up to 37 macro base stations which makes a total of 111 agents against 57 at training time. Figure  4  illustrates the performance of the trained model on the hexagonal evaluation scenarios when training with random Q N G Q N -G A T D Q N G A Q H 12 13 14 15 16 Average SINR (dB) hexagonal training hexagonal evaluation or hexagonal deployments. The performance is average over 50 episodes and across three random seeds. We can see that the GQN methods provide overall better performance than the other methods, followed closely by the GAQ algorithm. Using a graph attention layer in GQN provides an additional gain in performance. An interesting outcome is that the GQN(GAT) model trained on random deployment, has a comparable performance on the hexagonal evaluation scenario (right plot) than the DQN and GAQ models that were trained specifically on hexagonal topologies (left plot). When the difference between training deployment and evaluation deployment is large (from random to hexagonal), the heuristic has a very close performance to some of the reinforcement learning algorithms, indicating that generalization to those scenarios is a difficult task. We omitted N-DQN from the figure for readability, its performance is around 9 dB in all cases. A poor performance is expected for this baseline as the neighbor features are stacked in a single vector. This input does not take into account the graph structure and is dependent on the order of the neighbors are stacked in. G Q N G Q N -G A T D Q N G A Q H 12 \n D. Joint Control Problem In this experiment, we consider the possibility to control heterogeneous agents. In the simplest case, the joint tilt and power control can be addressed by designing one agent per cell with an action space of size 9. This is the approach we will refer to as "joint". Such approaches might not be scalable if the number of parameters to control grows. Instead, one could consider two agents per cell: one controlling the tilt, and one controlling the power, each with an action space of size 3. This is the approach we use in the GQN algorithm in this experiment. Instead of controlling 57 agents, the GQN will control 114 agents. We simply modify the graph topology such that each cell consists of two nodes connected to each other, but with the same neighbors as in the joint control case. Results are presented in Fig.  5 . By looking at the two objectives, SINR and average power, we can see that the final SINR is similar for all the methods while the final power value shows a significant difference. For a similar average SINR, the GQN algorithm is able to reduce the power by more than 35 %. In addition, the GQN methods tend to converge faster on this Training Steps Tx Power (W) Fig.  5 . Training performance of GQN with separate agents for tilt and power, joint GQN and the baselines in the joint control environment. For a similar average SINR, GQN is able to yield a greater power reduction. problem. Our hypothesis is that the increase of dimensionality in the action space impacts more the methods relying only on fully connected neural networks. Another observation is that the GQN approach yields the same average performance as joint GQN. This result suggests that GQN might be a good algorithm to decompose the antenna tuning problem into multiple agents per parameters. We expect that the standard deviation could be further reduced by more intense hyperparameter tuning. \n VI. CONCLUSIONS We presented a novel multi-agent reinforcement learning solution to tune many antennas in mobile networks. The proposed algorithm is an off-policy algorithm that, given a reward associated to the global network performance, learns to assign credits to the different antennas. Our method is able to learn the credit assignment and coordination behavior thanks to a graph Q-network, a graph neural network representation of the joint state action value function. Our experiments evaluate the learning algorithm in a tilt control scenario and a joint power and tilt control scenario. The results show that the trained model is able to learn control policies leading to a better global average SINR and more power savings. In addition, the trained GQN is able to generalize to denser network topologies unseen at training time, with almost double the number of agents. In the joint control scenario, we illustrated a way to separate the control of two parameters as different agents connected in a graph. By splitting the agents per parameter it allowed to maintain a smaller action space per agent while leading to similar performance as a joint controller. Future works involve evaluating the algorithm on a broader range of problems and learning the graph structure automatically. We also consider investigating the control of advanced antenna systems which would include a much larger number of parameters per antenna and hence increase the number of agents in the graph. \n APPENDIX The appendix contains additional details about the method, parameters used for the experiments and additional experiments. \n A. EXPERIMENTAL DETAILS In this appendix, we describe the different hyperparameters for the algorithm and some implementation details about the experiments. Our simulator models an LTE network with antennas that can be controlled remotely for changing the electrical downtilt angle and the maximum downlink transmitted power and operating at a frequency of 2 GHz. The network has 10 000 users uniformly distributed on the map, generating an average traffic of 1 Mbps per cell. The training consists of 20 000 steps split into episodes of 20 steps. At the beginning of an episode, a new deployment is sampled. We consider 19 base stations in the training environment, each consisting of 3 antennas, which makes a total of 57 cells. Each cell is associated to a learning agent. The average intersite distance between base stations is uniformly sampled between 300 m and 1500 m. We consider both hexagonal deployment and randomly generated deployment of the base stations with a minimum intersite distance, as illustrated in Fig.  6 . At the beginning of an episode, the electrical tilt of each antenna is reset to a random value within the range [0°, 15°]. In the joint tilt and power control experiment the power is also reset to a random value while for the tilt experiment it is fixed at 40 W. The baselines are using exactly the same simulator configuration and are trained with the same random seed such that they experience exactly the same simulated networks both in training and evaluation. The observation features and the reward signal are normalized during training. They all use an -greedy policy for exploration with a decay of the exploration rate, , from 1 to 0.01 during the first half of the training. Each training is repeated with three different random seeds.   All the baselines are implemented using rllib  [22]  and the Pytorch geometric library  [23] , they all use a target network, prioritized replay, double Q-learning and distributed experience collection with 5 workers, each using 3 CPUs. We report all the hyperparameters in Table  I . For all the baselines we carried out a hyperparameter search on the learning rate and number of layers and we rescaled the observation vectors and reward signals such that their value is between [−1, 1]. We used a discount factor of 0.0 since our problem\'s goal, reaching the final optimal antenna configuration, does not require visiting intermediate states with performance degradation. Increasing the discount factor to 0.9 did not lead to any benefits for any of the baselines for this specific application. However, the method, as described in the previous section, still holds for larger discount factors. It leads to the same asymptotic results but with slower convergence. In the N-DQN baseline, a maximum of 5 neighbors is considered. Each observation vector from the neighbors is stacked and fed to a feed forward neural network. For our GQN algorithm, we tried adding encoding MLPs before the GNN layer but it did not bring any improvement in performance (nor did it damage it). We also experimented with a graph convolutional neural network layer instead of the graph attention one. We provide a pseudocode of our proposed GQN method in Algorithm 1. It follows a similar off policy training procedure as DQN. We use a target network, double Q learning and prioritized experience replay. The state of each agent and the graphs are stored in the replay buffer. Otherwise choose a = a * according to Eq. (  2 ). \n 6: Observe the next joint state s, the next adjacency matrix A , and the global reward r. \n 7: Store the transition (s, A, a, r, s , A ) in D. 8: Sample batch {(s k , A k , a k , r k , s k , A k ) for k = 1, . . . , B} from D. 9: Assign y k = r k + γ max a i Q i (s k , A k , a i )∀k as per Eq. (1). 10: Perform a gradient descent step on the loss 1/B • k [( i Q i (s k , A k , a k i ) − y k ) 2 ] 11: end for \n B. ADDITIONAL EXPERIMENTS Tilt Generalization In the tilt generalization experiment, we tried different combination of training the models on random deployments and evaluating on hexagonal deployments and vice versa. The results are presented in Fig.  7 . In all cases, the GQN-GAT method present the best performance. An interesting outcome is that the GQN(GAT) model trained on random deployment, has a better performance on the hexagonal evaluation scenario (bottom left plot) than the DQN and GAQ models that were trained specifically on hexagonal topologies (top left plot). On the evaluation with random cell layout, the heuristic has a very close performance to some of the reinforcement learning algorithms, indicating that generalization to those scenarios is a difficult task, especially when the difference between training and deployment layout is large (top right plot). \n Joint Tilt and Power Control In this scenario the reward function is parameterized by w which controls the trade-off between minimizing power and maximizing signal quality. We experimented with value of w in {0.05, 0.1, 0.15, 0.2, 0.5} for the reward functions. For the most extreme values for w the agents learns to set the power to the maximum value (low w), or the minimum (high w). The values of 0.15 and 0.2 gave the most interesting results where the agent converged to non extreme power values. We witnessed the same effect for all methods and only report the results for w = 0.15 for the sake of readability. In Fig.  8 , we add the evolution of the global reward. The GQN model are directly trying to optimize this reward while other algorithms must rely on local signals. As a consequence they do not give as much power reduction as GQN. \n C. RSRP AND SINR CALCULATIONS To derive the relation between SINR and cell configurations we first define the reference signal received power (RSRP) R c,u for a user u connected to a cell c: R cu (φ, α) = P c (φ)G c,u (α) L c,u where P c (φ) is the transmitted power of the antenna per reference signal resource element, as a function of the maximum transmitted power φ. G c,u (α) is the antenna gain which depends on the tilt angle α and the azimuth which is kept fixed in our model. L c,u is the path loss. To compute the antenna gains, one can use the relationship defined in the third generation partnership project  [17] . The maximum power φ Training Steps Global Reward Fig.  8 . Training performance of GQN (including global reward) with separate agents for tilt and power, joint GQN and the baselines in the joint control environment. For a similar average SINR, GQN is able to yield a greater power reduction. The solid lines represent the mean over 3 random seeds and the shaded area is the 95th percentile confidence interval. is divided by the number of available resource blocks and multiplied by the cell specific reference signal power boost gain (set to one in our simulation). The received power is only depending on the cell that the user is connected to and on the propagation environment which affects the path loss. The user also receives power from other cells which is regarded as interference in the SINR calculation. For a user u, the cell c that the user is attached to is assumed to be the cell yielding the largest received power. The downlink SINR of a user u is defined as the ratio between the received power from cell c and the sum of the received power from all other cells, and the noise power µ: ρ u (φ, α) = R cu (φ c , α c ) Ncells i=1,i =c R i,u (φ i , α i ) + µ (4) The noise power is calculated over a frequency bandwidth of one resource element (15 kHz). Improving the SINR of a user involves improving the received power as well as reducing interference from other cells. Fig. 1 . 1 Fig. 1. Macro base station network deployments used in our experiments with the graph connecting the different cells (three per base station). The topology of the graph can change depending on the intersite distance. Evaluation layouts use denser deployments with more agents. The position of the node is offset from their real position for better visualization. \n Figure 3 Fig. 3 . 33 Figure3illustrates the performance of GQN, GQN(GAT), DQN, GAQ and N-DQN on a tilt tuning scenario. We report the average SINR of the whole network (averaged over all the user equipments) which corresponds to the global SINR optimization target. The results are shown when training in a hexagonal deployment. A training step corresponds to one interaction with the environment. Similar conclusions can be drawn when training in the scenario with random deployments.1 Experimental details can be found in appendix A here: technicalreport \n Fig. 4 . 4 Fig. 4. Generalization performance of the GQN algorithm and the baselines on deployments unseen at training time. The points represent the average performance and the error bars represent the 95th percentile confidence interval (sometimes smaller than the marker). \n Fig. 6 . 6 Fig. 6. Visualization of the hexagonal and random deployments used for training and evaluation. Macro base station network deployments used in our experiments with the graph connecting the different cells (three per base station). The topology of the graph can change depending on the intersite distance. Evaluation layouts use denser deployments with more agents. The position of the node is offset from their real position for better visualization. \n Algorithm 1 3 : 13 GQN training procedure. 1: Initialize: GQN weights, replay buffer D = {}, training steps T , batch size B, schedule 2: for t = 1, . . . T Observe joint state s, and adjacency matrix A 4:With probability , choose a random joint action a 5: \n Fig. 7 . 7 Fig. 7. Generalization performance of the GQN algorithm and the baselines on deployments unseen at training time. The evaluation deployments are denser, as illustrated in Fig. 1. The points represent the average performance and the error bars represent the 95th percentile confidence interval (sometimes smaller than the marker). \n TABLE I HYPERPARAMETERS I FOR TRAINING THE BASELINES AND OUR ALGORITHM. THE FIRST LINES ARE COMMON TO ALL ALGORITHMS. γ 0.0 learning rate 1 × 10 −3 initial 1.0 final 0.01 decrease steps 10 000 activation function ReLU batch size 64 (N)DQN architecture FC(64), FC(32) GAQ architecture GAT(32, 6 heads), GAT(32, 6 heads), FC(32), FC(32) GQN architecture GCN(32), GCN(32), FC(32), FC(32) GQN(GAT) architecture GAT(32, 4 heads), GAT(32, 4 heads), FC(32), FC(32) GQN learning rate 1 × 10 −2', acknowledgement=None, annex=None)
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Peter Van Roy', given_name='Peter', middle_name='Van', surname='Roy', email=None, orcid=None, affiliation=GrobidAffiliation(institution='Université catholique de Louvain Seif Haridi Royal Institute of Technology', department=None, laboratory=None, address=None))], index=None, id=None, unstructured=None, date='2022-09-16', title='Ideas for the future of Prolog inspired by Oz', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2302.00558v1[cs.PL]', pii=None, ark=None, istex_id=None, url=None), pdf_md5='5FF162BBE6E14933F82AD698043E07B0', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='Francesco Cesarini', given_name='Francesco', middle_name=None, surname='Cesarini', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Steve Vinoski', given_name='Steve', middle_name=None, surname='Vinoski', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date='2016', title='Designing for Scalability with Erlang/OTP: Implementing Robust, Fault-Tolerant Systems', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher="O'Reilly Media Inc", institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L Keith', given_name='L', middle_name=None, surname='Keith', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Clark', given_name=None, middle_name=None, surname='Clark', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='1987-06', title='PARLOG: The Language and its Applications', book_title='Proceedings of the Conference on Parallel Architectures and Languages Europe (PARLE). Volume II: Parallel Languages', series_title=None, editors=[GrobidAuthor(full_name='A J Nijman', given_name='A', middle_name='J', surname='Nijman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J W De Bakker', given_name='J', middle_name='W', surname='De Bakker', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P C Treleaven', given_name='P', middle_name='C', surname='Treleaven', email=None, orcid=None, affiliation=None)], journal=None, journal_abbrev=None, publisher='Springer', institution=None, issn=None, eissn=None, volume='259', issue=None, pages='30-53', first_page='30', last_page='53', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Raphaël Collet', given_name='Raphaël', middle_name=None, surname='Collet', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date='2007-12', title='The Limits of Network Transparency in a Distributed Programming Language', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution='Department of Computing Science and Engineering, Université catholique de Louvain', issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Seif Haridi', given_name='Seif', middle_name=None, surname='Haridi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Peter Van Roy', given_name='Peter', middle_name=None, surname='Van Roy', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Per Brand', given_name='Per', middle_name=None, surname='Brand', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Michael Mehl', given_name='Michael', middle_name=None, surname='Mehl', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Ralf Scheidhauer', given_name='Ralf', middle_name=None, surname='Scheidhauer', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Gert Smolka', given_name='Gert', middle_name=None, surname='Smolka', email=None, orcid=None, affiliation=None)], index=3, id='b3', unstructured=None, date='1999-05', title='Efficient Logic Variables for Distributed Computing', book_title=None, series_title=None, editors=None, journal='ACM Transactions on Programming Languages and Systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='21', issue='3', pages='569-626', first_page='569', last_page='626', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Gilles Kahn', given_name='Gilles', middle_name=None, surname='Kahn', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='David B Macqueen', given_name='David', middle_name='B', surname='Macqueen', email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='1977', title='Coroutines and networks of parallel processes', book_title='IFIP Congress', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='993-998', first_page='993', last_page='998', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name="Richard A O'keefe", given_name='Richard', middle_name='A', surname="O'keefe", email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date='1990', title='The Craft of Prolog', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='MIT Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Vijay Saraswat', given_name='Vijay', middle_name=None, surname='Saraswat', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Martin Rinard', given_name='Martin', middle_name=None, surname='Rinard', email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date='1990-01', title='Concurrent Constraint Programming', book_title='POPL 90', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='232-245', first_page='232', last_page='245', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Ehud Shapiro', given_name='Ehud', middle_name=None, surname='Shapiro', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date='1983-01', title='A Subset of Concurrent Prolog and its Interpreter', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution='Institute for New Generation Computer Technology (ICOT)', issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Marc Shapiro', given_name='Marc', middle_name=None, surname='Shapiro', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Nuno Preguiça', given_name='Nuno', middle_name=None, surname='Preguiça', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Carlos Baquero', given_name='Carlos', middle_name=None, surname='Baquero', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Marek Zawirski', given_name='Marek', middle_name=None, surname='Zawirski', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date='2011', title='Conflict-free Replicated Data Types', book_title='Symposium on Self-Stabilizing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='386-400', first_page='386', last_page='400', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Leon Sterling', given_name='Leon', middle_name=None, surname='Sterling', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Ehud Shapiro', given_name='Ehud', middle_name=None, surname='Shapiro', email=None, orcid=None, affiliation=None)], index=9, id='b9', unstructured=None, date='1986', title='The Art of Prolog: Advanced Programming Techniques', book_title='Series in Logic Programming', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='MIT Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Peter Van', given_name='Peter', middle_name=None, surname='Van', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Roy', given_name='Roy', middle_name=None, surname=None, email=None, orcid=None, affiliation=None)], index=10, id='b10', unstructured=None, date='2019', title='Why Time is Evil in Distributed Systems and What To Do About It', book_title='Keynote talk, CodeBEAM 2019', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='https://www.youtube.com/watch?v=NBJ5SiwCNmU'), GrobidBiblio(authors=[GrobidAuthor(full_name='Peter Van', given_name='Peter', middle_name=None, surname='Van', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Roy', given_name='Roy', middle_name=None, surname=None, email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Seif Haridi', given_name='Seif', middle_name=None, surname='Haridi', email=None, orcid=None, affiliation=None)], index=11, id='b11', unstructured=None, date='2004', title='Concepts, Techniques, and Models of Computer Programming', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='The MIT Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='Course website', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=':seectm.info.ucl.ac.be/en'), GrobidBiblio(authors=[GrobidAuthor(full_name='Peter Van Roy', given_name='Peter', middle_name=None, surname='Van Roy', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Seif Haridi', given_name='Seif', middle_name=None, surname='Haridi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Christian Schulte', given_name='Christian', middle_name=None, surname='Schulte', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Gert Smolka', given_name='Gert', middle_name=None, surname='Smolka', email=None, orcid=None, affiliation=None)], index=12, id='b12', unstructured=None, date='2020-06', title='A History of the Oz Multiparadigm Language', book_title='The Fourth ACM SIGPLAN History of Programming Languages Conference (HOPL IV), Proceedings of the ACM on Programming Languages (PACMPL)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='4', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None)], abstract='Both Prolog and Oz are multiparadigm languages with a logic programming core. There is a significant subset of Oz that is a syntactic variant of Prolog: pure Prolog programs with green or blue cuts and bagof/3 or setof/3 can be translated directly to Oz. Because of this close relationship between Prolog and Oz, we propose that the extensions made by Oz to logic programming can be an inspiration for the future evolution of Prolog. We explain three extensions, namely deterministic logic programming, lazy concurrent functional programming, and purely functional distributed computing. We briefly present these extensions and we explain how they can help Prolog evolve in its next 50 years.', body='Introduction The Prolog language was invented 50 years ago and today in 2022 it is still widely used. Many possible extensions were and are being defined for Prolog to allow it to evolve as a living language. The Oz language was developed starting in 1991 and it led to a major system release in 1999 called Mozart  [13] . The main goal of Oz was to cleanly support many programming paradigms. The Oz language and Mozart system saw wide use for about one decade, but then its use declined. After 2009 and up to the present day, it is mainly used only for programming education. This is unlike Prolog, which still enjoys wide use up to the present day. Despite its decline in usage, Oz pioneered many important concepts that have moved into the mainstream since its release. We mention deterministic dataflow computation, actors that return futures, deep support for constraint programming through computation spaces, and deep embedding of distributed computing. This is relevant for Prolog because of the close semantic relationship between Oz and Prolog. The semantic foundation of Oz is concurrent constraint programming, which is a generalization of Prolog\'s execution model  [7] . Prolog programs with logical semantics can be directly translated into Oz, as explained in Section 2. Given these facts, we believe that the design of Oz can be an inspiration for the future evolution of Prolog, so that Prolog can enjoy another 50 years of success. The purpose of this article is to present three important extensions to Prolog-style logic programming made by Oz and explain why they can be an inspiration to future Prolog development: • Deterministic logic programming (Section 3). We show how Oz supports writing logic programs that support managing deterministic execution while maintaining their logical semantics. For a given logical semantics, we show how Oz can express different operational semantics. • Lazy concurrent functional programming (Section 4). We show how the logic programming core of Oz can be the foundation of a series of functional programming paradigms, culminating in lazy deterministic dataflow. All these paradigms keep the strong confluence properties of pure functional programming as well as their origin in deterministic logic programming. • Purely functional distributed computing (Section 5). The Oz community built a distributed implementation of Oz based on a deep embedding approach, where each language entity has its own distributed algorithm. As part of this work, we designed and proved correctness of distributed rational tree unification. Based on the use of logic variables for dataflow synchronization, we showed that asynchronous message passing can be purely functional. Finally, we showed that most practical distributed systems used today greatly overuse nondeterminism, and that it would be better to design them as mostly functional, adding nondeterminism only in the few places where it is needed. Each of these extensions is presented separately in its own section, In each case, we explain the main ideas and we give references for readers wishing to investigate further. As a preliminary step, Section 2 explains how to translate Prolog programs into Oz, to justify our claim that Oz is a syntactic variant of Prolog. \n Oz is a syntactic variant of Prolog We explain how to translate any pure Prolog program extended with green or blue cuts and the bagof/3 or setof/3 predicates into an Oz program with the same logical and operational semantics as the Prolog program. We assume the reader has enough knowledge of Prolog to understand our examples. For brevity, we do not explain the semantics of the Oz statements in detail. Their meaning should be clear from the examples. For a more complete and formal explanation of the relationship between Oz and Prolog, we refer the reader to Chapter 9 of  [12] . Both Oz and Prolog support symbolic data structures that are bound using unification. Similar to many modern Prolog systems, Oz uses rational tree unification. There are slight differences in syntax and semantics between Oz and Prolog data structures, mainly because Oz supports additional data structures such as records. We do not explain these differences here; we refer the reader to  [12]  for a precise definition of Oz data structures. \n Deterministic predicates A predicate is deterministic if all its clauses are logically disjoint. This is the case when the clauses have disjoint guards. Deterministic predicates are translated using the Oz statements if and case, which both have a logical semantics.  \n Nondeterministic predicates A predicate is nondeterministic when its clauses are not logically disjoint. Such predicates can be backtracked into, possibly giving multiple solutions. Nondeterministic predicates are translated using the Oz statement choice. Consider the following nondeterministic Prolog predicate: place_queen(N, [N|_], [N|_], [N|_]). place_queen(N, [_|Cs2], [_|Us2], [_|Ds2]) :- place_queen(N, Cs2, Us2, Ds2). This predicate is translated into a nondeterministic Oz procedure:  \n First-class Prolog top level The previous two predicates are part of a program that computes solutions to the n-queens problem: how to place n queens on an n × n chessboard so that no queen attacks another. For completeness, we give the full program here: The syntax fun {$} ... end defines a zero-argument function that calls the one-argument function Queens. The system procedure Browse displays its argument. The answer displayed by Browse is a list giving the first solution: Oz also provides SolveAll to compute all solutions and Solve to compute a lazy list of solutions. The Solve corresponds closely to a Prolog top level where solutions are computed on demand. fun \n Predicates with green or blue cuts If your Prolog program uses cut "!", then the translation to Oz is simple if the cut is a green or blue cut as defined by O\'Keefe  [6] . A green cut removes irrelevant solutions. A blue cut indicates to the compiler that the program is deterministic. To show the translation scheme, we translate the following predicate: foo(X, Z) :-guard1(X, Y), !, body1(Y, Z). foo(X, Z) :-guard2(X, Y), !, body2(Y, Z). The two guards must not bind any head variables, i.e., they are quiet guards. It is good Prolog style to postpone binding head variables until after the cut. The translation has two cases, depending on whether the guards are deterministic or not. If a guard is deterministic (it has no choice), then it can be written as a deterministic boolean function. This gives the following simple translation: proc {Foo X Z} if Y in {Guard1 X Y} then {Body1 Y Z} elseif Y in {Guard2 X Y} then {Body2 Y Z} \n else fail end end If a guard is nondeterministic (it uses choice), then it can be written with one input and one output argument, like this: {Guard1 In Out}. It should not bind the input argument. This gives the following translation: proc {Foo X Z} case {SolveOne fun {$} {Guard1 X} end} of [Y] then {Body1 Y Z} elsecase {SolveOne fun {$} {Guard2 X} end} of [Y] then {Body2 Y Z} else fail then end If neither of these two cases apply to your Prolog program, e.g., either your guards bind head variables or you use cuts in other ways (i.e., as red cuts), then your program likely does not have a logical semantics. A program with red cuts is defined only by its operational semantics and this is outside the scope of our translation scheme. \n The bagof/3 and setof/3 predicates Prolog\'s bagof/3 predicate corresponds to using SolveAll inside an Oz program. Its extension setof/3 sorts the result and removes duplicates. This can be done with the Oz built-in Sort operation. We show how to translate bagof/3 both without and with existential quantification. Consider the following small biblical database (inspired by  [10] ): \n father(terach, abraham). father(abraham, isaac). father(haran, milcah). father(haran, yiscah). This can be written as follows in Oz: proc {Father F C} choice F=terach C=abraham [] F=abraham C=isaac [] F=haran C=milcah [] F=haran C=yiscah end end Calling bagof/3 without existential quantification, e.g.: children1(X, Kids) :-bagof(K, father(X,K), Kids). is defined as follows with SolveAll: proc {Children1 X Kids} {SolveAll proc {$ K} {Father X K} end Kids} end The Children1 definition is deterministic; it assumes X is known and it returns Kids. To search over different values of X the following definition should be used instead: proc {Children1 X Kids} {Father X _} {SolveAll proc {$ K} {Father X K} end Kids} end The call {Father X _} creates a choice point on X. The "_" is syntactic sugar for local X in X end, which is just a new variable with a very small scope. Calling bagof/3 with existential quantification, e.g.: children2(Kids) :-bagof(K, X^father(X,K), Kids). is defined as follows with SolveAll: proc {Children2 ?Kids} {SolveAll proc {$ K} {Father _ K} end Kids} end The Oz solution uses _ to add a new existentially scoped variable. The Prolog solution, on the other hand, introduces a new syntactic concept, namely the "existential quantifier" X^, which only has meaning in terms of setof/3 and bagof/3. The fact that this notation denotes an existential quantifier is defined explicitly in the Prolog semantics. The Oz solution, on the other hand, requires no new semantics. In addition to doing all-solutions bagof/3, Oz programs can do a lazy bagof/3, i.e., where each new solution is calculated on demand. Lazy bagof/3 is done by Solve, which returns a lazy list of solutions. \n Deterministic logic programming Writing a logic program in Prolog or another logic language consists in defining the logical semantics and then choosing an operational semantics that gives a satisfactory efficiency. This follows Kowalski\'s equation "Algorithm = Logic + Control". Logic and control need to be balanced. The art of logic programming consists in balancing two conflicting tensions: the logical semantics should be simple and the operational semantics should be efficient. When done well, this gives an elegant style in Prolog  [6] . Oz supports this kind of program design in logic programming by supporting both deterministic and nondeterministic control flow. For example, in Prolog we can define a list append predicate as follows: append([], L2, L2). append([X|M1], L2, [X|M3]) :-append(M1, L2, M3). This definition follows Prolog\'s operational semantics. Compilers can optimize this to make it deterministic in certain cases, for example if the first argument is bound to a list. In Oz we can define the operational semantics more precisely. Let us show three ways that the append can be defined in Oz. \n Nondeterministic append In our first definition, we define the append nondeterministically: proc {Append L1 L2 L3} choice L1=nil L3=L2 [] X M1 M3 in L1=X|M1 L3=X|M3 {Append M1 L2 M3} end end This has the same operational semantics as Prolog. \n Deterministic append (first version) We give another definition of append that has the same logical semantics as before but a deterministic operational semantics: fun {Append A B} case A of nil then B [] X|As then X|{Append As B} end end In this case, argument A is bound to a list so execution is directional. Oz allows to use a functional syntax for such definitions. \n Deterministic append (second version) We give yet another definition that again has the same logical semantics but a second deterministic operational semantics: fun {Append B C} if B==C then nil else case C of X|Cs then X|{Append B Cs} end end end This version of Append takes the last two arguments as inputs and returns the first argument as its output. For example, {Append [3] [1 2 3]} returns [1 2]. Correct execution requires that the second argument is a suffix of the third argument. \n Lazy concurrent functional programming An important insight of the Oz project was that the logic programming core of Oz can also support functional programming paradigms. We show this in four steps: • Functional programming with values. Widely used functional programming languages, such as Scheme or Haskell, compute with values. This can be done in Oz simply by not using unbound logic variables. To support higher-order programming, we extend the Oz computation model with function values. Variables can be bound to function values, giving traditional eager functional programming. • Functional programming with logic variables. We add logic variables to functional programming with values. The bind operation is unification. This gives exactly the deterministic logic programming paradigm of Section 3. This paradigm has a surprising benefit: many more function definitions become tail-recursive. • Deterministic dataflow. We add concurrency to functional programming with logic variables. We allow any statement to execute in its own thread where threads synchronize on variable binding. This is exactly the synchronization model of concurrent logic programming. Compared to concurrent logic programming, however, we are still purely functional. • Lazy deterministic dataflow. The final extension adds on-demand computation. We add a new synchronization operation that waits until another thread waits on a variable being bound. With this new primitive operation (called WaitNeeded), we have added all the power of lazy evaluation to deterministic dataflow. We examine these four steps in more detail. \n Functional programming with values We extend the computation model with function values. We allow variables to be bound to lexically scoped closures. This gives pure functional programming with eager evaluation. Pure functional programming is the foundation of higher-order programming, which underlies most of the development of data abstraction, such as object-oriented programming, abstract data types, components, templates, metaclasses, and so forth. In addition to being a foundation for data abstraction, this model has strong formal properties, which are different from but analogous to the strong properties of logic programming. The main property is known as the Church-Rosser property, often referred to as confluence, which states that the final result of an expression reduction is independent of the reduction choices made during the reduction. In what follows, we show how Oz exploits the synergy that is obtained by combining confluence with logic variables. \n Functional programming with logic variables We now extend functional programming with logic variables. This corresponds exactly to the deterministic logic programming of Section 3, so this form of functional programming is in fact doing logic programming. What have we gained from this extension? First of all, it is now possible to write functional programs with partially instantiated data structures, just like in Prolog. But we have gained much more than just the use of partial data structures. Allowing unbound logic variables makes possible reduction orders that are impossible in a functional language based on values only. For example, the two deterministic append predicates defined in Section 3 are both tail-recursive, which is not possible in a functional language based on values without doing complex program transformation. Let us examine precisely how adding logic variables makes functions tailrecursive. We start with the first deterministic append from the previous section. Section 3 defines it syntactically as a function, but in Oz functions are just syntactic sugar for procedures. The deterministic append is actually defined as a procedure: proc {Append A B C} case A of nil then C=B [] X|As then Cs in C=X|Cs {Append As B Cs} end end Look at the recursive call, {Append As B Cs}. This call comes after the binding C=X|Cs which incrementally builds the output of the append. C is bound to a new cons cell consisting of X paired with a new unbound variable Cs. The recursive call can be a tail call because we pass it the unbound variable Cs. This is not possible in functional programming with values only. \n Deterministic dataflow We extend the previous paradigm by adding threads and using logic variables for synchronization. To understand this paradigm, we first explain the dataflow behavior of logic variables. Assume we declare a logic variable X. In a first thread, we do X=10. In a second thread, we do Y=X+1. Because of the scheduler, we do not know which thread will run first. The second thread, if it is scheduled first, will suspend when it attempts to do the addition. It will wait until X is bound. When the first thread binds X, then the second thread becomes runnable again and the addition will complete, binding Y to 11. To summarize, this is a declarative form of dataflow with two basic operations, namely binding a logic variable and waiting until the variable is bound. Dataflow synchronization is such an important use of logic variables that we call them dataflow variables when we explain the history of Oz  [13] . The Oz community did not invent this; it was pioneered in concurrent logic programming, which was an early development of Prolog for concurrent systems  [2, 8] . The Oz contributions to this concept are its use for purely functional concurrent and distributed computing and its extension to lazy evaluation. In deterministic dataflow, any instruction can execute in its own thread. This lets us define networks of concurrent agents that communicate through streams, where we define a stream as a list that is built incrementally and that may have an unbound tail. For example, consider the following sequential functional program: This computes a list of successive integers, squares each element of this list, and displays the result. To follow the execution during an interactive session, we have slowed down the generation so it takes one second per element. We can make this concurrent by doing the generation and mapping in their own threads: thread Xs={Gen 1 10} end thread Ys={Map Xs fun {$ X} X * X end} end {Browse Ys} This uses the thread statement to create new threads. What is the difference between the concurrent and the sequential versions? The result of the calculation is the same in both cases, namely [1 4 9 16 ... 81 100]. So what is different? In the sequential version, Gen calculates the whole list before Map starts. The final result is displayed all at once when the calculation is complete, which happens after ten seconds. In the concurrent version, Gen and Map both execute concurrently. Whenever Gen adds an element to its list, Map will immediately calculate its square before the next element is added. The result is displayed incrementally as the elements are generated, one element each second. Concurrency has converted a batch computation into an incremental computation without changing the functional semantics. The concurrent executions of Gen and Map can be considered as functional agents, where we define an agent as a concurrent computation that reads zero or more input streams and writes zero or more output streams. Because of logic variables, both Gen and Map are tail-recursive. This means that the agents execute with constant stack size. This justifies calling these computations "agents". The concurrent execution is memory efficient as well as being purely functional. This shows clearly the synergy between logic variables and concurrency. \n Lazy deterministic dataflow The final extension adds the ability to do lazy evaluation to deterministic dataflow. In the previous section, we explained how logic variables are used for dataflow synchronization. We now extend this dataflow paradigm to do lazy evaluation. We do this by adding one new operation, WaitNeeded, which does by-need synchronization on logic variables. The operation {WaitNeeded X} suspends the current thread if X is unbound and no other thread is waiting for X to be bound. Otherwise, if another thread is suspended on X or if X is bound, the operation succeeds. We say that WaitNeeded "waits until X is needed". Adding this single operation to the language lets us fully define lazy evaluation. Oz introduces a syntax sugar to make this easy for the programmer. For example, we define the function {Ints N} that returns a lazy list of successive integers starting with N: fun lazy {Ints N} N|{Ints N+1} end The "lazy" annotation means that the function does lazy evaluation. It is syntactic sugar for the following procedure: proc {Ints N R} thread {WaitNeeded R} R=N|{Ints N+1} end end Calling S={Ints 1} will suspend in the WaitNeeded call until another thread needs the first element of S to run. When this happens, the WaitNeeded call succeeds and R is bound to a list with one new element. The recursive call of Ints continues this behavior for the next elements. We note that the compiler can optimize the above definition by reusing the thread, to avoid the creation of a new thread for each recursive call. The combination of lazy evaluation and concurrency has been known at least since 1977  [5] , but as far as we know, the definition of lazy evaluation in a dataflow paradigm and its connection to logic programming are both original with Oz. Summary We summarize the paradigms introduced in this section. We start with a first-order paradigm that computes with values. We extend this paradigm in four steps. First, we add function values, i.e., lexically scoped closures, which gives pure functional programming with eager evaluation. Second, we add logic variables, which gives deterministic logic programming as seen in Section 3. A side benefit is that functions that compute recursive data structures such as lists become tail-recursive. Third, we add threads and use the dataflow behavior of logic variables to do synchronization, which gives deterministic dataflow. This supports concurrent networks of functional agents that communicate through shared lists used as communication channels, which we call streams. Note that because list functions are tail-recursive, these agents use constant stack space. Fourth, in the final step, we add by-need synchronization using WaitNeeded. This gives a paradigm that supports both lazy evaluation and concurrency, which we call lazy deterministic dataflow. All four paradigms keep the strong confluence properties of pure functional programming, as well as being conservative extensions to a language supporting deterministic logic programming. Since Prolog is identical to a subset of Oz, we propose that some form of these extensions could become part of a future evolution of Prolog. \n Purely functional distributed computing The Oz research community realized early on that the Oz language design would be a good starting point for building a distributed programming system. Because the design cleanly separates immutable, dataflow, and mutable language entities, it would be possible to use a deep embedding approach, where each language entity would be implemented with its own distributed algorithm. The distribution support was part of the Mozart system at its first release in 1999. This design had many important innovations including the following: • Deep embedding. Each language concept was implemented with a distributed algorithm. This means that applications written for one distribution structure can easily be ported to another distribution structure without changing the source code. The only differences are failure behavior and timing. Failure behavior can be handled in a modular way, inspired by techniques from Erlang  [3, 1] . • Distributed rational tree unification. The concurrent examples of Section 4 can all be run with unchanged source code if distributed across different compute nodes. Concurrent pipelines become asynchronous distributed pipelines, while maintaining the semantics of pure functional programming. A key part of this system is the distributed rational tree unification algorithm. The first formal specification and proof of distributed unification was done during this work and published in  [4] . • Global references, distributed lexical scoping, distributed garbage collection, and migratory state. These are some of the other properties of this design. In the rest of this section we will focus on what we consider to be the most significant discovery that was made in the work on distributed computing for Oz. This discovery is intimately tied to the logic programming origins of Oz and it may open a window of opportunity for Prolog. Asynchronous message passing can be pure The deterministic dataflow paradigm and its lazy extension can be implemented in a distributed setting. A dataflow variable can be read on one compute node and bound on another node. This does a distributed synchronization, which in its general form is implemented by distributed unification. The salient property of this distributed synchronization is that it enjoys all the strong properties of pure functional programming. Specifically, it means that asynchronous message passing between compute nodes can be completely pure. This property remains mostly unknown in the distributed computing community. In our experience as members of this community, we observe that many often insist that asynchronous message passing is intrinsically impure, which is false. Overuse of nondeterminism Most of today\'s large distributed systems do not take advantage of this property. In fact, the contrary is true: today\'s systems greatly overuse nondeterminism. This is one of the main reasons why it is still difficult to build and debug such systems. Large distributed systems, such as used daily by Google, Facebook, Twitter, and so on, must be continually babysitted by experts. This is not an inevitable property of such systems. In our view, it is mainly due to the massive overuse of nondeterminism in the design of these systems. Every single library that is a part of such a system and that has its own API, which is true of most libraries, is a point of nondeterminism. Purely functional distributed computing The problem of overuse of nondeterminism and a possible solution are explained in a keynote talk given by one of the authors at CodeBEAM 2019, Why Time is Evil in Distributed Systems and What To Do About It  [11] . The solution we propose is to use deterministic dataflow programming as the default paradigm for distributed systems and to add nondeterminism only where it is needed and nowhere else. With this solution, most distributed systems become mostly functional with a limited use of nondeterminism. This greatly simplifies their development, maintenance, and evolution. We observe that this solution is intimately tied to logic programming, since the basic concept that makes it work is the dataflow variable, which is simply a logic variable used in a concurrent setting. \n Role of logic programming We believe that an appropriate extension of Prolog or another logic language can potentially solve this problem and play a significant role in distributed computing. If such an extension is not done by the logic programing community, then the distributed computing community will reinvent concepts from logic programming as it solves these problems. This reinvention has already started with the development of CRDTs (Conflict-Free Replicated Data Types), which have monotonic properties and can replace consensus algorithms, which are nondeterministic, by purely functional operations in many cases  [9] . \n Conclusions Prolog has enjoyed a relatively large popularity since its conception in 1972 up to the present day, unlike Oz, which despite having many innovations at its release in 1999, has seen a decline since 2009 and is only used today for education. Oz declined for sociological reasons that have nothing to do with its innovations; in fact we observe that the innovations are all becoming adopted in modern programming systems despite the decline of Oz. As members of both the Prolog and Oz communities, we see this as an opportunity to help Prolog evolve in order to maintain its popularity. This paper presents three innovations of Oz, namely deterministic logic programming, lazy concurrent functional programming, and purely functional distributed computing. We propose that it would be straightforward to add these innovations to Prolog because the core logic languages of Oz and Prolog are quite similar. Most of pure Prolog has a direct syntactic translation to Oz. This means that the hard work of formulating and understanding these innovations in a logic programming context has already been done. We hope that the ideas presented in this paper will help the future of Prolog as well as help Oz regain some recognition as a source of innovation. fun {Gen L H} {Delay 1000} % Suspend thread execution for 1000 ms if L>H then nil else L|{Gen L+1 H} end end Xs={Gen 1 10} Ys={Map Xs fun {$ X} X * X end} {Browse Ys} \n else fail end end Consider the following deterministic Prolog predicate: place_queens(I, _, _, _) :-I==0, !. place_queens(I, Cs, Us, [_|Ds]) :- I>0, J is I-1, place_queens(J, Cs, [_|Us], Ds), place_queen(I, Cs, Us, Ds). proc {PlaceQueens N Cs Us Ds} if N==0 then skip elseif N>0 then Ds2 Us2=_|Us in Ds=_|Ds2 {PlaceQueens N-1 Cs Us2 Ds2} {PlaceQueen N Cs Us Ds} This predicate has a blue cut according to O\'Keefe [6] , i.e., the cut is needed to inform naive implementations that the predicate is deterministic, so they can improve efficiency, but it does not change the program\'s results. The predicate place_queens/4 is translated into the following Oz code:', acknowledgement=None, annex=None)
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Gabriele Monaco', given_name='Gabriele', middle_name=None, surname='Monaco', email='monaco@eit.uni-kl.de', orcid=None, affiliation=GrobidAffiliation(institution='Technische Universität Kaiserslautern', department=None, laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement=None, country='Germany'))), GrobidAuthor(full_name='Gautam Gala', given_name='Gautam', middle_name=None, surname='Gala', email='gala@eit.uni-kl.de', orcid=None, affiliation=GrobidAffiliation(institution='Technische Universität Kaiserslautern', department=None, laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement=None, country='Germany'))), GrobidAuthor(full_name='Gerhard Fohler', given_name='Gerhard', middle_name=None, surname='Fohler', email='fohler@eit.uni-kl.de', orcid=None, affiliation=GrobidAffiliation(institution='Technische Universität Kaiserslautern', department=None, laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement=None, country='Germany')))], index=None, id=None, unstructured=None, date=None, title='Extensions for Shared Resource Orchestration in Kubernetes to Support RT-Cloud Containers', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), pdf_md5='20DC0843DC5F2B48E08AFA6715DAAAA2', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='Ericsson', given_name=None, middle_name=None, surname='Ericsson', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date=None, title='Network Compute Fabric', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='Avail-able:www.ericsson.com/en/edge-computing/network-compute-fabric'), GrobidBiblio(authors=[GrobidAuthor(full_name='G Gala', given_name='G', middle_name=None, surname='Gala', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Fohler', given_name='G', middle_name=None, surname='Fohler', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Tummeltshammer', given_name='P', middle_name=None, surname='Tummeltshammer', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Resch', given_name='S', middle_name=None, surname='Resch', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Hametner', given_name='R', middle_name=None, surname='Hametner', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='2021', title='RT-cloud: Virtualization technologies and cloud computing for railway use-case', book_title='IEEE 24th International Symposium on Real-Time Distributed Computing (ISORC)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='105-113', first_page='105', last_page='113', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G Gala', given_name='G', middle_name=None, surname='Gala', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Castillo', given_name='J', middle_name=None, surname='Castillo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Fohler', given_name='G', middle_name=None, surname='Fohler', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date='2021', title='Work-in-progress: Cloud computing for time-triggered safety-critical systems', book_title='IEEE Real-Time Systems Symposium (RTSS)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[], index=3, id='b3', unstructured=None, date='2022-03-30', title='Cloud Native Computing Foundation', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='What is kubernetes?', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/'), GrobidBiblio(authors=[GrobidAuthor(full_name='Inc Docker', given_name='Inc', middle_name=None, surname='Docker', email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='2022-05-03', title='Docker engine overview', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='https://docs.docker.com/engine/'), GrobidBiblio(authors=[GrobidAuthor(full_name='T Heo', given_name='T', middle_name=None, surname='Heo', email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date='2015', title='Control group v2', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2'), GrobidBiblio(authors=[GrobidAuthor(full_name='V Struhár', given_name='V', middle_name=None, surname='Struhár', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S S Craciunas', given_name='S', middle_name='S', surname='Craciunas', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Ashjaei', given_name='M', middle_name=None, surname='Ashjaei', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Behnam', given_name='M', middle_name=None, surname='Behnam', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A V Papadopoulos', given_name='A', middle_name='V', surname='Papadopoulos', email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date='2021', title='REACT: Enabling real-time container orchestration', book_title='26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1-8', first_page='1', last_page='8', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Cinque', given_name='M', middle_name=None, surname='Cinque', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R D Corte', given_name='R', middle_name='D', surname='Corte', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Eliso', given_name='A', middle_name=None, surname='Eliso', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Pecchia', given_name='A', middle_name=None, surname='Pecchia', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date='2019', title='RT-CASEs: Containerbased virtualization for temporally separated mixed-criticality task sets', book_title='ECRTS', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C Xu', given_name='C', middle_name=None, surname='Xu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Rajamani', given_name='K', middle_name=None, surname='Rajamani', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Felter', given_name='W', middle_name=None, surname='Felter', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date='2018', title='NBWGuard: Realizing network QoS for kubernetes', book_title='Proceedings of the 19th International Middleware Conference Industry', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T.-A Yeh', given_name='T.-A', middle_name=None, surname='Yeh', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H.-H Chen', given_name='H.-H', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J C', given_name='J', middle_name='C', surname=None, email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='-Y Chou', given_name='-Y', middle_name=None, surname='Chou', email=None, orcid=None, affiliation=None)], index=9, id='b9', unstructured=None, date='2020', title='KubeShare: A framework to manage GPUs as first-class and shared resources in container cloud', book_title='Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Guerra', given_name='R', middle_name=None, surname='Guerra', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Arzen', given_name='K', middle_name=None, surname='Arzen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Fohler', given_name='G', middle_name=None, surname='Fohler', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Bini', given_name='E', middle_name=None, surname='Bini', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V R Segovia', given_name='V', middle_name='R', surname='Segovia', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Buttazzo', given_name='G', middle_name=None, surname='Buttazzo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Scordino', given_name='C', middle_name=None, surname='Scordino', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Schorr', given_name='S', middle_name=None, surname='Schorr', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Eker', given_name='J', middle_name=None, surname='Eker', email=None, orcid=None, affiliation=None)], index=10, id='b10', unstructured=None, date='2011-05', title='Resource management on multicore systems: The ACTORS approach', book_title=None, series_title=None, editors=None, journal='IEEE Micro', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='31', issue='03', pages='72-81', first_page='72', last_page='81', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G Fohler', given_name='G', middle_name=None, surname='Fohler', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Gala', given_name='G', middle_name=None, surname='Gala', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D G Pérez', given_name='D', middle_name='G', surname='Pérez', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Pagetti', given_name='C', middle_name=None, surname='Pagetti', email=None, orcid=None, affiliation=None)], index=11, id='b11', unstructured=None, date='2018', title='Evaluation of DREAMS resource management solutions on a mixed-critical demonstrator', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G Gala', given_name='G', middle_name=None, surname='Gala', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Fohler', given_name='G', middle_name=None, surname='Fohler', email=None, orcid=None, affiliation=None)], index=12, id='b12', unstructured=None, date='2020', title='Distributed decision-making for safe and secure global resource management via blockchain: Work-in-progress', book_title='2020 International Conference on Embedded Software (EMSOFT)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='28-30', first_page='28', last_page='30', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Z Wang', given_name='Z', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Goudarzi', given_name='M', middle_name=None, surname='Goudarzi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Aryal', given_name='J', middle_name=None, surname='Aryal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Buyya', given_name='R', middle_name=None, surname='Buyya', email=None, orcid=None, affiliation=None)], index=13, id='b13', unstructured=None, date='2022', title='Container orchestration in edge and fog computing environments for real-time iot applications', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Fiori', given_name='S', middle_name=None, surname='Fiori', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Abeni', given_name='L', middle_name=None, surname='Abeni', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Cucinotta', given_name='T', middle_name=None, surname='Cucinotta', email=None, orcid=None, affiliation=None)], index=14, id='b14', unstructured=None, date='2022', title='Rt-kubernetes: containerized realtime cloud computing', book_title='Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V Struhár', given_name='V', middle_name=None, surname='Struhár', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Behnam', given_name='M', middle_name=None, surname='Behnam', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Ashjaei', given_name='M', middle_name=None, surname='Ashjaei', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A V Papadopoulos', given_name='A', middle_name='V', surname='Papadopoulos', email=None, orcid=None, affiliation=None)], index=15, id='b15', unstructured=None, date='2020', title='Real-time containers: A survey', book_title='Fog-IoT', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Cinque', given_name='M', middle_name=None, surname='Cinque', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R D Corte', given_name='R', middle_name='D', surname='Corte', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Ruggiero', given_name='R', middle_name=None, surname='Ruggiero', email=None, orcid=None, affiliation=None)], index=16, id='b16', unstructured=None, date='2021', title='Preventing timing failures in mixed-criticality clouds with dynamic real-time containers', book_title='17th European Dependable Computing Conference (EDCC)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='17-24', first_page='17', last_page='24', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L Abeni', given_name='L', middle_name=None, surname='Abeni', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Balsini', given_name='A', middle_name=None, surname='Balsini', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Cucinotta', given_name='T', middle_name=None, surname='Cucinotta', email=None, orcid=None, affiliation=None)], index=17, id='b17', unstructured=None, date='2019', title='Container-based real-time scheduling in the linux kernel', book_title=None, series_title=None, editors=None, journal='SIGBED Rev', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='16', issue=None, pages='33-38', first_page='33', last_page='38', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Cucinotta', given_name='T', middle_name=None, surname='Cucinotta', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Abeni', given_name='L', middle_name=None, surname='Abeni', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Marinoni', given_name='M', middle_name=None, surname='Marinoni', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Mancini', given_name='R', middle_name=None, surname='Mancini', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vitucci', given_name='C', middle_name=None, surname='Vitucci', email=None, orcid=None, affiliation=None)], index=18, id='b18', unstructured=None, date='2021', title='Strong temporal isolation among containers in openstack for nfv services', book_title=None, series_title=None, editors=None, journal='IEEE Transactions on Cloud Computing', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Yun', given_name='H', middle_name=None, surname='Yun', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Yao', given_name='G', middle_name=None, surname='Yao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Pellizzoni', given_name='R', middle_name=None, surname='Pellizzoni', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Caccamo', given_name='M', middle_name=None, surname='Caccamo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L R Sha', given_name='L', middle_name='R', surname='Sha', email=None, orcid=None, affiliation=None)], index=19, id='b19', unstructured=None, date='2013', title='MemGuard: Memory bandwidth reservation system for efficient performance isolation in multi-core platforms', book_title='IEEE 19th Real-Time and Embedded Technology and Applications Symposium (RTAS)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='55-64', first_page='55', last_page='64', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None)], abstract='Industries are considering the adoption of cloud computing for real-time applications due to current improvements in network latencies and the advent of Fog and Edge computing. To create an RT-cloud capable of hosting real-time applications, it is increasingly significant to improve the entire stack, including containerization of applications, and their deployment and orchestration across nodes. However, state-of-the-art orchestrators (e.g., Kubernetes) and underlying container engines are designed for general purpose applications. They ignore orchestration and management of shared resources (e.g. memory bandwidth, cache, shared-interconnect) making them unsuitable for use with an RT-cloud. Taking inspiration from existing resource management architectures for multicore nodes, such as ACTORS, and for distributed mixed-criticality systems, such as the DREAMS, we propose a series of extensions in the way shared resources are orchestrated by Kubernetes and managed by the underlying Linux layers. Our approach allows fine-grained monitoring and allocation of low-level shared resources on nodes to provide better isolation to real-time containers and supports dynamic orchestration and balancing of containers across the nodes based on the availability and demand of shared resources.', body="I. INTRODUCTION As seen in Ericson's Computing Fabric  [1] , and EU SE-CREDAS project  [2] , industries are exploring cloud computing for real-time applications to benefit from ease of re-usability, maintainability, and reconfiguration while providing workload elasticity and higher availability. However, the adoption of cloud computing for real-time applications is hampered by performance uncertainties as the current cloud models are not designed to guarantee worst case requirements. Some of the major hurdles are achieving high resource utilization in the cloud while providing timing guarantees and low interference between users, and supporting dynamic workloads and fluctuations in infrastructure availability. Previous work, such as  [2] ,  [3] , have proposed using private RT-clouds with virtualization solutions and resource management extensions to host realtime and best-effort virtual machines in the cloud. However, virtualization solutions tend to have high overheads. In contrast, the overhead of containers compared to direct execution on top of the operating system is extremely low, making them a lucrative option for real-time applications. Containers are becoming the de facto industrial approach, especially in cloud environments as they provide an ideal underlying layer for edge-to-cloud and multi-cloud scenarios. Organizations leverage containers to use multiple public cloud providers as well as their on-premises cloud. Container orchestrators, such as Kubernetes (K8s)  [4] , manage, deploy and scale containers across clusters of physical and virtual nodes. Orchestrators allow easy deployment of containers by properly setting up their runtime and monitoring their execution without the requirement of a cumbersome and error-prone manual process. In cloud environments, containers are often deployed via such orchestrators, managing certain resources, such as CPU-time and memory space, and delivering acceptable Quality-of-Service (QoS) to different users, typically based on their subscription plans. Based on the user configuration, orchestrators deploy containers to balance the usage of certain resources across the cluster nodes and guarantee these resources to the containers. Moreover, orchestrators can be configured to have container replicas, which in case of node failures, can be deployed on different nodes. Orchestration of cloud resources is gaining relevance for the real-time systems, especially due to the advent of Fog and Edge computing, which brings the nodes closer to the endusers, thus, considerably lowering the latencies and enabling time-sensitive containers in the cloud. Orchestrators can help configure containers to exploit resource isolation possibilities provided by the underlying hardware and software layers to support meeting real-time requirements. However, widely used resource orchestrators (e.g., K8s) and the underlying Linux container technology (e.g. Docker  [5] ) and resource management approaches (e.g. Cgroups  [6] ) are not designed to consider strong shared resource isolation and end-to-end guarantees. They typically aim to improve average-case performance without regard for the worst case. Thus, the level of isolation provided by containerization and supported by container orchestrators is not adequate for real-time systems. Although optimizations and patches to reduce latencies are available, it is challenging to meet end-to-end constraints by considering only temporal allocation of CPU and spatial allocation of memory. A system with dynamically changing availability and requirement of resources requires the orchestrators to be aware of other resources of the node (e.g., memory bandwidth, cache, and shared interconnect) and services running in the container. The orchestrators must coordinate system-wide resources and adapt the QoS of services in the containers to the current resource availability (and not suspend containers, even the best effort ones, as far as possible). Existing cloud orchestrators (e.g. K8s) work well when considering temporal allocation of CPU and spatial allocation of memory space, but do not support other shared resources. However, they require several extensions to ensure strong shared resource isolation and end-to-end guarantees under varying operating conditions. Existing works, such as  [7] , enable some real-time capabilities in Kubernetes and underlying technologies. Cinque et al.  [8]  enable containers and the Linux kernel to react with real-time performance. Such works aim to prevent over-reservation of CPU time to containers, indirectly targeting the weak isolation intrinsic to containers. Other works, such as  [9] ,  [10] , exploit the modularity of Kubernetes to extend its capabilities over more resource types (e.g. network bandwidth), allowing better resource allocations decisions, but still do not consider allocation and monitoring of shared resources. Contrarily, our solution considers the allocation of shared resources and relies on low overhead run-time monitoring of hardware and software at various levels to obtain a system-wide view of availability of resources and current/predicted demand of containers, allowing the orchestrator to meet the end-to-end requirements of real-time containers and help achieve the best possible QoS by best-effort containers. In this paper, we consider Kubernetes as the currently most popular container orchestration technology  [4] , We take inspiration from existing resource management architecture for multicore systems, such as ACTORS framework  [11] , and for distributed mixed-criticality systems, such as the DREAMS  [12] , to extend it. The existing ACTORS framework only considers a single multicore node, while DREAMS framework considers offline allocation of shared resources to virtual machine in a distributed system. However, offline allocation of resources is unrealistic in modern clouds. In fact, clouds require orchestrators to dynamically provide resource and maximise their utilization, and add new containers or provide scalability to existing ones at run-time. We build upon the DREAMS nodelevel (local) and system-wide (global) resource managers to extend the dynamic resource orchestration capabilities of Kubernetes while still ensuring strong resource isolation and fault tolerance. We aim to add configurable monitoring capabilities to Kubernetes at various levels ranging from low-level hardware signals (e.g., performance monitor counter events), operating system level events, and events in the containers, so that the orchestrator can keep a system-wide view of availability of shared resources and current/predicted demands of services running in the containers. Based on this system-wide view, the orchestrator can allocate resources to new containers or reallocate resources to existing containers. The aim is to support real-time applications in clouds and ensure that their resource demands are promptly met while best-effort containers achieve the best possible QoS. Section II introduces the DREAMS resource management and the standard resource orchestration in Kubernetes. Section III illustrates previous works that extended Kubernetes and Linux containers to support real-time containers. Section IV provides a series of enhancements for the orchestrator and the underlying layers to orchestrate and manage real-time containers together with best-effort containers. We describe how these enhancements can be applied in Kubernetes (with Linux) and finally conclude in Section V, providing future research and implementation directions. \n II. BACKGROUND \n A. DREAMS resource management architecture We identified the resource management architecture delineated by the DREAMS project as a good base for creating extensions to the Kubernetes container orchestrator and the underlying Linux layers for supporting RT-Cloud. Several works are based on and extend this architecture  [13] ,  [3] ; hence we can take benefit of those as well. The resource management architecture delivers faulttolerance and ensures real-time guarantees in mixed criticality distributed systems, where each node is capable of running multiple real-time and best-effort VMs. It attempts to provide best-effort applications with the best QoS level without compromising real-time guarantees in the system. The DREAMS architecture decouples the system-wide orchestration decision making from the fine-grained resource management on the nodes. Each node runs a Local Resource Manager (LRM), capable of monitoring and scheduling the resources of the node. Furthermore, a Global Resource Manager (GRM) controls and coordinates the LRMs. The GRM takes decisions related to the entire system. By knowing the abstract sysem status on each node via the LRMs, the GRM can react to failures by reconfiguring nodes, and improves the resource balancing when overloaded situation occurs in a node. The capabilities of LRMs are split among the two subcomponents: monitors (MON) and Local Resource Schedulers (LRSs). The MON is responsible for observing the utilization of resources, and status and health of applications and resources. Examples of MONs are core failure monitor or performance monitoring event for CPU cache misses. The LRS schedules and controls resources accesses from applications on the nodes. Examples of LRS are CPU scheduler, network bandwidth regulators or techniques such as cache partitioning. \n B. Kubernetes Kubernetes is highly configurable. We can extend or even replace components without patching the original codebase. In this section, we will introduce the default resource management. During configuration, each container can specify resource requests and limits. Requests are used for scheduling: containers will not be deployed to nodes that cannot guarantee the resource, based on declared requests of other containers. Limits are usually enforced with Linux cgroups, to avoid overbooking (use of more peak resources than requested). This optimistic approach relies on resources being used less than their requests, allowing containers to use more than their requests and effectively allocating more resources than available. A more predictable scenario uses limits to avoid this situation (overbooking) from negatively impacting critical tasks. Natively available resources are CPU and memory space. Providers can define the so-called extended resources, designed for resources like external devices (GPUs, FPGAs, etc.), whose number is discrete and which cannot easily be shared. Specific device plugins can bridge the actual driver with Kubernetes, initializing devices and providing the orchestrator with useful information about the status and availability of the nodes. Containers requesting a device will be granted full access to it until all available devices have been requested. A device is hence consumed if a container requests it in its specifications. Due to this architecture, the system does not enforce limits: the requested amount of resources is all the containers can get, regardless of their actual usage. \n III. RELATED WORKS Works in the literature employ Kubernetes with performanceconstrained IoT devices as  [14] , where the authors integrated a light version of the orchestrator (K3s) with the FogBus2 framework. Struhár et al.  [7]  enriched Kubernetes' support for real-time hierarchical scheduling on nodes via Linux kernel patches. They tackle the poor isolation of containers by monitoring the execution of tasks (e.g., deadline misses and lateness) and adjusting with future scheduling decisions. Similarly, Fiori et al.  [15]  modified Kubernetes for awareness of realtime scheduling parameters on deployment. Parameters such as runtime on rt-cpus per period are checked during admission control and the task scheduler is configured accordingly. Xu et al.  [9]  implemented the network bandwidth as a resource in Kubernetes. The authors react on pod deployment and control the Linux kernel stack according to specific rules. By creating some differentiated queues on virtual interfaces, they can give guarantees and limit overbooked resources, still keeping the Kubernetes handling unmodified. Yeh et al.  [10]  extend the GPUs handling in Kubernetes. They create an additional controller that schedules the GPUs based on user requirements. This can set time and memory (just like CPU), the parallel scheduler ensures minimal fragmentation while sharing devices across containers. Struhár et al.  [16]  assess whether and how Linux containers can deliver real-time performances and identify gaps regarding their deployment in safety-critical scenarios. They identify three categories of ways to achieve this: PREEMPT_RT for Linux Kernel to improve its responsiveness, exploiting a real-time co-kernel running alongside Linux and scheduling with improved guarantees the real-time containers, and hierarchical scheduling of containers and their internal tasks. Cinque et al.  [8]  propose an implementation scheme for real-time-enabled containers running on a Linux kernel with the co-kernel patch RTAI. Their implementation enforces temporal separation among containers by using a fixed-priority scheduler, run-time execution monitoring and mitigation. In case of misbehaviour, they provide policies to prevent interference from lower priority containers. In  [17] , they improve the previous work with a dynamic EDF scheduler for more flexibility. Abeni et al.  [18]  based their work on hierarchical scheduling, patching the kernel to allow 2 scheduling levels, for both LXC containers and their tasks. The authors instantiate their work and implement supporting facilities in the OpenStack orchestrator  [19] , configuring their hierarchical scheduler from a modified version of the orchestrator to implement Network Function Virtualization services. \n IV. EXTENDING KUBERNETES AND LINUX CONTAINERS TO SUPPORT REAL-TIME APPLICATIONS Kubernetes is designed to be extensible, it provides a default configuration and implementation of most of the required components, but it isn't limited to it. System designers can reimplement the pod scheduler to substitute or even assist the default scheduler's implementation. This allows extensions without the need to restructure nor patch the orchestrator. Due to its modularity, it relies on certain components, the Linux containers and the related Control Groups, to name a few. Such systems may not be ready to target real-time applications, but their improvement is transparent to Kubernetes. Orchestration systems work on networks of distributed nodes, deployment latencies are hence typically high by design. However, for real-time tasks to properly work, the run-time (after deployment) must be interfered with as little as possible. Kubernetes relies on the underlining OS for most of the runtime checks and operations (e.g. task scheduling or resource control). We can therefore achieve real-time guarantees by both having each node real-time capable (i.e. running a real-time capable OS) and making the orchestrator aware of the relevant resources to consider during deployment. In the following subsections, we will discuss some points where the orchestrator and its stack can be improved or extended, giving some possible implementation and design guidelines on how to do it.   \n A. Resource monitoring Most virtualization and orchestration solutions used in the cloud already support some resource monitoring and regulation, accounting for runtime usage and preventing tasks to use more than reserved. Nevertheless, the considered items are usually only CPU time and memory space. Due to the shared nature of resources such as CPU cache, memory and bus, multicore architectures typically suffer from interferences. Those shared resources are generally ignored by the tools currently used in the cloud. Works such as Memguard  [20]  propose a way to regulate the memory accesses by relying on data provided by the hardware Performance Monitoring Units (PMUs). Such units are widely supported on modern processor architectures and typically allow several low-level events to be counted. Raw monitoring data can be retrieved natively or with tools such as Linux perf. We hence employ the hardware PMU to monitor multiple resources using the same principle, exploiting the architecture support, for instance, memory bandwidth usage, shared bus and processor cache. Raw values can be obtained with low overhead, as produced by hardware components, however, the values alone are not always meaningful, counted events can greatly vary, making the resulting response inaccurate Moreover, frequent reactions to changes introduce high overheads, negatively affecting the system. Consequently, we abstract the resource values in significant bands, previously defined per resource. As discrete numbers, bands are easier to handle and we can smooth out changes between them with filters. By properly stating rules for transitions among resource bands, we can consistently reduce the overhead due to reacting to state changes, still without losing generality or precision. Monitoring can be exploited to implement various features in Kubernetes and its orchestrator stack. Besides resource usage, monitoring instances can detect faults on components as well as on tasks and this knowledge can improve scheduling and orchestration decisions. Our monitoring solution is integrated into the Linux kernel and monitors separately each task. By following the scheduling events in the kernel, we can react to changes in the executing activity and we store values for each monitored process. Thereby, we have a separate view of each running task and we can enforce limits on a process basis. \n B. Shared resource isolation for containers Kubernetes relies on the container runtime to execute pods, however containers have weak isolation with respect to shared resources (memory, network or disk). As a lightweight alternative to Virtual Machines (VM), containers share the underlying kernel and OS. The simpler structure makes them even more vulnerable to interferences which can negatively and unpredictably affect the performance of critical tasks. Essentially containers are traditional Linux tasks with a set of common properties to group them and the use of namespaces to emulate isolation. We can monitor the execution of such tasks and assign resource budgets to such groups. The typical use case of pods/containers is to host what is logically a single application, It can include multiple processes and even multiple binaries, it is however conceptually a service. We model containers as tasks and the resource monitoring targets them rather than processes. By exploiting our monitoring facilities, we derive the resource usages of processes in containers, we then throttle the processes whose container is running out of predefined budgets and we communicate to the GRM to take proper action. The architecture for resource managers on nodes and containers is depicted in Figure  1 . Shared resource management is complementary to hierarchical scheduling used in previous works, as we cannot achieve strong timing guarantees without considering the impact of interferences on such resources. \n C. Integrating awareness of shared resources in Kubernetes The resources explicitly targeted by Kubernetes are memory and CPU, with possible extension to external devices. Our monitoring instances can provide an abstraction to various low level shared resources and with the use of filters can carefully distinguish between transient and continuous overload conditions. We designed the resource as divided into allocation levels, corresponding to the previously described resource bands. Pods can request a certain amount of levels, based on their need and the system will reserve them from the pool of available ones. We implement this type of management natively in Kubernetes using extended resources, however, the system does not allow different values for requests and limits for such resources. Nevertheless, we exploit annotations in the containers specifications to indicate whether the requirements are strict (for realtime priorities) or loose (best effort), for instance, which we handle in our module. If global scheduling decisions take into account metrics such as available memory bandwidth, the main overhead of the orchestrator extension is during deployment (startup), while the control run-time logic is fully relying on the monitoring instances on the nodes. Both real-time and best-effort containers must declare their resource requirements and/or limits, also regarding, for instance, memory bandwidth. It is hence a responsibility of the system designer to both know the total available memory bandwidth and how can it be shared among the tasks of the system. We provide different policies such as throttling containers on the node or rescheduling them to a different node, based on the status and availability of the cluster. The architecture for resource managers across the cluster is depicted in Figure  2 , our additional resource manager is shown in solid background. \n D. System-wide shared resource balancing Kubernetes allocates containers on nodes taking into account requested resources and nominal availability on nodes, once this check is passed, containers are assigned without further balancing or prioritization. Nodes can however receive a score after the feasible ones have been filtered. This value will be used to select the most suitable nodes for deployment, the scoring system is however not defined and we need to design it with a plugin to the scheduler. Kubernetes doesn't implement seamless rescheduling, simple pods are either allocated on a node or deleted from it, with no possibility to move them across nodes. The closest functionality Fig.  2 : Global and local resource managers with Kubernetes is to instantiate pods in controller structures (i.e. deployments), taking care of recreating the pod if terminated. Nonetheless, recreating evicted pods might imply losing the entire job executed by the pod, unless some architectural refactoring prevents this condition. Pods can be automatically evicted as resource consumption on nodes crosses certain thresholds, such as out of memory or storage conditions. Yet, this can only prevent OOM conditions and not to improve the cluster performances by balancing resource-intensive pods. Resources such as memory bandwidth are more likely to run out compared to storage, and their effect is usually more relevant for the performances of RT tasks. It is hence crucial to be able to keep a correct balance among nodes before overload conditions happen. We design different policies exploiting Kubernetes' scoring system, for instance, gathering best effort containers on the same nodes or prefer allocation on empty nodes for resourceintensive tasks and allocating tasks with lower resource requirements on busy nodes. We rely on live monitoring for further runtime checks and reorchestrate if the status is different from the predicted one or requested during deployment. Live monitoring can detect poorly configured containers and require readjustments before they unbalance the system. As rescheduling to a different node can result in high latencies due to network communication, we need to select with care the containers undergoing the process. Those can be less critical tasks or, collaboratively, tasks advertising their availability to migrate. \n E. System overload monitoring Kubernetes is primarily designed for resource usage maximisation rather than for guarantees to critical tasks. While this works during general usage, during overload, priority classes don't provide deterministic guarantees for real-time tasks, as limits enforcement and regulation can be slow. Kubernetes allows to either have all containers exposing same values for limits and requests, preventing overload by design, or better exploit the resources on the system but suffer from unpredictability. The first case is predictable as we set our requests to the worst case requirement. This can result in poor resource usage, as most applications don't consume their worst case requirements all the time. The less conservative approach allows the limit value to mirror the worst case usage, with requests corresponding only to best or average cases. Assuming the spikes in resource consumption on nodes don't occur at the same time, the system can use its resources more efficiently. Monitoring the status of resources during runtime we can identify overload situation in advance and enforce stricter regulations only then. We allow hence resource overload without penalties in the general case and dynamically restrict it only when it can affect critical tasks. \n F. Collaborative QoS reduction on overload The default policy of Kubernetes in overload situations is to throttle down tasks (in case of CPU) or kill them (if running out of memory). The likelihood of these measures depends on the pod's priority class, defined by the pattern of request/limits (e.g. if both values are equal, the priority is higher). Applications are unaware of this and cannot adapt preventively (e.g. by internally throttling their QoS) nor be reallocated To avoid uncontrolled degradation, we propose an interface for tasks to programmatically lower their resources footprint and readapt before the overload even occurs. We can implement this with scheduler extensions, starting a communication to tasks that request this functionality before corrective actions are taken, then if the overload condition is still present, the standard scheduler can take action. \n G. Dynamic resource orchestration Kubernetes provides resource requesting and limiting on pod creation, this cannot be changed dynamically as it is mainly relevant during allocation. Allowing limits higher than requests lowers the priority class. If critical applications change their requirement during their execution, they need either to have their requests mirroring the worst case or to join the Burstable class and have their resources guaranteed only in the best case. We design additional interfaces to allow applications to change their resource requirements at run-time, triggering all required rescheduling on the orchestrator's side. A secondary pod scheduler in Kubernetes can be used to implement such interfaces. \n H. Node priorities and orchestration in heterogeneous clusters The default Kubernetes scheduler assumes all nodes are equivalent and schedules new workloads on the first available node according to its requirements. As mentioned earlier, user defined scoring systems can leverage that by sorting preferred nodes in case multiple options are available. For example, static prioritisation can help in a cluster with nodes powered by different energy sources, the owners might prefer to run and reschedule on nodes powered by green energy whenever possible, using the others only as a secondary option. In a cluster with heterogeneous nodes, with some driven by power conservative but weaker processors and others with stronger computing capability, a scoring system can exploit the CPU requirements of each container and deploy them accordingly. We can enhance this pattern with live monitoring: gathering data from containers to understand which of them are either power intensive or need more CPU and reschedule them dynamically to optimize distribution in the aforementioned scenarios. \n I. Control for scheduling classes on nodes Kubernetes doesn't control the actual scheduling of tasks, just their assignment to nodes, then the OS scheduler will do the job. Some workloads, especially safety critical ones could take benefit from having a different kind of scheduling properties on nodes, such as a time triggered with slot shifting. The admission control of those pods can also compute a new scheduling table for the supported nodes and send it with the new task, ready to be scheduled in the end of the hyperperiod and with reduced overhead on the worker node. The nodes classes can be seen as a meta-resource by Kubernetes, which would activate a secondary OS scheduler for this task. V. CONCLUSION Shared resources are ignored by popular container engines and orchestrators, making them unsuitable for use with a real-time(RT)-cloud. In this paper, we proposed extensions to Kubernetes and the underlying container engine for shared resource orchestration and management to support containers of RT-cloud. Our extensions are inspired by the DREAMS and ACTORS resource management architectures. We provided configurable monitoring capabilities at various levels so that the orchestrator can keep a system-wide view of the availability of shared resources and current/predicted demands of services running in the containers. Based on this system-wide view, Kubernetes can now dynamically orchestrate resources to execute new containers or dynamically balance containers across the nodes based on the current availability and demand of the shared resources. Moreover, the extensions to the underlying Linux layers ensure strong isolation in shared resources. The overall goal of our extensions is to ensure that real-time applications meet their resource demands while best-effort applications achieve the best possible QoS. We follow the philosophy of Kubernetes of keeping all components modular and extensible. Our proposed design is transparent to different algorithms and strategies. Based on the requirements, system designers can select the monitoring and scheduling components that best fit their needs and plug them into our resource managers. Thus, we reduce the integration complexity without sacrificing flexibility and exploiting the full potential of Kubernetes. Future steps involve completing the implementation of the proposed extensions and deploying this extended Kubernetesbased RT-cloud with industrial use cases to assess and evaluate the improvements over vanilla Kubernetes. Then by following the design principle in DREAMS during implementation, we can effectively guarantee end-to-end requirements of critical containers. Fig. 1 : 1 Fig. 1: Local resource managers in a node", acknowledgement=None, annex=None)
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Amir Said', given_name='Amir', middle_name=None, surname='Said', email=None, orcid=None, affiliation=GrobidAffiliation(institution='Packard Laboratories', department=None, laboratory='Imaging Systems Laboratory', address=GrobidAddress(addr_line=None, post_code='HPL-2004-76', settlement='Palo Alto', country=None)))], index=None, id=None, unstructured=None, date='2004-04-21', title='Introduction to Arithmetic Coding -Theory and Practice 1', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2302.00819v1[cs.IT]', pii=None, ark=None, istex_id=None, url=None), pdf_md5='9C0A9F157F51F0C0D8326E43CB8B251B', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='C E Shannon', given_name='C', middle_name='E', surname='Shannon', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date='1948-07', title='A mathematical theory of communications', book_title=None, series_title=None, editors=None, journal='Bell Syst. Tech. J', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='27', issue=None, pages='379-423', first_page='379', last_page='423', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D A Huffman', given_name='D', middle_name='A', surname='Huffman', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='1952', title='A method for construction of minimum redundancy codes', book_title=None, series_title=None, editors=None, journal='Proc. IRE', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='40', issue=None, pages='1098-1101', first_page='1098', last_page='1101', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S W Golomb', given_name='S', middle_name='W', surname='Golomb', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date='1966', title='Run-length encoding', book_title=None, series_title=None, editors=None, journal='IEEE Trans. Inform. Theory', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='12', issue='4', pages='399-401', first_page='399', last_page='401', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R G Gallager', given_name='R', middle_name='G', surname='Gallager', email=None, orcid=None, affiliation=None)], index=3, id='b3', unstructured=None, date='1968', title='Information Theory and Reliable Communication', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='John Wiley & Sons', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='F Jelinek', given_name='F', middle_name=None, surname='Jelinek', email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='1968', title='Probabilistic Information Theory', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='McGraw Hill', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Pasco', given_name='R', middle_name=None, surname='Pasco', email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date='1976', title='Source Coding Algorithms for Fast Data Compression', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution='Dept. of Electrical Engineering, Stanford University, Stanford', issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J J Rissanen', given_name='J', middle_name='J', surname='Rissanen', email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date='1976-05', title='Generalized Kraft inequality and arithmetic coding', book_title=None, series_title=None, editors=None, journal='IBM J. Res. Develop', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='20', issue='3', pages='198-203', first_page='198', last_page='203', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Ziv', given_name='J', middle_name=None, surname='Ziv', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Lempel', given_name='A', middle_name=None, surname='Lempel', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date='1977', title='A universal algorithm for sequential data compression', book_title=None, series_title=None, editors=None, journal='IEEE Trans. Inform. Theory', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='23', issue='3', pages='337-343', first_page='337', last_page='343', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R G Gallagher', given_name='R', middle_name='G', surname='Gallagher', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date='1978-11', title='Variations on a theme by Huffman', book_title=None, series_title=None, editors=None, journal='IEEE Trans. Inform. Theory', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='24', issue='6', pages='668-674', first_page='668', last_page='674', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Ziv', given_name='J', middle_name=None, surname='Ziv', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Lempel', given_name='A', middle_name=None, surname='Lempel', email=None, orcid=None, affiliation=None)], index=9, id='b9', unstructured=None, date='1978', title='Compression of individual sequences via variable-rate coding', book_title=None, series_title=None, editors=None, journal='IEEE Trans. Inform. Theory', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='24', issue='5', pages='530-536', first_page='530', last_page='536', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G G Langdon', given_name='G', middle_name='G', surname='Langdon', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J J Rissanen', given_name='J', middle_name='J', surname='Rissanen', email=None, orcid=None, affiliation=None)], index=10, id='b10', unstructured=None, date='1978-10', title='Method and Means for Arithmetic String Coding', book_title=None, series_title=None, editors=None, journal='United States Patent', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='4', issue=None, pages='440', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G N N Martin', given_name='G', middle_name='N N', surname='Martin', email=None, orcid=None, affiliation=None)], index=11, id='b11', unstructured=None, date='1979-07', title='Range encoding: an algorithm for removing redundancy from a digitized message', book_title='Video and Data Recording Conf', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='F Rubin', given_name='F', middle_name=None, surname='Rubin', email=None, orcid=None, affiliation=None)], index=12, id='b12', unstructured=None, date='1979-11', title='Arithmetic stream coding using fixed precision registers', book_title=None, series_title=None, editors=None, journal='IEEE Trans. Inform. Theory', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='25', issue='6', pages='672-675', first_page='672', last_page='675', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J J Rissanen', given_name='J', middle_name='J', surname='Rissanen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G G Langdon', given_name='G', middle_name='G', surname='Langdon', email=None, orcid=None, affiliation=None)], index=13, id='b13', unstructured=None, date='1981-01', title='Universal modeling and coding', book_title=None, series_title=None, editors=None, journal='IEEE Trans. Inform. Theory', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='27', issue=None, pages='12-23', first_page='12', last_page='23', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G G Langdon', given_name='G', middle_name='G', surname='Langdon', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J J Rissanen', given_name='J', middle_name='J', surname='Rissanen', email=None, orcid=None, affiliation=None)], index=14, id='b14', unstructured=None, date='1981-06', title='Compression of black-white images with arithmetic coding', book_title=None, series_title=None, editors=None, journal='IEEE Trans. Commun', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='29', issue=None, pages='858-867', first_page='858', last_page='867', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G G Langdon', given_name='G', middle_name='G', surname='Langdon', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J J Rissanen', given_name='J', middle_name='J', surname='Rissanen', email=None, orcid=None, affiliation=None)], index=15, id='b15', unstructured=None, date='1981-08', title='Method and Means for Arithmetic Coding Utilizing a Reduced Number of Operations', book_title=None, series_title=None, editors=None, journal='United States Patent', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='4', issue=None, pages='256', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Gill', given_name='P', middle_name=None, surname='Gill', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M H Wright', given_name='M', middle_name='H', surname='Wright', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Murray', given_name='W', middle_name=None, surname='Murray', email=None, orcid=None, affiliation=None)], index=16, id='b16', unstructured=None, date='1982', title='Practical Optimization', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Academic Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G G Langdon', given_name='G', middle_name='G', surname='Langdon', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J J Rissanen', given_name='J', middle_name='J', surname='Rissanen', email=None, orcid=None, affiliation=None)], index=17, id='b17', unstructured=None, date='1982-09', title='A simple general binary source code', book_title=None, series_title=None, editors=None, journal='IEEE Trans. Inform. Theory', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='28', issue=None, pages='800-803', first_page='800', last_page='803', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J R Rice', given_name='J', middle_name='R', surname='Rice', email=None, orcid=None, affiliation=None)], index=18, id='b18', unstructured=None, date='1983', title='Numerical Methods, Software and Analysis', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='McGraw Hill', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N S Jayant', given_name='N', middle_name='S', surname='Jayant', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Noll', given_name='P', middle_name=None, surname='Noll', email=None, orcid=None, affiliation=None)], index=19, id='b19', unstructured=None, date='1984', title='Digital Coding of Waveforms', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Prentice-Hall, Inc', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R J Mceliece', given_name='R', middle_name='J', surname='Mceliece', email=None, orcid=None, affiliation=None)], index=20, id='b20', unstructured=None, date='1984', title='The Theory of Information and Coding', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Cambridge University Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Papoulis', given_name='A', middle_name=None, surname='Papoulis', email=None, orcid=None, affiliation=None)], index=21, id='b21', unstructured=None, date='1984', title='Probability, Random Variables, and Stochastic Processes', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='McGraw-Hill Pub. Co', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G G Langdon', given_name='G', middle_name='G', surname='Langdon', email=None, orcid=None, affiliation=None)], index=22, id='b22', unstructured=None, date='1984-03', title='An introduction to arithmetic coding', book_title=None, series_title=None, editors=None, journal='IBM J. Res. Develop', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='28', issue='2', pages='135-149', first_page='135', last_page='149', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D E Knuth', given_name='D', middle_name='E', surname='Knuth', email=None, orcid=None, affiliation=None)], index=23, id='b23', unstructured=None, date='1985-06', title='Dynamic Huffman coding', book_title=None, series_title=None, editors=None, journal='J. of Algorithms', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='6', issue=None, pages='163-180', first_page='163', last_page='180', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='I H Witten', given_name='I', middle_name='H', surname='Witten', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R M Neal', given_name='R', middle_name='M', surname='Neal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J G Cleary', given_name='J', middle_name='G', surname='Cleary', email=None, orcid=None, affiliation=None)], index=24, id='b24', unstructured=None, date='1987-06', title='Arithmetic coding for data compression', book_title=None, series_title=None, editors=None, journal='Commun. ACM', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='30', issue='6', pages='520-540', first_page='520', last_page='540', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J S Vitter', given_name='J', middle_name='S', surname='Vitter', email=None, orcid=None, affiliation=None)], index=25, id='b25', unstructured=None, date='1987-10', title='Design and analysis of dynamic Huffman codes', book_title=None, series_title=None, editors=None, journal='J. ACM', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='34', issue=None, pages='825-845', first_page='825', last_page='845', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W B Pennebaker', given_name='W', middle_name='B', surname='Pennebaker', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J L Mitchell', given_name='J', middle_name='L', surname='Mitchell', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G G Langdon Jr', given_name='G', middle_name='G', surname='Langdon', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R B Arps', given_name='R', middle_name='B', surname='Arps', email=None, orcid=None, affiliation=None)], index=26, id='b26', unstructured=None, date='1988-11', title='An overview of the basic principles of the Q-coder adaptive binary arithmetic coder', book_title=None, series_title=None, editors=None, journal='IBM J. Res. Develop', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='32', issue='6', pages='717-726', first_page='717', last_page='726', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W B Pennebaker', given_name='W', middle_name='B', surname='Pennebaker', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J L Mitchell', given_name='J', middle_name='L', surname='Mitchell', email=None, orcid=None, affiliation=None)], index=27, id='b27', unstructured=None, date='1988-11', title='Probability estimation for the Q-coder', book_title=None, series_title=None, editors=None, journal='IBM J. Res. Develop', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='32', issue='6', pages='737-752', first_page='737', last_page='752', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T C Bell', given_name='T', middle_name='C', surname='Bell', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I H Witten', given_name='I', middle_name='H', surname='Witten', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Cleary', given_name='J', middle_name=None, surname='Cleary', email=None, orcid=None, affiliation=None)], index=28, id='b28', unstructured=None, date='1990-02', title='Text Compression', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Prentice Hall', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T H Cormen', given_name='T', middle_name='H', surname='Cormen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C E Leiserson', given_name='C', middle_name='E', surname='Leiserson', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R L Rivest', given_name='R', middle_name='L', surname='Rivest', email=None, orcid=None, affiliation=None)], index=29, id='b29', unstructured=None, date='1990', title='Introduction to Algorithms', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='MIT Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Moffat', given_name='A', middle_name=None, surname='Moffat', email=None, orcid=None, affiliation=None)], index=30, id='b30', unstructured=None, date='1990-03', title='Linear time adaptive arithmetic coding', book_title=None, series_title=None, editors=None, journal='IEEE Trans. Inform. Theory', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='36', issue=None, pages='401-406', first_page='401', last_page='406', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T M Cover', given_name='T', middle_name='M', surname='Cover', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J A Thomas', given_name='J', middle_name='A', surname='Thomas', email=None, orcid=None, affiliation=None)], index=31, id='b31', unstructured=None, date='1991', title='Elements of Information Theory', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='John Wiley & Sons', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[], index=32, id='b32', unstructured=None, date='1992', title='(ITU), Digital Compression and Coding of Continuous-Tone Still Images: Requirements and Guidelines, ITU-T Recommendation T.81', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution='International Telecommunication Union', issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P G Howard', given_name='P', middle_name='G', surname='Howard', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J S Vitter', given_name='J', middle_name='S', surname='Vitter', email=None, orcid=None, affiliation=None)], index=33, id='b33', unstructured=None, date='1992', title='Practical implementations of arithmetic coding', book_title='Image and Text Compression', series_title=None, editors=[GrobidAuthor(full_name='J A Storer', given_name='J', middle_name='A', surname='Storer', email=None, orcid=None, affiliation=None)], journal=None, journal_abbrev=None, publisher='Kluwer Academic Publishers', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Gersho', given_name='A', middle_name=None, surname='Gersho', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R M Gray', given_name='R', middle_name='M', surname='Gray', email=None, orcid=None, affiliation=None)], index=34, id='b34', unstructured=None, date='1992', title='Vector Quantization and Signal Compression', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Kluwer Academic Publishers', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W B Pennebaker', given_name='W', middle_name='B', surname='Pennebaker', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J L Mitchell', given_name='J', middle_name='L', surname='Mitchell', email=None, orcid=None, affiliation=None)], index=35, id='b35', unstructured=None, date='1992', title='JPEG: Still Image Data Compression Standard', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Von Nostrand Reinhold', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P G Howard', given_name='P', middle_name='G', surname='Howard', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J S Vitter', given_name='J', middle_name='S', surname='Vitter', email=None, orcid=None, affiliation=None)], index=36, id='b36', unstructured=None, date='1992', title='Analysis of arithmetic coding for data compression', book_title=None, series_title=None, editors=None, journal='Inform. Proc. and Management', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='28', issue='6', pages='749-764', first_page='749', last_page='764', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[], index=37, id='b37', unstructured=None, date='1993', title='(ITU), Progressive Bi-Level Image Compression, ITU-T Recommendation T.82', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution='International Telecommunication Union', issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W H Press', given_name='W', middle_name='H', surname='Press', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S A Teukolsky', given_name='S', middle_name='A', surname='Teukolsky', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W T Vetterling', given_name='W', middle_name='T', surname='Vetterling', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B P Flannery', given_name='B', middle_name='P', surname='Flannery', email=None, orcid=None, affiliation=None)], index=38, id='b38', unstructured=None, date='1993', title='Numerical Recipes in C: The Art of Scientific Computing', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Cambridge University Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='2nd ed', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J M Shapiro', given_name='J', middle_name='M', surname='Shapiro', email=None, orcid=None, affiliation=None)], index=39, id='b39', unstructured=None, date='1993-12', title='Embedded image coding using zerotrees of wavelet coefficients', book_title=None, series_title=None, editors=None, journal='IEEE Trans. on Signal Processing', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='41', issue=None, pages='3445-3462', first_page='3445', last_page='3462', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Burrows', given_name='M', middle_name=None, surname='Burrows', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D J Wheeler', given_name='D', middle_name='J', surname='Wheeler', email=None, orcid=None, affiliation=None)], index=40, id='b40', unstructured=None, date='1994', title='A Block-sorting Lossless Data Compression Algorithm', book_title='Digital Systems Research Center Tehnical Report', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Digital Equipment Corporation', institution=None, issn=None, eissn=None, volume='124', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P M Fenwick', given_name='P', middle_name='M', surname='Fenwick', email=None, orcid=None, affiliation=None)], index=41, id='b41', unstructured=None, date='1994-03', title='A new data structure for cumulative frequency tables', book_title=None, series_title=None, editors=None, journal='Softw. Pract. Exper', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='24', issue=None, pages='327-336', first_page='327', last_page='336', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Moffat', given_name='A', middle_name=None, surname='Moffat', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Sharman', given_name='N', middle_name=None, surname='Sharman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I H Witten', given_name='I', middle_name='H', surname='Witten', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T C Bell', given_name='T', middle_name='C', surname='Bell', email=None, orcid=None, affiliation=None)], index=42, id='b42', unstructured=None, date='1994', title='An empirical evaluation of coding methods for multi-symbol alphabets', book_title=None, series_title=None, editors=None, journal='Information Processing and Management', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Said', given_name='A', middle_name=None, surname='Said', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W A Pearlman', given_name='W', middle_name='A', surname='Pearlman', email=None, orcid=None, affiliation=None)], index=43, id='b43', unstructured=None, date='1995-09', title='Reduced-complexity waveform coding via alphabet partitioning', book_title='Proc. IEEE Int. Symp. Information Theory', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='373', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M J Weinberger', given_name='M', middle_name='J', surname='Weinberger', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J J Rissanen', given_name='J', middle_name='J', surname='Rissanen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R B Arps', given_name='R', middle_name='B', surname='Arps', email=None, orcid=None, affiliation=None)], index=44, id='b44', unstructured=None, date='1996-04', title='Applications of universal context modeling to lossless compression of gray-scale images', book_title=None, series_title=None, editors=None, journal='IEEE Trans. on Image Processing', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='5', issue=None, pages='575-586', first_page='575', last_page='586', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='X Wu', given_name='X', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Memon', given_name='N', middle_name=None, surname='Memon', email=None, orcid=None, affiliation=None)], index=45, id='b45', unstructured=None, date='1996-05', title='CALIC -a context based adaptive lossless image codec', book_title='Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='4', issue=None, pages='1890-1893', first_page='1890', last_page='1893', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Said', given_name='A', middle_name=None, surname='Said', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W A Pearlman', given_name='W', middle_name='A', surname='Pearlman', email=None, orcid=None, affiliation=None)], index=46, id='b46', unstructured=None, date='1996-06', title='A new fast and efficient image codec based on set partitioning in hierarchical trees', book_title=None, series_title=None, editors=None, journal='IEEE. Trans. Circ. Syst. Video Tech', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='6', issue=None, pages='243-250', first_page='243', last_page='250', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V Bhaskaran', given_name='V', middle_name=None, surname='Bhaskaran', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Konstantinides', given_name='K', middle_name=None, surname='Konstantinides', email=None, orcid=None, affiliation=None)], index=47, id='b47', unstructured=None, date='1997', title='Image and Video Compression Standards: Algorithms and Architectures', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Kluwer Academic Publishers', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Lopresto', given_name='S', middle_name=None, surname='Lopresto', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Ramchandran', given_name='K', middle_name=None, surname='Ramchandran', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M T Orchard', given_name='M', middle_name='T', surname='Orchard', email=None, orcid=None, affiliation=None)], index=48, id='b48', unstructured=None, date='1997-03', title='Image coding based on mixture modeling of wavelet coefficients and a fast estimation-quantization framework', book_title='Data Compression Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='221-230', first_page='221', last_page='230', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Moffat', given_name='A', middle_name=None, surname='Moffat', email=None, orcid=None, affiliation=None)], index=49, id='b49', unstructured=None, date='1997-11', title="Critique of the paper 'Novel design of arithmetic coding for data compression", book_title=None, series_title=None, editors=None, journal='IEE Proc. -Comput. Digit. Tech', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='144', issue=None, pages='394-396', first_page='394', last_page='396', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Moffat', given_name='A', middle_name=None, surname='Moffat', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R M Neal', given_name='R', middle_name='M', surname='Neal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I H Witten', given_name='I', middle_name='H', surname='Witten', email=None, orcid=None, affiliation=None)], index=50, id='b50', unstructured=None, date='1998', title='Arithmetic coding revisited', book_title=None, series_title=None, editors=None, journal='ACM Transactions on Information Systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='16', issue='3', pages='256-294', first_page='256', last_page='294', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Schindler', given_name='M', middle_name=None, surname='Schindler', email=None, orcid=None, affiliation=None)], index=51, id='b51', unstructured=None, date='1998', title='A fast renormalisation for arithmetic coding', book_title='Proc. IEEE Data Compression Conf', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='I H Witten', given_name='I', middle_name='H', surname='Witten', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Moffat', given_name='A', middle_name=None, surname='Moffat', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T C Bell', given_name='T', middle_name='C', surname='Bell', email=None, orcid=None, affiliation=None)], index=52, id='b52', unstructured=None, date='1999', title='Managing Gigabytes: Compressing and Indexing Documents and Images', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Morgan Kaufmann Publishers', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='2nd ed', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P-C Wu', given_name='P-C', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None)], index=53, id='b53', unstructured=None, date='1999-04', title='A byte-wise normalization method in arithmetic coding', book_title=None, series_title=None, editors=None, journal='Softw. Pract. Exper', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='29', issue=None, pages='299-309', first_page='299', last_page='309', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Sayood', given_name='K', middle_name=None, surname='Sayood', email=None, orcid=None, affiliation=None)], index=54, id='b54', unstructured=None, date='2000', title='Introduction to Data Compression', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Morgan Kaufmann Publishers', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='2nd ed', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Salomon', given_name='D', middle_name=None, surname='Salomon', email=None, orcid=None, affiliation=None)], index=55, id='b55', unstructured=None, date='2000', title='Data Compression', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Springer Verlag', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='2nd ed', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='I Sodagar', given_name='I', middle_name=None, surname='Sodagar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B-B. Chai', given_name='B-B.', middle_name=None, surname='Chai', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Wus', given_name='J', middle_name=None, surname='Wus', email=None, orcid=None, affiliation=None)], index=56, id='b56', unstructured=None, date='2000-06', title='A new error resilience technique for image compression using arithmetic coding', book_title='Proc. Int. Conf. Acoustics, Speech and Signal Proc', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='4', issue=None, pages='2127-2130', first_page='2127', last_page='2130', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[], index=57, id='b57', unstructured=None, date='2000', title='Texas Instruments Incorporated, TMS320C6000 CPU and Instruction Set Reference Guide, Literature Number: SPRU189F', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[], index=58, id='b58', unstructured=None, date='2001', title="International Business Machines Corporation: PowerPC 750CX/CXe RISC Microprocessor User's Manual", book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='preliminary edition', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[], index=59, id='b59', unstructured=None, date='2001', title='Intel Pentium 4 Processor Optimization, Reference Manual 248966', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution='Intel Corporation', issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Inc', given_name='Inc', middle_name=None, surname=None, email=None, orcid=None, affiliation=None)], index=60, id='b60', unstructured=None, date='2001', title='UltraSPARC III Technical Highlights', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution='Sun Microsystems', issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D S Taubman', given_name='D', middle_name='S', surname='Taubman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M W Marcellin', given_name='M', middle_name='W', surname='Marcellin', email=None, orcid=None, affiliation=None)], index=61, id='b61', unstructured=None, date='2000', title='Image Compression Fundamentals, Standards and Practice', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Kluwer Academic Publishers', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='JPEG', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Said', given_name='A', middle_name=None, surname='Said', email=None, orcid=None, affiliation=None)], index=62, id='b62', unstructured=None, date='2004-04', title='Comparative Analysis of Arithmetic Coding Computational Complexity', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution='Hewlett-Packard Laboratories Report', issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None)], abstract='This introduction to arithmetic coding is divided in two parts. The first explains how and why arithmetic coding works. We start presenting it in very general terms, so that its simplicity is not lost under layers of implementation details. Next, we show some of its basic properties, which are later used in the computational techniques required for a practical implementation.', body='In the second part, we cover the practical implementation aspects, including arithmetic operations with low precision, the subdivision of coding and modeling, and the realization of adaptive encoders. We also analyze the arithmetic coding computational complexity, and techniques to reduce it. We start some sections by first introducing the notation and most of the mathematical definitions. The reader should not be intimidated if at first their motivation is not clear: these are always followed by examples and explanations. \n Chapter 1 Arithmetic Coding Principles 1.1 Data Compression and Arithmetic Coding Compression applications employ a wide variety of techniques, have quite different degrees of complexity, but share some common processes. Figure  1 .1 shows a diagram with typical processes used for data compression. These processes depend on the data type, and the blocks in Figure  1 .1 may be in different order or combined. Numerical processing, like predictive coding and linear transforms, is normally used for waveform signals, like images and audio  [20, 35, 36, 48, 55] . Logical processing consists of changing the data to a form more suited for compression, like run-lengths, zero-trees, set-partitioning information, and dictionary entries  [3, 20, 38, 40, 41, 44, 47, 55] . The next stage, source modeling, is used to account for variations in the statistical properties of the data. It is responsible for gathering statistics and identifying data contexts that make the source models more accurate and reliable  [14, 28, 29, 45, 46, 49, 53] . What most compression systems have in common is the fact that the final process is entropy coding, which is the process of representing information in the most compact form. It may be responsible for doing most of the compression work, or it may just complement what has been accomplished by previous stages. When we consider all the different entropy-coding methods, and their possible applications in compression applications, arithmetic coding stands out in terms of elegance, effectiveness and versatility, since it is able to work most efficiently in the largest number of circumstances and purposes. Among its most desirable features we have the following. • When applied to independent and identically distributed (i.i.d.) sources, the compression of each symbol is provably optimal (Section 1.5). • It is effective in a wide range of situations and compression ratios. The same arithmetic coding implementation can effectively code all the diverse data created by the different processes of Figure  1 .1, such as modeling parameters, transform coefficients, signaling, etc. (Section 1.6.1). • It simplifies automatic modeling of complex sources, yielding near-optimal or significantly improved compression for sources that are not i.i.d (Section 1.6.3). System with typical processes for data compression. Arithmetic coding is normally the final stage, and the other stages can be modeled as a single data source Ω. • Its main process is arithmetic, which is supported with ever-increasing efficiency by all general-purpose or digital signal processors (CPUs, DSPs) (Section 2.3). • It is suited for use as a "compression black-box" by those that are not coding experts or do not want to implement the coding algorithm themselves. Even with all these advantages, arithmetic coding is not as popular and well understood as other methods. Certain practical problems held back its adoption. • The complexity of arithmetic operations was excessive for coding applications. • Patents covered the most efficient implementations. Royalties and the fear of patent infringement discouraged arithmetic coding in commercial products. • Efficient implementations were difficult to understand. However, these issues are now mostly overcome. First, the relative efficiency of computer arithmetic improved dramatically, and new techniques avoid the most expensive operations. Second, some of the patents have expired (e.g.,  [11, 16] ), or became obsolete. Finally, we do not need to worry so much about complexity-reduction details that obscure the inherent simplicity of the method. Current computational resources allow us to implement simple, efficient, and royalty-free arithmetic coding. \n Notation Let Ω be a data source that puts out symbols s k coded as integer numbers in the set {0, 1, . . . , M − 1}, and let S = {s 1 , s 2 , . . . , s N } be a sequence of N random symbols put out by Ω  [1, 4, 5, 21, 55, 56] . For now, we assume that the source symbols are independent and identically distributed  [22] , with probability p(m) = Prob{s k = m}, m = 0, 1, 2, . . . , M − 1, k = 1, 2, . . . , N. (1.1) We also assume that for all symbols we have p(m) = 0, and define c(m) to be the cumulative distribution, We assume that the compressed data (output of the encoder) is saved in a vector (buffer) d. The output alphabet has D symbols, i.e., each element in d belongs to set {0, 1, . . . , D − 1}. Under the assumptions above, an optimal coding method  [1]  codes each symbol s from Ω with an average number of bits equal to B(s) = − log 2 p(s) bits. ( ⊳ Data source Ω can be a file with English text: each symbol from this source is a single byte representatinh a character. This data alphabet contains M = 256 symbols, and symbol numbers are defined by the ASCII standard. The probabilities of the symbols can be estimated by gathering statistics using a large number of English texts. Table  1 .1 shows some characters, their ASCII symbol values, and their estimated probabilities. It also shows the number of bits required to code symbol s in an optimal manner, − log 2 p(s). From these numbers we conclude that, if data symbols in English text were i.i.d., then the best possible text compression ratio would be about 2:1 (4 bits/symbol). Specialized text compression methods  [8, 10, 29, 41]  can yield significantly better compression ratios because they exploit the statistical dependence between letters. ⊲ This first example shows that our initial assumptions about data sources are rarely found in practical cases. More commonly, we have the following issues. Table  1 .1: Estimated probabilities of some letters and punctuation marks in the English language. Symbols are numbered according to the ASCII standard. \n Code Values Arithmetic coding is different from other coding methods for which we know the exact relationship between the coded symbols and the actual bits that are written to a file. It codes one data symbol at a time, and assigns to each symbol a real-valued number of bits (see examples in the last column of Table  1 .1). To figure out how this is possible, we have to understand the code value representation: coded messages mapped to real numbers in the interval [0, 1). The code value v of a compressed data sequence is the real number with fractional digits equal to the sequence\'s symbols. We can convert sequences to code values by simply adding "0." to the beginning of a coded sequence, and then interpreting the result as a number in base-D notation, where D is the number of symbols in the coded sequence alphabet. For example, if a coding method generates the sequence of bits 0011000101100, then we have \n Code sequence d = [ 0011000101100 ] Code value v = 0. 0011000101100 2 = 0.19287109375  (1.5)  where the "2" subscript denotes base-2 notation. As usual, we omit the subscript for decimal notation. This construction creates a convenient mapping between infinite sequences of symbols from a D-symbol alphabet and real numbers in the interval [0,  1) , where any data sequence can be represented by a real number, and vice-versa. The code value representation can be used for any coding system and it provides a universal way to represent large amounts of information independently of the set of symbols used for coding (binary, ternary, decimal, etc.). For instance, in  (1.5)  we see the same code with base-2 and base-10 representations. We can evaluate the efficacy of any compression method by analyzing the distribution of the code values it produces. From Shannon\'s information theory  [1]  we know that, if a coding method is optimal, then the cumulative distribution  [22]  of its code values has to be a straight line from point (0, 0) to point  (1, 1) . \n Example 2 ⊳ Let us assume that the i.i.d. source Ω has four symbols, and the probabilities of the data symbols are p = [ 0.65 0.2 0.1 0.05 ]. If we code random data sequences from this source with two bits per symbols, the resulting code values produce a cumulative distribution as shown in Figure  1 .2, under the label "uncompressed." Note how the distribution is skewed, indicating the possibility for significant compression. The same sequences can be coded with the Huffman code for Ω  [2, 4, 21, 55, 56] , with one bit used for symbol "0", two bits for symbol "1", and three bits for symbols "2" and "3". The corresponding code value cumulative distribution in Figure  1 .2 shows that there is substantial improvement over the uncompressed case, but this coding method is still clearly not optimal. The third line in Figure  1 .2 shows that the sequences compressed with arithmetic coding simulation produce a code value distribution that is practically identical to the optimal. ⊲ The straight-line distribution means that if a coding method is optimal then there is no statistical dependence or redundancy left in the compressed sequences, and consequently its code values are uniformly distributed on the interval [0, 1). This fact is essential for understanding of how arithmetic coding works. Moreover, code values are an integral part of the arithmetic encoding/decoding procedures, with arithmetic operations applied to real numbers that are directly related to code values. One final comment about code values: two infinitely long different sequences can correspond to the same code value. This follows from the fact that for any D > 1 we have ∞ n=k (D − 1)D −n = D 1−k . (1.6) For example, if D = 10 and k = 2, then (1.6) is the equality 0.09999999 . . . = 0.1. This fact has no important practical significance for coding purposes, but we need to take it into account when studying some theoretical properties of arithmetic coding. \n Arithmetic Coding \n Encoding Process In this section we first introduce the notation and equations that describe arithmetic encoding, followed by a detailed example. Fundamentally, the arithmetic encoding process consists of creating a sequence of nested intervals in the form Φ where S is the source data sequence, α k and β k are real numbers such that 0 ≤ α k ≤ α k+1 , and β k+1 ≤ β k ≤ 1. For a simpler way to describe arithmetic coding we represent intervals in the form | b, l , where b is called the base or starting point of the interval, and l the length of the interval. The relationship between the traditional and the new interval notation is k (S) = [ α k , β k ) , k = 0, 1, . . . , N, | b, l = [ α, β ) if b = α and l = β − α. (1.7) The intervals used during the arithmetic coding process are, in this new notation, defined by the set of recursive equations  [5, 13]   Φ 0 (S) = | b 0 , l 0 = | 0, 1 , (1.8) Φ k (S) = | b k , l k = | b k−1 + c(s k ) l k−1 , p(s k ) l k−1 , k = 1, 2, . . . , N. (1.9) The properties of the intervals guarantee that 0 ≤ b k ≤ b k+1 < 1, and 0 < l k+1 < l k ≤ 1. Figure  1 .3 shows a dynamic system corresponding to the set of recursive equations (1.9). We later explain how to choose, at the end of the coding process, a code value in the final interval, i.e., v(S) ∈ Φ N (S). The coding process defined by (1.8) and (1.9), also called Elias coding, was first described in  [5] . Our convention of representing an interval using its base and length has been used \n Data source \n Source model (tables) Delay Delay since the first arithmetic coding papers  [12, 13] . Other authors have intervals represented by their extreme points, like [base, base+length), but there is no mathematical difference between the two notations. ✒✑ ✓✏ ❅ ❅ ✒✑ ✓✏ ✒✑ ✓✏ ❅ ❅ r r r ✲ ✲ ✲ ✲ ✛ ✛ ✛ ✲ ❄ ❄ ❄ s k b k−1 l k−1 l k b k c(s k ) p(s k ) s -data symbol p -symbol probability c -cumulative distribution b -interval base l -interval length \n Example 3 ⊳ Let us assume that source Ω has four symbols (M = 4), the probabilities and distribution of the symbols are p = [ 0.2 0.5 0.2 0.1 ] and c = [ 0 0.2 0.7 0.9 1 ], and the sequence of (N = 6) symbols to be encoded is S = {2, 1, 0, 0, 1, 3}. Figure  1 .4 shows graphically how the encoding process corresponds to the selection of intervals in the line of real numbers. We start at the top of the figure, with the interval [0, 1), which is divided into four subintervals, each with length equal to the probability of the data symbols. Specifically, interval [0, 0.2) corresponds to s 1 = 0, interval [0.2, 0.7) corresponds to s 1 = 1, interval [0.7, 0.9) corresponds to s 1 = 2, and finally interval [0.9, 1) corresponds to s 1 = 3. The next set of allowed nested subintervals also have length proportional to the probability of the symbols, but their lengths are also proportional to the length of the interval they belong to. Furthermore, they represent more than one symbol value. For example, interval [0, 0.04) corresponds to s 1 = 0, s 2 = 0, interval [0.04, 0.14) corresponds to s 1 = 0, s 2 = 1, and so on. The interval lengths are reduced by factors equal to symbol probabilities in order to obtain code values that are uniformly distributed in the interval [0, 1) (a necessary condition for optimality, as explained in Section 1.3). For example, if 20% of the sequences start with symbol "0", then 20% of the code values must be in the interval assigned to those sequences, which can only be achieved if we assign to the first symbol "0" an interval with length equal to its probability, 0.2. The same reasoning applies to the assignment of the subinterval lengths: every occurrence of symbol "0" must result in a reduction of the interval length to 20% its current length. This way, after encoding several symbols the distribution of code values should be a very good approximation of a uniform distribution. Iteration Input Interval Interval Decoder Output Symbol base length updated value symbol k s k b k l k ṽk = v−b k−1 l k−1 ŝk 0 - 0 1 - - 1 2 Equations (1.8) and (1.9) provide the formulas for the sequential computation of the intervals. Applying them to our example we obtain: Φ 0 (S) = | 0, 1 = [ 0, 1 ) , Φ 1 (S) = | b 0 + c(2)l 0 , p(2)l 0 = | 0 + 0.7 × 1, 0.2 × 1 = [ 0.7, 0.9 ) , Φ 2 (S) = | b 1 + c(1)l 1 , p(1)l 1 = | 0.7 + 0.2 × 0.2, 0.5 × 0.2 = [ 0.74, 0.84 ) , . . . Φ 6 (S) = | b 5 + c(3)l 5 , p(3)l 5 = | 0.7426, 0.0002 = [ 0.7426, 0.7428 ) , The list with all the encoder intervals is shown in the first four columns of Table  1 .2. Since the intervals quickly become quite small, in Figure  1 .4 we have to graphically magnify them (twice) so that we can see how the coding process continues. Note that even though the intervals are shown in different magnifications, the intervals values do not change, and the process to subdivide intervals continues in exactly the same manner. ⊲ The final task in arithmetic encoding is to define a code value v(S) that will represent data sequence S. In the next section we show how the decoding process works correctly for any code value v ∈ Φ N (S). However, the code value cannot be provided to the decoder as a pure real number. It has to be stored or transmitted, using a conventional number representation. Since we have the freedom to choose any value in the final interval, we want to choose the values with the shortest representation. For instance, in Example 3, the shortest decimal representation comes from choosing v = 0.7427, and the shortest binary representation is obtained with v = 0.10111110001 2 = 0.74267578125.  The process to find the best binary representation is quite simple and best shown by induction. The main idea is that for relatively large intervals we can find the optimal value by testing a few binary sequences, and as the interval lengths are halved, the number of sequences to be tested has to double, increasing the number of bits by one. Thus, according to the interval length l N , we use the following rules: v ˆ = 0.74267578125 v ˆ = 0.74267578125 Φ 0 s 1 = 2 s 1 = 0 s 1 = 1 s 1 = 3 Φ 1 Φ 2 Φ 5 Φ 3 Φ 6 s 2 = 1 s 3 =0 s 4 = 0 s 5 = 1 s 6 = 3 • If l N ∈ [0.5, 1), then choose code value v ∈ {0, 0.5} = {0.0 2 , 0.1 2 } for a 1-bit representation. • If l N ∈ [0.25, 0.5), then choose value v ∈ {0, 0.25, 0.5, 0.75} = {0.00 2 , 0.01 2 , 0.10 2 , 0.11 2 } for a 2-bit representation. • If l N ∈ [0.125, 0.25), then choose value v ∈ {0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875} = {0.000 2 , 0.001 2 , 0.010 2 , 0.011 2 , 0.100 2 , 0.101 2 , 0.110 2 , 0.111 2 } for a 3-bit representation. By observing the pattern we conclude that the minimum number of bits required for representing v ∈ Φ N (S) is B min = ⌈− log 2 (l N )⌉ bits, (1.10) where ⌈x⌉ represents the smallest integer greater than or equal to x. We can test this conclusion observing the results for Example 3 in Table  1 .2. The final interval is l N = 0.0002, and thus B min = ⌈− log 2 (0.0002)⌉ = 13 bits. However, in Example 3 we can choose v = 0.10111110001 2 , and it requires only 11 bits! The origin of this inconsistency is the fact that we can choose binary representations with the number of bits given by  (10) , and then remove the trailing zeros. However, with optimal coding the average number of bits that can be saved with this process is only one bit, and for that reason, it is rarely applied in practice. \n Decoding Process In arithmetic coding, the decoded sequence is determined solely by the code value v of the compressed sequence. For that reason, we represent the decoded sequence as Ŝ(v) = {ŝ 1 (v), ŝ2 (v), . . . , ŝN (v)} . (1.11) We now show the decoding process by which any code value v ∈ Φ N (S) can be used for decoding the correct sequence (i.e., Ŝ(v) = S). We present the set of recursive equations that implement decoding, followed by a practical example that provides an intuitive idea of how the decoding process works, and why it is correct. The decoding process recovers the data symbols in the same sequence that they were coded. Formally, to find the numerical solution, we define a sequence of normalized code values {ṽ 1 , ṽ2 , . . . , ṽN }. Starting with ṽ1 = v, we sequentially find ŝk from ṽk , and then we compute ṽk+1 from ŝk and ṽk . The recursion formulas are ṽ1 = v, (1.12) ŝk (v) = { s : c(s) ≤ ṽk < c(s + 1)} , k = 1, 2, . . . , N, (1.13 ) ṽk+1 = ṽk − c(ŝ k (v)) p(ŝ k (v)) , k = 1, 2, . . . , N − 1. (1.14) (In equation  (1.13)  the colon means "s that satisfies the inequalities.") A mathematically equivalent decoding method-which later we show to be necessary when working with fixed-precision arithmetic-recovers the sequence of intervals created by the encoder, and searches for the correct value ŝk (v) in each of these intervals. It is defined by Φ 0 ( Ŝ) = | b 0 , l 0 = | 0, 1 , (1.15) ŝk (v) = s : c(s) ≤ v − b k−1 l k−1 < c(s + 1) , k = 1, 2, . . . , N, (1.16) Φ k ( Ŝ) = | b k , l k = | b k−1 + c(ŝ k (v)) l k−1 , p(ŝ k (v)) l k−1 , k = 1, 2, . . . , N. (1.17) The combination of recursion (1.14) with recursion (1.17)  .18)  showing that (1.13) is equivalent to  (1.16) . yields ṽk = v − k−1 i=1 c(ŝ i ) i−1 j=1 p(ŝ j ) k−1 i=1 p(ŝ i ) = v − b k−1 l k−1 . ( 1 \n Example 4 ⊳ Let us apply the decoding process to the data obtained in Example 3. In Figure  1 .4, we show graphically the meaning of v: it is a value that belongs to all nested intervals created during coding. The dotted line shows that its position moves as we magnify the graphs, but the value remains the same. From Figure  1 .4, we can see that we can start decoding from the first interval Φ 0 (S) = [0, 1): we just have to compare v with the cumulative distribution c to find the only possible value of ŝ1 ŝ1 (v) = { s : c(s) ≤ v = 0.74267578125 < c(s + 1)} = 2. We can use the value of ŝ1 to find out interval Φ 1 (S), and use it for determining ŝ2 . In fact, we can "remove" the effect of ŝ1 in v by defining the normalized code value ṽ2 = v − c(ŝ 1 ) p(ŝ 1 ) = 0.21337890625. Note that, in general, ṽ2 ∈ [0, 1), i.e., it is a value normalized to the initial interval. In this interval we can use the same process to find ŝ2 (v) = { s : c(s) ≤ ṽ2 = 0.21337890625 < c(s + 1)} = 1. The last columns of Table  1 .2 show how the process continues, and the updated values computed while decoding. We could say that the process continues until ŝ6 is decoded. However, how can the decoder, having only the initial code value v, know that it is time to stop decoding? The answer is simple: it can\'t. We added two extra rows to Table  1  It is important to understand that arithmetic encoding maps intervals to sets of sequences. Each real number in an interval corresponds to one infinite sequence. Thus, the sequences corresponding to Φ 6 (S) = [0.7426, 0.7428) are all those that start as {2, 1, 0, 0, 1, 3, . . .}. The code value v = 0.74267578125 corresponds to one such infinite sequence, and the decoding process can go on forever decoding that particular sequence. There are two practical ways to inform that decoding should stop: 1. Provide the number of data symbols (N) in the beginning of the compressed file. 2. Use a special symbol as "end-of-message," which is coded only at the end of the data sequence, and assign to this symbol the smallest probability value allowed by the encoder/decoder. As we explained above, the decoding procedure will always produce a decoded data sequence. However, how do we know that it is the right sequence? This can be inferred from the fact that if S and S ′ are sequences with N symbols then S = S ′ ⇔ Φ N (S) ∩ Φ N (S ′ ) = ∅. (1.19) This guarantees that different sequences cannot produce the same code value. In Section 1.6.6 we show that, due to approximations, we have incorrect decoding if  (1.19 ) is not satisfied. \n Optimality of Arithmetic Coding Information theory  [1, 4, 5, 21, 32, 55, 56]  shows us that the average number of bits needed to code each symbol from a stationary and memoryless source Ω cannot be smaller than its entropy H(Ω), defined by H(Ω) = − M −1 m=0 p(m) log 2 p(m) bits/symbol. (1.20) We have seen that the arithmetic coding process generates code values that are uniformly distributed across the interval [0, 1). This is a necessary condition for optimality, but not a sufficient one. In the interval Φ N (S) we can choose values that require an arbitrarily large number of bits to be represented, or choose code values that can be represented with the minimum number of bits, given by equation (1.10). Now we show that the latter choice satisfies the sufficient condition for optimality. To begin, we have to consider that there is some overhead in a compressed file, which may include • Extra bits required for saving v with an integer number of bytes. • A fixed or variable number of bits representing the number of symbols coded. • Information about the probabilities (p or c). Assuming that the total overhead is a positive number σ bits, we conclude from (1.10) that the number of bits per symbol used for coding a sequence S should be bounded by B S ≤ σ − log 2 (l N ) N bits/symbol. (1.21) It follows from (1.9) that l N = N k=1 p(s k ), (1.22) and thus B S ≤ σ − N k=1 log 2 p(s k ) N bits/symbol. (1.23) Defining E {•} as the expected value operator, the expected number of bits per symbol is B = E{B S } ≤ σ − N k=1 E {log 2 p(s k )} N = σ − N k=1 M −1 m=0 p(m) log 2 p(m) N (1.24) ≤ H(Ω) + σ N Since the average number of bits per symbol cannot be smaller than the entropy, we have H(Ω) ≤ B ≤ H(Ω) + σ N , (1.25) and it follows that lim N →∞ B = H(Ω), (1.26) which means that arithmetic coding indeed achieves optimal compression performance. At this point we may ask why arithmetic coding creates intervals, instead of single code values. The answer lies in the fact that arithmetic coding is optimal not only for binary output-but rather for any output alphabet. In the final interval we find the different code values that are optimal for each output alphabet. Here is an example of use with non-binary outputs. \n Example 5 ⊳ Consider transmitting the data sequence of Example 3 using a communications system that conveys information using three levels, {-V, 0, +V} (actually used in radio remote controls). Arithmetic coding with ternary output can simultaneously compress the data and convert it to the proper transmission format. The generalization of (1.10) for a D-symbol output alphabet is B min (l N , D) = ⌈− log D (l N )⌉ symbols. (1.27) Thus, using the results in Table  1 .2, we conclude that we need ⌈− log 3 (0.0002)⌉ = 8 ternary symbols. We later show how to use standard arithmetic coding to find that the shortest ternary representation is v3 = 0.20200111 3 ≈ 0.742722146, which means that the sequence S = {2, 1, 0, 0, 1, 3} can be transmitted as the sequence of electrical signals {+V, 0, +V, 0, 0, -V, -V, -V}. ⊲ \n Arithmetic Coding Properties 1.6.1 Dynamic Sources In Section 1.2 we assume that the data source Ω is stationary, so we have one set of symbol probabilities for encoding and decoding all symbols in the data sequence S. Now, with an understanding of the coding process, we generalize it for situations where the probabilities change for each symbol coded, i.e., the k-th symbol in the data sequence S is a random variable with probabilities p k and distribution c k . The only required change in the arithmetic coding process is that instead of using (1.9) for interval updating, we should use Φ k (S) = | b k , l k = | b k−1 + c k (s k ) l k−1 , p k (s k ) l k−1 , k = 1, 2, . . . , N. (1.28) To understand the changes in the decoding process, remember that the process of working with updated code values is equivalent to "erasing" all information about past symbols, and decoding in the [0, 1) interval. Thus, the decoder only has to use the right set of probabilities for that symbol to decode it correctly. The required changes to  (1.16 ) and (1.17) yield ŝk (v) = s : c k (s) ≤ v − b k−1 l k−1 < c k (s + 1) , k = 1, 2, . . . , N, (1.29) Φ k (S) = | b k , l k = | b k−1 + c k (ŝ k (v)) l k−1 , p k (ŝ k (v)) l k−1 , k = 1, 2, . . . , N. (1.30) Note that the number of symbols used at each instant can change. Instead of having a single input alphabet with M symbols, we have a sequence of alphabet sizes {M 1 , M 2 , . . . , M N }. \n Encoder and Decoder Synchronized Decisions In data compression an encoder can change its behavior (parameters, coding algorithm, etc.) while encoding a data sequence, as long as the decoder uses the same information and the same rules to change its behavior. In addition, these changes must be "synchronized," not in time, but in relation to the sequence of data source symbols. For instance, in Section 1.6.1, we assume that the encoder and decoder are synchronized in their use of varying sets of probabilities. Note that we do not have to assume that all the probabilities are available to the decoder when it starts decoding. The probability vectors can be updated with any rule based on symbol occurrences, as long as p k is computed from the data already available to the decoder, i.e., {ŝ 1 , ŝ2 , . . . , ŝk−1 }. This principle is used for adaptive coding, and it is covered in Section 2.2. This concept of synchronization is essential for arithmetic coding because it involves a nonlinear dynamic system (Figure  1 .3), and error accumulation leads to incorrect decoding, unless the encoder and decoder use exactly the same implementation (same precision, number of bits, rounding rules, equations, tables, etc.). In other words, we can make arithmetic coding work correctly even if the encoder makes coarse approximations, as long as the decoder makes exactly the same approximations. We have already seen an example of a choice based on numerical stability: equations (1.16) and (1.17) enable us to synchronize the encoder and decoder because they use the same interval updating rules used by (1.9), while (1.13) and (1.14) use a different recursion. \n Data sequence \n Recovered data Delay Delay Source modeling \n Source modeling \n Choice of probability distribution \n Choice of probability distribution \n Arithmetic encoding \n Interval updating \n Arithmetic decoding Interval selection and updating ✲ ✲ ✲ ✛ ✻ ❄ ✻ ❄ ❄ r r s k ŝk c k c k d Figure 1 .5: Separation of coding and source modeling tasks. Arithmetic encoding and decoding process intervals, while source modeling chooses the probability distribution for each data symbol. \n Separation of Coding and Source Modeling There are many advantages for separating the source modeling (probabilities estimation) and the coding processes  [14, 25, 29, 38, 45, 51, 53] . For example, it allows us to develop complex compression schemes without worrying about the details in the coding algorithm, and/or use them with different coding methods and implementations. Figure  1 .5 shows how the two processes can be separated in a complete system for arithmetic encoding and decoding. The coding part is responsible only for updating the intervals, i.e., the arithmetic encoder implements recursion (1.28), and the arithmetic decoder implements (1.29) and  (1.30) . The encoding/decoding processes use the probability distribution vectors as input, but do not change them in any manner. The source modeling part is responsible for choosing the distribution c k that is used to encode/decode symbol s k . Figure  1 .5 also shows that a delay of one data symbol before the source-modeling block guarantees that encoder and decoder use the same information to update c k . Arithmetic coding simplifies considerably the implementation of systems like Figure  1 .5 because the vector c k is used directly for coding. With Huffman coding, changes in probabilities require re-computing the optimal code, or using complex code updating techniques  [9, 24, 26] . \n Interval Rescaling Figure  1 .4 shows graphically one important property of arithmetic coding: the actual intervals used during coding depend on the initial interval and the previously coded data, but the proportions within subdivided intervals do not. For example, if we change the initial interval to Φ 0 = | 1, 2 = [ 1, 3 ) and apply (1.9), the coding process remains the same, except that all intervals are scaled by a factor of two, and shifted by one. We can also apply rescaling in the middle of the coding process. Suppose that at a certain stage m we change the interval according to b ′ m = γ (b m − δ), l ′ m = γ l m , (1.31) and continue the coding process normally (using (1.9) or (1.28)). When we finish coding we obtain the interval Φ ′ N (S) = | b ′ N , l ′ N and the corresponding code value v ′ . We can use the following equations to recover the interval and code value that we would have obtained without rescaling: b N = b ′ N γ + δ, l N = l ′ N γ , v = v ′ γ + δ. (1.32) The decoder needs the original code value v to start recovering the data symbols. It should also rescale the interval at stage m, and thus needs to know m, δ, γ. Furthermore, when it scales the interval using (1.31), it must scale the code value as well, using v ′ = γ (v − δ). (1.33) We can generalize the results above to rescaling at stages m ≤ n ≤ . . . ≤ p. In general, the scaling process, including the scaling of the code values is b ′ m = γ 1 (b m − δ 1 ), l ′ m = γ 1 l m , v ′ = γ 1 (v − δ 1 ), b ′′ n = γ 2 (b ′ n − δ 2 ), l ′′ n = γ 2 l ′ n , v ′′ = γ 2 (v ′ − δ 2 ), . . . . . . . . . b (T ) p = γ T (b (T −1) p − δ T ), l (T ) p = γ T l (T −1) p , v (T ) = γ T (v (T −1) − δ T ). (1.34) At the end of the coding process we have interval ΦN (S) = bN , lN and code value v. We recover original values using Φ N (S) = | b N , l N = δ 1 + 1 γ 1 δ 2 + 1 γ 2 δ 3 + 1 γ 3 • • • δ T + bN γ T , lN T i=1 γ i , (1.35 ) and v = δ 1 + 1 γ 1 δ 2 + 1 γ 2 δ 3 + 1 γ 3 • • • δ T + v γ T . (1.36) These equations may look awfully complicated, but in some special cases they are quite easy to use. For instance, in Section 2.1 we show how to use scaling with δ i ∈ {0, 1/2} and γ i ≡ 2, and explain the connection between δ i and the binary representation of b N and v. The next example shows another simple application of interval rescaling.  \n Approximate Arithmetic To understand how arithmetic coding can be implemented with fixed-precision we should note that the requirements for addition and for multiplication are quite different. We show that if we are willing to lose some compression efficiency, then we do not need exact multiplications. We use the double brackets ( [[ • ]]) around a multiplication to indicate that it is an approximation, i.e., [[α • β]] ≈ α • β. We define truncation as any approximation such that [[α • β]] ≤ α • β. The approximation we are considering here can be rounding or truncation to any precision. The following example shows an alternative way to interpret inexact multiplications. \n Example 7 ⊳ We can see in Figure  1 .3 that the arithmetic coding multiplications always occur with data from the source model-the probability p and the cumulative distribution c. Suppose we have l = 0.04, c = 0.317, and p = 0.123, with l × c = 0.04 × 0.317 = 0.01268, l × p = 0.04 × 0.123 = 0.00492. Instead of using exact multiplication we can use an approximation (e.g., with table look-up and short registers) such that [[l × c]] = [[0.04 × 0.317]] = 0.012, [[l × p]] = [[0.04 × 0.123]] = 0.0048. Now, suppose that instead of using p and c, we had used another model, with c ′ = 0.3 and p ′ = 0.12. We would have obtained l × c ′ = 0.04 × 0.3 = 0.012, l × p ′ = 0.04 × 0.12 = 0.0048, which are exactly the results with approximate multiplications. This shows that inexact multiplications are mathematically equivalent to making approximations in the source model and then using exact multiplications. ⊲  What we have seen in this example is that whatever the approximation used for the multiplications we can always assume that exact multiplications occur all the time, but with inexact distributions. We do not have to worry about the exact distribution values as long as the decoder is synchronized with the encoder, i.e., if the decoder is making exactly the same approximations as the encoder, then the encoder and decoder distributions must be identical (just like having dynamic sources, as explained in Section 1.6.1). v ′ ′ = 0.6689453125 v ˆ = 0.74267578125 Φ 0 s 1 = 2 Φ 1 Φ 2 Φ 5 Φ 3 Φ 6 s 2 = 1 s 3 =0 s 4 = 0 s 5 = 1 s 6 = 3 Φ 4 v′ = 0.0267578125 The version of (1.9) with inexact multiplications is Φ k (S) = | b k , l k = | b k−1 + [[c(s k ) • l k−1 ]], [[p(s k ) • l k−1 ]] , k = 1, 2, . . . , N. (1.37) We must also replace  (1.16 ) and (1.17) with ŝk (v) = {s : b k−1 + [[c(s) • l k−1 ]] ≤ v < b k−1 + [[c(s + 1) • l k−1 ]]} , k = 1, 2, . . . , N, (1.38) Φ k (v) = | b k , l k = | b k−1 + [[c(ŝ k (v)) • l k−1 ]], [[p(ŝ k (v)) • l k−1 ]] , k = 1, 2, . . . , N. (1.39) In the next section we explain which conditions must be satisfied by the approximate multiplications to have correct decoding. In equations (1.37) to (1.39) we have one type of approximation occurring from the multiplication of the interval length by the cumulative distribution, and another approximation resulting from the multiplication of the interval length by the probability. If we want to use only one type of approximation, and avoid multiplications between length and probability, we should update interval lengths according to l k = (b k−1 + [[c(s k + 1) • l k−1 ]]) − (b k−1 + [[c(s k ) • l k−1 ]]) . (1.40) The price to pay for inexact arithmetic is degraded compression performance. Arithmetic coding is optimal only as long as the source model probabilities are equal to the true data symbol probabilities; any difference reduces the compression ratios. A quick analysis can give us an idea of how much can be lost. If we use a model with probability values p ′ in a source with probabilities p, the average loss in compression is ∆ = M −1 n=0 p(n) log 2 p(n) p ′ (n) , bits/symbol. (1.41) This formula is similar to the relative entropy  [32] , but in this case p ′ represents the values that would result from the approximations, and it is possible to have M −1 n=0 p ′ (n) = 1. Assuming a relative multiplication error within ε, i.e., 1 − ε ≤ p(n) p ′ (n) ≤ 1 + ε, (1.42) we have ∆ ≤ M −1 n=0 p(n) log 2 (1 + ε) ≈ ε ln(2) ≈ 1.4 ε bits/symbol. (1.43) This is not a very tight bound, but it shows that if we can make multiplication accurately to, say 4 digits, the loss in compression performance can be reasonably small. 1.6. Arithmetic Coding Properties k b ¡ ]] ) 0 ( [[ k k l c b ⋅ + k k l b + ]] ) 2 ( [[ k k l c b ⋅ + ]] ) 1 ( [[ k k l c b ⋅ + ]] ) 1 ( [[ k k l M c b ⋅ − + ]] ) ( [[ k k l M c b ⋅ + v ˆ Unused intervals ("leakage") k k l c b ⋅ + ) 1 ( Figure 1 .7: Subdivision of a coding interval with approximate multiplications. Due to the fixed-precision arithmetic, we can only guarantee that all coding intervals are disjoint if we leave small regions between intervals unused for coding.  \n Conditions for Correct Decoding k + [[c(1) • l k ]] , but this difference does not lead to decoding errors if the decoder uses the same approximation. Decoding errors occur when condition (1.19) is not satisfied. Below we show the constraints that must be satisfied by approximations, and analyze the three main causes of coding error to be avoided. (a) The interval length must be positive and intervals must be disjoint. The constraints that guarantee that the intervals do not collapse into a single point, and that the interval length does not become larger than the allowed interval are 0 < l k+1 = [[p(s)•l k ]] ≤ (b k + [[c(s + 1) • l k ]])−(b k + [[c(s) • l k ]]) , s = 0, 1, . . . , M −1. (1.44) For example, if the approximations can create a situation in which [[c(s + 1) • l k ]] < [[c(s) • l k ]], there would be an non-empty intersection of subintervals assigned for s + 1 and s, and decoder errors would occur whenever a code value belongs to the intersection. If [[c(s + 1) • l k ]] = [[c(s) • l k ]] then the interval length collapses to zero, and stays as such, independently of the symbols coded next. The interval length may become zero due to arithmetic underflow, when both l k and p(s) = c(s + 1) − c(s) are very small. In Section 2.1 we show that interval rescaling is normally used to keep l k within a certain range to avoid this problem, but we also have to be sure that all symbol probabilities are larger than a minimum value defined by the arithmetic precision (see Sections (2.5) and (A.1)). Besides the conditions defined by (1.44), we also need to have [[c(0) • l k ]] ≥ 0, and [[c(M) • l k ]] ≤ l k . (1.45) These two condition are easier to satisfy because c(0) ≡ 0 and c(M) ≡ 1, and it is easy to make such multiplications exact. (b) Sub-intervals must be nested. We have to be sure that the accumulation of the approximation errors, as we continue coding symbols, does not move the interval base to a point outside all the previous intervals. With exact arithmetic, as we code new symbols, the interval base increases within the interval assigned to s k+1 , but it never crosses the boundary to the interval assigned to s k+1 + 1, i.e., b k+n = b k + k+n−1 i=k c(s i+1 ) • l i < b k + c(s k+1 + 1) • l k , for all n ≥ 0. (1.46) The equivalent condition for approximate arithmetic is that for every data sequence we must have b k + [[c(s k+1 + 1) • l k ]] > b k + [[c(s k+1 ) • l k ]] + ∞ i=k+1 [[c(s i+1 ) • l i ]]. (1.47) To determine when (1.47) may be violated we have to assume some limits on the multiplication approximations. There should be a non-negative number ε such that [[c(s i+1 ) • l i ]](1 − ε) < c(s i+1 ) • l i . (1.48) We can combine (1.40), (1.47) and (1.48) to obtain (1 − ε) • l k > ∞ i=k+1 c(s i+1 ) • l i , (1.49) which is equal to 1 − ε > c(s k+2 ) + p(s k+3 ) (c(s k+3 ) + p(s k+3 ) (c(s k+4 ) + p(s k+4 ) (• • •))) . (1.50) To find the maximum for the right-hand side of (1.50) we only have to consider the case s k+2 = s k+3 = . . . = M − 1 to find 1 − ε > c(M − 1) + p(M − 1) (c(M − 1) + p(M − 1) (c(M − 1) + p(M − 1) (• • •))) , (1.51) which is equivalent to 1 − ε > c(M − 1) + p(M − 1). (1.52) But we know from (1.3) that by definition c(M − 1) + p(M − 1) ≡ 1! The answer to this contradiction lies in the fact that with exact arithmetic we would have equality in  (1.46)  only after an infinite number of symbols. With inexact arithmetic it is impossible to have semi-open intervals that are fully used and match perfectly, so we need to take some extra precautions to be sure that (1.47) is always satisfied. What equation (1.52) tells us is that we solve the problem if we artificially decrease the interval range assigned for p(M − 1). This is equivalent to setting aside small regions, indicated as gray areas in Figure  1 .7, that are not used for coding, and serve as a "safety net." This extra space can be intentionally added, for example, by replacing (1.40) with l k = (b k−1 + [[c(s k + 1) • l k−1 ]]) − (b k−1 + [[c(s k ) • l k−1 ]]) − ζ (1.53) where 0 < ζ ≪ 1 is chosen to guarantee correct coding and small compression loss. The loss in compression caused by these unused subintervals is called "leakage" because a certain fraction of bits is "wasted" whenever a symbol is coded. This fraction is on average ∆ s = p(s) log 2 p(s) p ′ (s) bits, (1.54) where p(s)/p ′ (s) > 1 is the ratio between the symbol probability and the size of interval minus the unused region. With reasonable precision, leakage can be made extremely small. For instance, if p(s)/p ′ (s) = 1.001 (low precision) then leakage is less than 0.0015 bits/symbol. (c) Inverse arithmetic operations must not produce error accumulation. Note that in (1.38) we define decoding assuming only the additions and multiplications used by the encoder. We could have used ŝk (v) = s : c(s) ≤ v − b k−1 l k−1 < c(s + 1) , k = 1, 2, . . . , N. (1.55) However, this introduces approximate subtraction and division, which have to be consistent with the encoder\'s approximations. Here we cannot possibly cover all problems related to inverse operations, but we should say that the main point is to observe error accumulation. For example, we can exploit the fact that in (1.16) decoding only uses the difference ̟ k ≡ v − b k , and use the following recursions. | ̟ 0 , l 0 = | v, 1 , (1.56) ŝk = {s : [[c(s) • l k−1 ]] ≤ ̟ k < [[c(s + 1) • l k−1 ]]} , k = 1, 2, . . . , N. (1.57) | ̟ k , l k = | ̟ k−1 − [[c(ŝ k ) • l k−1 ]], [[p(ŝ k ) • l k−1 ]] , k = 1, 2, . . . , N. (1.58) However, because we are using a sequence of subtractions in  (1.58) , this technique works with integer arithmetic implementations (see Appendix A), but it may not work with floatingpoint implementations because of error accumulation. \n Chapter 2 Arithmetic Coding Implementation In this second part, we present the practical implementations of arithmetic coding. We show how to exploit all the arithmetic coding properties presented in the previous sections and develop a version that works with fixed-precision arithmetic. First, we explain how to implement binary extended-precision additions that exploit the arithmetic coding properties, including the carry propagation process. Next, we present complete encoding and decoding algorithms based on an efficient and simple form of interval rescaling. We provide the description for both floating-point and integer arithmetic, and present some alternative ways of implementing the coding, including different scaling and carry propagation strategies. After covering the details of the coding process, we study the symbol probability estimation problem, and explain how to implement adaptive coding by integrating coding and source modeling. At the end, we analyze the computational complexity of arithmetic coding. \n Coding with Fixed-Precision Arithmetic Our first practical problem is that the number of digits (or bits) required to represent the interval length exactly grows when a symbol is coded. For example, if we had p(0) = 0.99 and we repeatedly code symbol 0, we would have l 0 = 1, l 1 = 0.99, l 2 = 0.9801, l 3 = 0.970299, l 4 = 0.96059601, . . . We solve this problem using the fact we do not need exact multiplications by the interval length (Section 1.6.5). Practical implementations use P -bit registers to store approximations of the mantissa of the interval length and the results of the multiplications. All bits with significance smaller than those in the register are assumed to be zero. With the multiplication precision problem solved, we still have the problem of implementing the additions in (1.37) when there is a large difference between the magnitudes of the interval base and interval length. We show that rescaling solves the problem, simultaneously enabling exact addition, and reducing loss of multiplication accuracy. For a binary output, we can use rescaling in the form of (1.31), with δ ∈ {0, 1/2} and γ = 2 whenever the length of the interval is below 1/2. Since the decoder needs to know the rescaling parameters, they are saved in the data buffer d, using bits "0" or "1" to indicate whether δ = 0 or δ = 1/2. Special case δ = 1 and γ = 1, corresponding to a carry in the binary representation, is explained later. To simplify the notation we represent the rescaled intervals simply as | b, l (no subscripts), and the rescaled code value as v. l = 2 t(l k ) l k , b = frac(2 t(l k ) b k ) = 2 t(l k ) b k − 2 t(l k ) b k , (2.1) v = frac(2 t(l k ) v), where frac(•) is the fractional part of a number and t(x) = {n : 2 −n−1 < x ≤ 2 −n } = ⌊− log 2 (x)⌋ . (2.2) Note that under these conditions we have b ∈ [0, 1) and l ∈ (0.5, 1], and for that reason the rescaling process is called renormalization. Of course, l, b, and v change with k, but this new notation is more convenient to represent variables in algorithm descriptions or computer programs. The binary representations of the interval base and length have the following structure: l k = 0.0000 . . . 00 0000 . . . 00 where symbol a represents an arbitrary bit value. (L=2 P l) We can see in (2.3) that there is "window" of P active bits, forming integers L and B, corresponding to the nonzero bits of l k , and the renormalized length l. Because the value of l is truncated to P -bit precision, there is a set of trailing zeros that does not affect the additions. The bits to the left of the active bits are those that had been saved in the data buffer d during renormalization, and they are divided in two sets. The first set to the left is the set of outstanding bits: those that can be changed due to a carry from the active bits when new symbols are encoded. The second is the set of bits that have been settled, i.e., they stay constant until the end of the encoding process. This happens because intervals are nested, i.e., the code value cannot exceed This equation shows that only the outstanding bits may change due to a carry from the active bits. Furthermore, inequality (2.4) also shows that there can be only one carry that would change these bits. If there is a carry, or when it is found that there can be no carry, these bits become settled. For that reason, the set of outstanding bits always start with 0, and is possibly followed only by 1s. As new symbols are encoded, all sets move to the right. \n Algorithm 1 Function Arithmetic Encoder (N, S, M, c, d)  1. set { b ← 0; l ← 1; ⋆ Initialize interval t ← 0; } ⋆ and \n Implementation with Buffer Carries Combining all we have seen, we can present an encoding algorithm that works with fixedprecision arithmetic. Algorithm 1 shows a function Arithmetic Encoder to encode a sequence S of N data symbols, following the notation of Section 1.2. This algorithm is very similar to the encoding process that we used in Section 1.4, but with a renormalization stage after each time a symbol is coded, and the settled and outstanding bits being saved in the buffer d. The function returns the number of bits used to compress S. In Algorithm 1, Step 1 sets the initial interval equal to [0, 1), and initializes the bit counter t to zero. Note that we use curly braces ({ }) to enclose a set of assignments, and use symbol "⋆" before comments. In Step 2, we have the sequential repetition of interval resizing and renormalizations. Immediately after updating the interval we find out if there is a carry, i.e., if b ≥ 1, and next we check if further renormalization is necessary. The encoding process finishes in Step 3, when the final code value v that minimizes the number of code bits is chosen. In all our algorithms, we assume that functions receive references, i.e., variables can be changed inside the function called. Below we describe each of the functions used by Algorithm 1. There are many mathematically equivalent ways of updating the interval | b, l . We do not need to have both vectors p and c stored to use (1.9). In Algorithm 2 we use (1.40) to update length as a difference, and we avoid multiplication for the last symbol (s = M − 1), since it is more efficient to do the same at the decoder. To simplify notation, we do not use double brackets to indicate inexact multiplications, but it should be clear that here all numbers represent the content of CPU registers. In Step 2.2.1 of Algorithm 1, the function to propagate the carry in the buffer d is called, changing bits that have been added to d previously, and we shift the interval to have b < 1. Figure  2 .1 shows the carry propagation process. Active bits are shown in bold and outstanding bits are underlined. Whenever there is a carry, starting from the most recent bits added to buffer d, we complement all bits until the first 0-bit is complemented, as in \n Algorithm 2 Procedure Interval Update (s, b, l, M, c) Algorithm 4 implements interval renormalization, where we test if active bits became outstanding or settled. While the interval length is smaller than 0.5, the interval is rescaled by a factor of two, and a bit is added to the bit buffer d. 1. if s = M − 1 ⋆ Special case for last symbol then set y ← b + l; ⋆ end of interval else set y ← b + l • c(s + 1); ⋆ base of next subinterval 2. set { b ← b + l • c(s); ⋆ Update interval base l ← y − b; } ⋆ Update interval length as difference 3. return. • 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 b k−1 [[l k−1 • c(s k )]] b k = b k−1 + [[l k−1 • c(s k )]] The final encoding stage is the selection of the code value. We use basically the same process explained at the end of Section 1.4.1, but here we choose a code value belonging to the rescaled interval. Our choice is made easy because we know that, after renormalization, we always have 0.5 < l ≤ 1 (see (2.1)), meaning that we only need an extra bit to define the final code value. In other words: all bits that define the code value are already in buffer d, and we only need to choose one more bit. The only two choices to consider in the rescaled interval are v = 0.5 or v = 1. The decoding procedure, shown in Algorithm 6, gets as input the number of compressed symbols, N, the number of data symbols, M, and their cumulative distribution c, and the array with the compressed data bits, d. Its output is the recovered data sequence Ŝ. The decoder must keep the P -bit register with the code value updated, so it will read P extra bits at the end of d. We assume that this can be done without problems, and that those bits had been set to zero. The interval selection is basically an implementation of (1.38): we want to find the subinterval that contains the code value v. The implementation that we show in Algorithm 7 has one small shortcut: it combines the symbol decoding with interval updating (1.39) in a single function. We do a sequential search, starting from the last symbol (s = M − 1), because we assume that symbols are sorted by increasing probability. The advantages of Algorithm 3 Procedure Propagate Carry (t, d)  1. set { b ← 0; l ← 1; ⋆ Initialize interval v = P n=1 2 −n d(n); ⋆ Read P bits of code value t ← P ; } ⋆ Initialize bit counter 2. for k = 1 to N do ⋆ Decode N data symbols 2.1. ŝk = Interval Selection (v, b, l, M, c); ⋆ Decode symbol and update interval 2.2. if b ≥ 1 then ⋆ Check for "overflow" 2.2.1. set { b ← b − 1; ⋆ shift interval base v ← v − 1; } ⋆ shift code value 2.3. if l ≤ 0.5 then ⋆ If interval is small enough 2.3.1. Decoder Renormalization (v, b, l, t, d); ⋆ then renormalize interval 3. return. • Algorithm 7 Function Interval Selection (v, b, l, M, c) 1. set { s ← M − 1; ⋆ Start search from last symbol x ← b + l • c(M − 1); ⋆ Base of search interval y ← b + l; } ⋆ End of search interval 2. while x > v do ⋆ Sequential search for correct interval 2.1. set { s ← s − 1; ⋆ Decrement symbol by one y ← x; ⋆ move interval end x ← b + l • c(s); } ⋆ compute new interval base 3. set { b ← x; ⋆ Update interval base l ← y − b; } ⋆ Update interval length as difference 4. return s. • sorting symbols and more efficient searches are explained in Section 2.2. In Algorithm 7 we use only arithmetic operations that are exactly equal to those used by the encoder. This way we can easily guarantee that the encoder and decoder approximations are exactly the same. Several simplifications can be used to reduce the number of arithmetic operations (see Appendix A). The renormalization in Algorithm 8 is similar to Algorithm 4, and in fact all its decisions (comparisons) are meant to be based on exactly the same values used by the encoder. However, it also needs to rescale the code value in the same manner as the interval base (compare (1.35) and (1.36)), and it reads its least significant bit (with value 2 −P ).  ⋆ scale interval base v ← 2v; } ⋆ scale code value 1.2. set { t ← t + 1; ⋆ Increment bit counter v ← v + 2 −P d(t); ⋆ Set least significant bit of code value l ← 2l; } ⋆ Scale interval length 2. return. • the results of Algorithm 2. We indicate the interval changes during renormalization (Algorithm 4) by showing the value of δ used for rescaling, according to  (1.31) . Comparing these results with those in Table  1 .2, we see how renormalization keeps all numerical values within a range that maximizes numerical accuracy. In fact, the results in Table  2 .1 are exact, and can be shown with a few significant digits. Table  2 .1 also shows the decoder\'s updated code value. Again, these results are exact and agree with the results shown in Table  1 .2. The third column in Table  2 .1 shows the contents of the bit buffer d, and the bits that are added to this buffer every time the interval is rescaled. Note that carry propagation occurs twice: when s = 3, and when the final code value v = 1 is chosen by Algorithm 5. ⊲ \n Implementation with Integer Arithmetic Even though we use real numbers to describe the principles of arithmetic coding, most practical implementations use only integer arithmetic. The adaptation is quite simple, as we just have to assume that the P -bit integers contain the fractional part of the real numbers, with the following adaptations. (Appendix A has the details.) • Define B = 2 P b, L = 2 P l, V = 2 P v, and C(s) = 2 P c(s). Products can be computed with 2P bits, and the P least-significant bits are discarded. For example, when updating the interval length we compute L ← L • [C(s + 1) − C(s)] • 2 −P . The length value l = 1 cannot be represented in this form, but this is not a real problem. We only need to initialize the scaled length with L ← 2 P − 1, and apply renormalization only when l < 0.5 (strict inequality). • The carry condition b ≥ 1 is equivalent to B ≥ 2 P , which can mean integer overflow. It can be detected accessing the CPU\'s carry flag, or equivalently, checking when the value of B decreases. • Since l > 0 we can work with a scaled length equal to L ′ = 2 P l − 1. This way we can represent the value l = 1 and have some extra precision if P is small. On the other hand, updating the length using L ′ ← (L ′ + 1) • [C(s + 1) − C(s)] • 2 −P − 1 requires two more additions. When multiplication is computed with 2P bits, we can determine what is the smallest allowable probability to avoid length underflow. Since renormalization guarantees that L ≥ 2 P −1 , we have (1.44) satisfied if [C(s + 1) − C(s)] 2 −P L ≥ [C(s + 1) − C(s)] 2 −1 ≥ 1 ⇒ C(s + 1) − C(s) ≥ 2, (2.5) Meaning that the minimum probability supported by such implementations with P -bit integers is p(s) ≥ 2  \n Example 9 ⊳ Table  2 .2 shows the results of an 8-bit register implementation (P = 8) applied to the data sequence and source used in Example 3. The table shows the interval | b k , l k in base-2 notation to make clear that even though we use 8-bit arithmetic, we are actually implementing exact additions. Following the conventions in (2.3) and Figure  2 .1, we used bold letters to indicate active bits, and underlined for outstanding bits. We also show the approximate results of the multiplications l k−1 • c(s k ). The last column shows the binary contents of the registers with active bits. We used L ′ = 2 P l − 1 to represent the length. The results are also shown in decimal notation so that they can be compared with the exact results in Table  1 .2. Note that the approximations change the code value after only 7 bits, but the number of bits required to represent the final code value is still 13 bits. ⊲ Algorithm 9 Procedure Encoder Renormalization (b, l, t, d) 1. while l ≤ 1/D do ⋆ Renormalization loop 1.1. set { t ← t + 1; } ⋆ Increment symbol counter d(t) ← ⌊D • b⌋; ⋆ Output symbol from most significant bits b ← D • b − d(t); ⋆ Update interval base l ← D • l; } ⋆ Scale interval length 2. return. • \n Efficient Output Implementations with short registers (as in Example 9) require renormalizing the intervals as soon as possible to avoid losing accuracy and compression efficacy. We can see in Table  2 .1 that, as a consequence, intervals may be rescaled many times whenever a symbol is coded. Even though rescaling can be done with bit shifts instead of multiplications, this process still requires many CPU cycles (see Section 2.3). If we use longer registers (e.g., 16 bits or more), we can increase efficiency significantly by moving more than one bit to the output buffer d whenever rescaling. This process is equivalent to have an encoder output alphabet with D symbols, where D is a power of two. For example, moving groups of 1, 2, 4, or 8 bits at a time, corresponds to output alphabets with D = 2, 4, 16, and 256 symbols, respectively. Carry propagation and the use of the output buffer d also become more efficient with larger D. It is not difficult to modify the algorithms in Section 2.1.1 for a D-symbol output. Renormalization is the most important change. The rescaling parameters defined by  (1.31)  are such that δ ∈ {0, 1/D, 2/D, . . . , 1} and γ ≡ D. Again, δ = 1 corresponds to a carry. Algorithm 9 has the required changes to Algorithm 4, and the corresponding changes in the decoder renormalization are shown in Algorithm 10. In Appendix A we have an integer arithmetic implementation with D-symbol output, with the carry propagation in Algorithm 25. We also need to change the equations that define the normalized intervals, (2.1) and (2.2), to    Algorithms 9 and 10 can also be used for values of D that are not powers of two, but with inefficient radix-D arithmetic. The problem is that while the multiplications by the interval length can be approximated, rescaling has to be exact. For example, if D = 2 then multiplication by 2 −P is computed exactly with bit-shifts, but exact multiplication by 3 −P requires special functions  [39] . l = D t(l k ) l k , b = frac(D t(l k ) b k ) = D t(l k ) b k − D t(l k ) b k , (2.6) v = frac(D t(l k ) v), and t(x) = {n : D −n−1 < x ≤ D −n } = ⌊− log D (x)⌋ . ( 2 1. while l ≤ 1/D do ⋆ Renormalization loop 1.1. set { t ← t + 1; } ⋆ Increment symbol counter a ← ⌊D • b⌋; ⋆ Most significant digit b ← D • b − a; ⋆ Update interval base v ← D • v − a + D −P d( 1 v = 1 1 - BE1 - δ = 1 0 - BE2 - δ = 0 - - BE20 - ✲ ✲ ✛ ✛ ❄ Ω p(Ω) = [p(0) p(1) • • • p(M − 1)] p(Ω) = [p(0) p(1) • • • p(M − 1)] p(D) = [1/D 1/D • • • 1/D] p(D) = [1/D 1/D • • • 1/D] A better alternative is to use a small trick, shown in Figure  2 .2. We can compress the data sequence using a standard encoder/decoder with binary arithmetic. Next, we "decode" the binary compressed data using a D-symbol alphabet with uniform probability distribution p = [ 1/D 1/D • • • 1/D ]. The uniform distribution does not change the distribution of the code values (and thus will not alter compression), but converts the data to the desired alphabet. Since both processes implemented by the decoder are perfectly reversible, the decoder only has to implement the inverse processes. This takes twice the time for encoding and decoding, but is significantly faster than using radix-D arithmetic. \n Care with Carries We have seen in Section 2.1 that carry propagation is applied only to the set of outstanding bits, which always start with a 0-bit, and is followed possibly by 1-bits. Examples of set of outstanding bits are ∅, {0}, {0, 1}, {0, 1, 1}, {0, 1, 1, 1}, • • • , {0, 1, 1, 1, . . . , 1}. Clearly, with such simple structure we do not have to save the outstanding bits to know what they are. We can just keep a counter with the number of outstanding bits, which is incremented as new bits are defined during renormalization. We can output (or "flush") these bits whenever a carry occurs or when a new outstanding 0-bit comes from renormalization. Note that not all bits put out by rescaling have to be outstanding before becoming settled. For example, if we have b < b + l ≤ 0.5, we not only know that the next bit is zero, but we know that it cannot possibly be changed by a carry, and is thus settled. We can disregard these details when implementing carry propagation in buffers, but not when using counters. With D-symbol output the set of outstanding symbols starts with a symbol σ = D − 1, and is possibly followed by several occurrences of the symbol D − 1, as shown below. ∅, {σ}, {σ, D − 1}, {σ, D − 1, D − 1}, • • • , {σ, D − 1, D − 1, . . . , D − 1}. In this case we can only keep the first symbol σ and a counter with the number of outstanding symbols. It is important to know that the number of outstanding symbols can grow indefinitely, i.e., we can create distributions and infinitely long data sequences such that bits never become settled. The final code value selection (Algorithm 5) settles all outstanding symbols, so that we can periodically restart the encoder to limit the number of outstanding symbols. There are many choices for dealing with carry propagation. The most common are: 1. Save the outstanding symbols temporarily to a buffer, and then implement carry propagation in the buffer. This simple technique is efficient when working with bytes (D = 256), but can be inefficient when D is small. It can only be used if we know that the buffer is large enough to fit all the compressed symbols, since all of them can be outstanding. 2. Use a counter to keep track of the outstanding symbols, saving all symbols to a buffer or file as soon as they become settled. There is chance of overflow depending on the number of bits used by the counter (mantissas with 32, 53, and 64 bits allow, respectively, 4 • 10 9 , 9 • 10 15 , and 2 • 10 19 outstanding symbols). In practical applications a counter overflow is extremely improbable, specially with adaptive coders. 3. Use carry propagation in a buffer, plus "bit-stuffing"  [15, 27]  in the following form. Outstanding bits are moved out from the buffer as soon as they become settled. Whenever the number of 1-bits exceeds a threshold (e.g.,  16 ), an artificial zero is added to the compressed bit sequence, forcing the outstanding bits to become settled. The decoder can identify when this happens by comparing the number of consecutive 1-bits read. When it exceeds the threshold, the decoder interprets the next bit not as data, but as carry information. If it is 1, then a carry is propagated in the decoder buffer, and the decoding process continues normally. This technique is used by the Q-coder  [27] . \n Alternative Renormalizations We show in Section 1.6.4 that there is a great deal of flexibility in rescaling the intervals, and we present in Section 2.1.1 an implementation based on a particular form of renormalization. Other choices lead to renormalized intervals with distinct properties, which had been exploited in several manners by different arithmetic coding implementations. Below we show a few examples. • We have chosen renormalization (2.6), which produces intervals such that b ∈ [0, 1), l ∈ (1/D, 1], and b + l ∈ (1/D, 2). Its main advantage is that it simplifies carry detection and renormalization, especially when D > 2. Note that it has a criterion for when to renormalize that is based only on the interval length. • The decision of when to renormalize can be based on settled symbols. For example, the method by Rubin  [13]  keeps intervals such that b ∈ [0, 1), and b + l ∈ (0, 1], which are renormalized when the most significant symbol becomes settled, i.e., ⌊Db⌋ = ⌊D(b + l)⌋, meaning that the outstanding symbols are kept in the registers. To avoid the interval length eventually collapsing to zero, the encoder and decoder rescale and shift the interval when its length gets too small, forcing outstanding symbols to become settled. • Witten, Neal, and Cleary  [25, 51]  proposed an arithmetic coding implementation that became quite popular. Instead of the base length, it uses the extremes points to represent the interval, and keeps it renormalized in such a manner that b + l ∈ (0, 1]. Renormalization occurs whenever bits are settled, and also when 0.25 ≤ b < b + l ≤ 0.75. Thus, it avoids the precision loss that occurs in  [13] , and uses counters to keep track of the number of outstanding bits. This technique can be adapted to D-symbol output, but it is not as simple as what we have in Appendix A. • The binary Q-coder developed by IBM  [27, 28] , keeps intervals with l ∈ (0.75, 1.5]. This way we normally have l ≈ 1, and we can approximate multiplications with p•l ≈ p, and (1 − p) • l ≈ l − p. Variations include the QM  [36]  and MQ-coder  [62] . \n Adaptive Coding Since typical information sources tend to be quite complex, we must have a good model of the data source to achieve optimal compression. There are many techniques for modeling complex sources  [14, 25, 29, 44, 45, 46, 49]  that decompose the source data in different categories, under the assumption that in each category the source symbols are approximately independent and identically distributed, and thus well suited to be compressed with arithmetic coding. In general, we do not know the probabilities of the symbols in each category. Adaptive coding is the estimation of the probabilities of the source symbols during the coding process. In this section we study techniques to efficiently combine arithmetic coding with dynamic probability estimation. \n Strategies for Computing Symbol Distributions The most efficient technique for computing distributions depends on the data type. When we are dealing with completely unknown data we may want adaptation to work in a completely automatic manner. In other cases, we can use some knowledge of the data properties to reduce or eliminate the adaptation effort. Below we explain the features of some of the most common strategies for estimating distributions. • Use a constant distribution that is available before encoding and decoding, normally estimated by gathering statistics in a large number of typical samples. This approach can be used for sources such as English text, or weather data, but it rarely yields the best results because few information sources are so simple as to be modeled by a single distribution. Furthermore, there is very little flexibility (e.g., statistics for English text do not fit well Spanish text). On the other hand, it may work well if the source model is very detailed, and in fact it is the only alternative in some very complex models in which meaningful statistics can only be gathered from a very large amount of data. • Use pre-defined distributions with adaptive parameter estimation. For instance, we can assume that the data has Gaussian distribution, and estimate only the mean and variance of each symbol. If we allow only a few values for the distribution parameters, then the encoder and decoder can create several vectors with all the distribution values, and use them according to their common parameter estimation. See ref.  [49]  for an example. • Use two-pass encoding. A first pass gathers the statistics of the source, and the second pass codes the data with the collected statistics. For decoding, a scaled version of vectors p or c must be included at the beginning of the compressed data. For example, a book can be archived (compressed) together with its particular symbol statistics. It is possible to reduce the computational overhead by sharing processes between passes. For example, the first pass can simultaneously gather statistics and convert the data to run-lengths. • Use a distribution based on the occurrence of symbols previously coded, updating c with each symbol encoded. We can start with a very approximate distribution (e.g., uniform), and if the probabilities change frequently, we can reset the estimates periodically. This technique, explained in the next section, is quite effective and the most convenient and versatile. However, the constant update of the cumulative distribution can increase the computational complexity considerably. An alternative is to update only the probability vector p after each encoded symbol, and update the cumulative distribution c less frequently (Section 2.2.5). \n Direct Update of Cumulative Distributions After encoding/decoding k symbols the encoder/decoder can estimate the probability of a symbol as p(m) = P (m) k + M , m = 0, 1, 2, . . . , M − 1, (2.8) where P (m) > 0 is the number of times symbol m was encoded/decoded plus one (added to avoid zero probabilities). The symbol occurrence counters are initialized with P (m) = 1, and incremented after a symbol is encoded. We define the cumulative sum of occurrences as C(m) = m−1 i=0 P (i), m = 0, 1, 2, . . . , M, (2.9) and the cumulative distribution as c(m) = C(m) k + M = C(m) C(M) , m = 0, 1, 2, . . . , M (2.10) Algorithm 13 Function Interval Selection (v, b, l, M, C) 1. set { s ← M − 1; ⋆ Initialize search from last symbol γ = l/ C(M ) ⋆ Compute division result W ← (v − b)/γ; } ⋆ Code value scaled by C(M ) 2. while C(s) > W do ⋆ Look for correct interval 2.1. set s ← s − 1; ⋆ decrement symbol by one 3. set { b ← b + γ • C(s); ⋆ Update interval base l ← γ • [ C(s + 1) − C(s)]; } ⋆ Update interval length 4. return s. • Algorithm 14 Procedure Update Distribution (s, M, C) 1. for m = s + 1 to M do ⋆ For all symbols larger than s 1.1 set C(s) ← C(s) + 1; ⋆ increment cumulative distribution 2. return. • After the modifications above, Algorithms 1 and 6 are made adaptive by adding the following line: 2.4. Update Distribution (s k , M, C); The procedure to update the cumulative distribution is shown in Algorithm 14. Note that in this algorithm it is necessary to compute up to M additions. Similarly, in Step 2 of Algorithm 13 we have to perform up to M comparisons and subtractions. Since the number of operations in both cases decreases with the symbol number, it is good to sort the symbol by increasing probability. Reference  [25]  presents an implementation that simultaneously updates the distribution while keeping it sorted. \n Binary Arithmetic Coding Binary arithmetic coders work only with a binary source alphabet (M = 2). This is an important type of encoder because it helps to solve many of the complexity issues created with the dynamic update of the cumulative distributions (Algorithm 14). When M = 2 the cumulative distribution vector is simply c = [ 0 p(0) 1 ], which makes coding and updating the cumulative distribution much simpler tasks. However, it is important to observe that there is a performance trade-off here. While binary arithmetic coding greatly simplifies coding each binary data symbol, its final throughput of information (actual information bits) cannot be larger than one bit per coded symbol, which normally means one bit per few CPU clock cycles  [63] . Consequently, they are not as attractive for fast coding as they used to be, but there is no reason not to use their special properties when coding binary sources. Algorithms 15, 16, and 17, have the procedures for, respectively, binary encoding (inter-Algorithm 15 Procedure Binary Interval Update (s, b, l, C(1), C(2)) 1. set x ← l • C(1)/ C (2); ⋆ Point for interval division 2. if s = 0 ⋆ If symbol is zero then set l ← x; ⋆ Update interval length, else set { b ← b + x; ⋆ move interval base and l ← l − x; } ⋆ update interval length 3. return. • Algorithm 16 Function Binary Interval Selection (v, b, l, C(1), C(2)) 1. set x ← l • C(1)/ C (2); ⋆ Point for interval division 2. if b + x > v ⋆ Look for correct interval then set { s ← 0; ⋆ Symbol is 0: no change to interval base l ← x; } ⋆ update interval length else set { s ← 1; ⋆ Symbol is 1: b ← b + x; ⋆ move interval base and l ← l − x; } ⋆ update interval length 3. return s. • val update), decoding (interval selection and update), and distribution update. Note that instead of using the full vector C we use only C(1) = P (0) and C(2) = P (0) + P (1). The renormalization procedures do not have to be changed for binary arithmetic coding. \n Example 11 ⊳ Binary arithmetic coding has universal application because, just as any number can be represented using bits, data symbols from any alphabet can be coded as a sequence of binary symbols. Figure  2 .3 shows how the process of coding data from a 6-symbol source can be decomposed in a series of binary decisions, which can be represented as a binary search tree  [30] . The leaf nodes correspond to the date source symbols, and intermediate nodes correspond to the decisions shown below them. Underlined numbers are used for the intermediate nodes, and their value corresponds to the number used for the comparison (they are the "keys" for the binary search tree  [30] ). For instance, node m corresponds to test "s < m?" Symbols are coded starting from the root of the tree, and continue until a leaf node is encountered. For example, if we want to code the symbol s = 2, we start coding the information "s < 3?" indicated by node 3; next we go to node 1, and code the information "s < 1?", and finally move to node 2 to code "s < 2?". At each node the information is coded with a different set of probabilities, which in Figure  2 .3 are shown below the tree nodes. These probabilities, based on the number of symbol occurrences, are updated with Algorithm 17. An alphabet with M symbols needs M − 1 probability \n Algorithm 17 Procedure Update Binary Distribution (s, C) 1. if s = 0 then set C(1) ← C(1) + 1; ⋆ If s = 0 then increment 0-symbol counter 2. set C(2) ← C(2) + 1; ⋆ Increment symbol counter 3. return. • estimates for the intermediate nodes. The decoder follows the same order, using the same set of probabilities. (Note that this is equivalent to using the scheme explained in Section 1.6.3, and shown in Figure  1 .5.) There is no loss in compression in such scheme. For example, when coding symbol s = 2 we can compute the symbol probability as a product of conditional probabilities  [22] . Prob(s = 2) = Prob(s < 3) • Prob(s = 2|s < 3) = Prob(s < 3) • Prob(s ≥ 1|s < 3) • Prob(s = 2|1 ≤ s < 3) (2.13) = Prob(s < 3) • Prob(s ≥ 1|s < 3) • Prob(s ≥ 2|s ≥ 1, s < 3) This means that log 2 [Prob(s = 2)] = log 2 [Prob(s < 3)] + log 2 [Prob(s ≥ 1|s < 3)] + (2.14) + log 2 [Prob(s ≥ 2|1 ≤ s < 3)] The left-hand-side of (2.14) is the number of bits required to code symbol s = 2 directly with a 6-symbol model, which is equal to the sum of the bits used to code the same symbol by successively coding the binary symbols in the binary-search tree (see Figure  2 .3). We do not have to worry about computing explicit values for the conditional probabilities because, when we use a different adaptive binary model for each node, we automatically get the estimate of the proper conditional probabilities. This property is valid for binary-tree decompositions of any data alphabet. ⊲ A binary-search tree as in Figure  2 .3 can be automatically generated using, for example, the bisection algorithm  [17, 19, 39] . Algorithms 18 and 19 show such implementations for encoding, decoding, and overall probability estimation updates. Note that they use the binary coding and decoding functions of Algorithms 15 and 16, and use only one vector, C, with dimension M − 1, to store the tree-branch occurrence counters. Each component C(m), 0 < m < M, contains the number of the number of times we had "s < m?" on tree node m, and C(0) contains the total number of symbols coded. This vector is initialized with the number of leaf nodes reachable from the left branch of the corresponding node. The binary conditional probabilities estimates are computed during the encoding and decoding directly from C. For example, in the example of  Since we use bisection search in Algorithms 18 and 19, the number of times the binary encoding and decoding functions are called is between ⌊log 2 M ⌋ and ⌈log 2 M⌉, for all data symbols. Thus, by using binary-tree searches and binary arithmetic coding we greatly reduce the worst-case complexity required to update probability estimates. Y N ❅ ❅ ❅ ■ ✒ 2 ✖✕ ✗✔ s < 2? (8/18) Y N ❅ ❅ ❅ ■ ✒ 4 ✖✕ ✗✔ s < 4? (15/99) Y N ❅ ❅ ❅ ■ ✒ 5 ✖✕ ✗✔ s < 5? (19/84) Y N ❅ ❅ ❅ ■ ✒ 3 ✖✕ ✗✔ s < 3? (22/121) Y N ❍ ❍ ❍ ❍ ❍ ❍ ❍ ( ✟ ✟ ✟ ✟ ✟ ✟ ✟ ✯ \n Example 12 ⊳ Figure  2 .4 shows a second example of a binary-search tree that can be used to code the 6-symbol data of Example 10. Different search algorithms can be used to create different trees. For example, we could have used a sequential search, composed of tests, "s < 5?", "s < 4?", "s < 3?", . . . , "s < 1?" The trees created from bisection search minimize the maximum number of binary symbols to be coded, but not the average. The tree of Figure  2 .4 was designed so that the most frequent symbols are reached with the smallest number of tests. Table  2 .  4  shows the average number of symbols required for coding symbols from the source of \n Algorithm 18 Procedure Interval Update (s, b, l, M, C)  Because we have the symbols sorted by increasing probability, the performance of the tree defined by sequential search, starting from the most probable symbol, is quite good. The tree of Figure  2 .4 is the one that minimizes the average number of coded binary symbols. Below we explain how it is designed. ⊲ Prefix coding  [4, 5, 21, 32, 55, 56]  is the process of coding information using decision trees as defined above. The coding process we have shown above is identical, except that we call a binary arithmetic encoding/decoding function at each node. Thus, we can use all the known facts about prefix coding to analyze the computational complexity of binary arithmetic encoding/decoding, if we measure complexity by the number of coded binary symbols. 1. set { u ← 0; n ← M ; ⋆ Initialize bisection search limits k ← C(0); ⋆ First divisor = symbol counter C(0) ← C(0) + 1; } ⋆ Increment symbol counter 2. while n − u > 1 do ⋆ Bisection search loop 2.1. set m ← ⌊(u + n)/2⌋; ⋆ Compute middle point 2.2. if s < m ⋆ If symbol is smaller than middle then set { n ← m; ⋆ then update upper limit Binary Interval Update (0, b, l, C(m), k); ⋆ code symbol 0 k ← C(m); ⋆ set next divisor C(m) ← C(m) + 1; } ⋆ increment 0-symbol counter else set { u ← m; ⋆ else update lower limit Binary Interval Update (1, b, l, C(m), k); ⋆ code symbol 1 k ← k − C(m); } ⋆ set next divisor 3. return. • Algorithm 19 Function Interval Selection (v, b, l, M, C) 1. set { s ← 0; n ← M ; ⋆ Initialize bisection search bounds k ← C(0); ⋆ First divisor = symbol counter C(0) ← C(0) + 1; } ⋆ Increment symbol counter 2. while n − s > 1 do ⋆ Bisection search loop 2.1. set m ← ⌊(s + n)/2⌋; ⋆ Compute middle point 2.2. if Binary Interval Selection (v, b, l, C(m), k) = 0 ⋆ If Y N ❍ ❍ ❍ ❍ ❍ ❍ ❍ ( ✟ ✟ ✟ ✟ ✟ ✟ ✟ ✯ Since the optimal trees for prefix coding are created using the Huffman algorithm  [2] , these trees are also optimal for binary arithmetic encoding/decoding  [23] . Strictly speaking, if the data symbols are not sorted according to their probability, the optimal Huffman tree does not satisfy the requirements for binary-search trees, i.e., "keys" are not properly sorted, and we cannot define a node with a simple comparison of the type "s < m?" This problem is solved by storing the paths from the root node to leaf nodes, i.e., the Huffman codewords.  \n Tree-based Update of Cumulative Distributions In this section, we show that we can use binary-search trees (Section 2.2.3) to efficiently combine computing the cumulative distribution, updating it, encoding and decoding, without having to use a binary arithmetic encoder. We present techniques similar to the methods proposed by Moffat  [31]  and Fenwick  [42] . We start with an example of how to compute the cumulative distribution vector C from the statistics C gathered while using the binary search trees. \n Example 13 ⊳ Let us consider the binary search tree shown in Figure  2 .3. Let us assume that we had been using Algorithms 18 and 19 to compute the number of symbols occurrences in the tree, C, and we want to compute the cumulative distribution C from C = [ 121 4 8 22  15 19 ] . From the tree structure we can find out that, except for the root node, the counter at each node has the number of occurrences of all symbols found following the left branch, i.  \n ⊲ The equations that we obtain from any decision tree are linearly independent, and thus it is always possible to compute the cumulative distribution C from the counters C. We show next how to use the tree structure to efficiently compute the components of C required for encoding symbol s, C(s) and C(s + 1). In order to compute C(s), when we move from the root of the tree, up to the leaf representing symbol s, we simply add the value of C(n) for each node n that does not have its condition satisfied (i.e., we move up its right-side branch). For example, to compute C(2) using the tree of Figure  2 .3, we start from node 3, and move left to node 1, right to node 2, and right to leaf 2. Since we move along the right branch of nodes 1 and 2, we conclude that C(2) = C(1) + C(2) = 12. The value of C(s + 1) can be computed together with C(s): we just have to add the running sum for C(s) and C(n) at the last left-side branch taken before reaching leaf s. For example, when computing C(2), the last left-side branch is taken at root node 3, and thus C(3) is equal to the running sum (zero) plus C(3) = 22. \n Algorithm 20 Procedure Interval Update (s, b, l, M, C) Algorithms 20 and 21 show how to combine all the techniques above to simultaneously compute and update the cumulative distribution, and use the computed values for encoding and decoding. They use a tree defined by bisection search, but it is easy to modify them to other tree structures. After Step 2 of Algorithm 20 we have E = C(s) and F = C(s + 1) computed and updated with a number of additions proportional to log 2 (M). \n Periodic Updates of the Cumulative Distribution We have seen in Section 2.2.2 that adaptive coding can increase the arithmetic coding computational complexity significantly, because of the effort to update the cumulative distributions. In Sections 2.2.3 and 2.2.4 we present tree-based updating techniques that reduce this complexity very significantly. However, adaptation can still be a substantial fraction of the overall coding computational complexity, and it happens that there is not much we can do if we insist on the assumption that the probability model has to be updated immediately after each encoded/decoded symbol, i.e., estimates are in the form given by (2.10), with a division by the factor C(M). However, with accurate and fast source modeling we can avoid having probabilities changing so quickly that we need to refresh estimates on a symbol-by-symbol basis. For example, an image encoder may use different probabilities depending on a classification of the part being encoded, which can be something like "constant," "smooth," "edge," "high-frequency pattern," etc. With these techniques, we can assume that the source state may change quickly, but the source properties (symbol probabilities) for each state change slowly.  Under these assumptions, and unless the number of data symbols is very large (thousands or millions  [51] ), a significantly more efficient form of adaptive arithmetic coding updates only the vector with symbol occurrence counters ( P) after each symbol, and updates the distribution estimate (c) periodically, or following some special events. For example, the Q-coder updates its probability estimation only during renormalization  [27] . For periodic updates, we define R as number of symbol coded between updates of c. Typically, the period R is a small multiple of M (e.g., R = 4M), but to improve the accuracy while minimizing the computations, we can start with frequent updates, and then decrease their frequency. One immediate consequence of this approach is that while coding we can use the simpler procedures of Section 2.1.1 and Appendix A, and can completely avoid the divisions in equations (2.10) to (2.12). Furthermore, if the period R is large enough, then it is feasible to do many complex tasks while updating c, in order to increase the encoding/decoding speed. For instance, in Section 2.3.2 we explain how to make decoding faster by sorting the symbols according to their probability, finding the optimal decoding sequence (Huffman tree), or computing the data for fast table look-up decoding. \n Complexity Analysis Large computational complexity had always been a barrier to the adoption of arithmetic coding. In fact, for many years after arithmetic coding was invented, it was considered little more than a mathematical curiosity because the additions made it slow, multiplications made it very expensive, and divisions made it impractical. In this section, we analyze the complexity of arithmetic coding and explain how the technological advances that gives us fast arithmetic operations change the complexity analysis. Many of the conclusions in this section are based on experimental tests designed for analyzing the arithmetic coding complexity, which are described in reference  [63] . Another earlier experimental evaluation of complexity is in reference  [43] . We have to observe that the relative computational complexity of different coding operations changed dramatically. Arithmetic operations today are much more efficient, and not only due the great increase in the CPU clock frequency. In the past, the ratio between clock cycles used by some simple operations (comparisons, bit shifts, table look-up) and arithmetic operations (specially division) was quite large. Today, this ratio is much smaller  [58, 59, 60, 61] , invalidating previous assumptions for complexity reduction. For instance, a set of comparisons, bit shifts, and table look-up now takes significantly more time than a multiplication. The speed of arithmetic coding needs to be measured by the rate of data (true information) bits coming out of the encoder, or into the decoder (information throughput). For example, binary coders can be very simple and fast, but their throughput is limited to a fraction of bits per CPU clock cycle. Arithmetic coders with larger alphabets, on the other hand, process information in a significantly more efficient manner, and can easily yield throughputs of many bits per CPU clock cycle  [63] . In this section we use the "big-O" notation of  [30] , where O(•) indicates asymptotic upper bounds on the computational complexity. The main factors that influence complexity are • Interval renormalization and compressed data input and output. • Symbol search. • Statistics estimation (adaptive models only). • Arithmetic operations. In the next sections we analyze each of these in some detail. However, we will not consider special hardware implementations (ASICs) for three reasons: (a) it is definitely outside the scope of this text; (b) our model also applies to some specialized hardware, like DSP chips; (c) many optimization techniques for general CPUs also apply to efficient hardware. \n Interval Renormalization and Compressed Data Input and Output In integer arithmetic implementations the interval renormalization can be calculated with only bit shifts. However, when D = 2, renormalization occurs quite frequently, consuming many clock cycles. Using larger output alphabets in the form D = 2 r reduces the frequency of the renormalization by a factor of r (see  Example 10) , and thus may reduce the number of clock cycles used for renormalization very significantly. Floating-point implementations may need multiplication during renormalization, but if D is large enough then these operations do not add much to the overall complexity. Data input and output, which occurs during renormalization, also has significant impact on the encoding and decoding speed. Since computers and communication systems work with groups of 8 bits (bytes), processing one bit at a time requires extra clock cycles to properly align the data. Thus, there is substantial speedup when renormalization produces bits that can be easily aligned in bytes (e.g., D = 2 4 , D = 2 8 , or D = 2 16 ). The case D = 256 has been shown to be particularly efficient  [52, 54] . \n Symbol Search The computational complexity of the arithmetic decoder may be many times larger than the encoder\'s because the decoder needs to find out in which subinterval the current code value is, i.e., solve ŝ (v) = { s : c(s)l ≤ v − b < c(s + 1)l} , (2.15) or ŝ(v) = s : c(s) ≤ v − b l < c(s + 1) . (2.16) Mathematically these are equivalent, but we use (2.16) to represent the case in which the division (v − b)/l is computed first (e.g., Algorithm 13). This is a line-search problem  [17, 30] , where we need to minimize both the number of points tested and the computations used for determining these points. The possible difference in complexity between the encoder and decoder grows with the number of data symbols, M. We can see from Algorithms 15 and 16 that when M = 2, the complexity is practically the same for the encoder and decoder. There are five main schemes for symbol search that are described next. (a) Sequential search on sorted symbols This search method, used in Algorithms 7 and 12, in the worst-case searches M − 1 intervals to find the decoded symbol. We can try to improve the average performance by sorting the symbols according to their probability. Assuming that the symbols are sorted with increasing probability, the average number of tests is  Ns (Ω) = M −1 m=0 p(m)(M − m) = M − M −1 m=0 m p(m) = M − s(Ω) ≤ M, ( 2 (M)⌋ ≤ Nb (Ω) ≤ ⌈log 2 (M)⌉ , (2.18) independently of the probabilities of the data symbols. Note that the encoder may also have to implement the bisection search when it is used with binary coding (Section 2.2.3), or when using trees for updating the probability estimates (Section 2.2.4). \n (c) Optimal tree-based search We show in Section 2.2.4 how the process of encoding and decoding data from a M-symbol alphabet can be decomposed in a set of binary decisions, and in Example 12 we show its similarities to prefix coding, and an optimal decision tree designed with the Huffman algorithm. The interval search process during decoding is also defined by binary decisions, and the optimal set of decisions is also computed with the Huffman algorithm  [2, 23] . Thus, we can conclude that the average number of tests in such scheme is bounded as the average number of bits used to code the source with Huffman coding, which is given by  [9]  max{1, H(Ω)} ≤ No (Ω) ≤ H(Ω) + 0.086 + max m=0,1,...,M −1 {p(m)}. (2.19) Implementing the optimal binary-search tree requires some extra storage, corresponding to the data normally used for representing a Huffman code, and it is necessary to reorder the symbols according to probability (as in the example of Figure  2 .4), or use a modified definition of the cumulative distribution. With adaptive coding we have the problem that the optimal tree is defined from the symbol probabilities, which are unknown when encoding and decoding start. This problem can be solved by periodically redesigning the tree, together with the distribution updates (Section 2.2.5). \n (d) Bisection search on sorted symbols We can combine the simplicity of bisection with a technique that takes into account the symbol probabilities. When the data symbols are sorted with increasing probability, we can look for the symbol m such that c(m) ≈ 0.5, and use it as the first symbol (instead of middle point) to divide the interval during bisection. If the distribution is nearly uniform, then this symbol should be near the middle and the performance is similar to standard bisection search. On the other hand, if the distribution is highly skewed, them m ≈ M − 1, meaning that the most probable symbols are tested first, reducing the average number of tests. We can extend this technique to find also find the symbols with c(m) near 0.25 and 0.75, and then 0.125, 0.375, 0.625, and 0.825, and so on. Figure  2 .5 shows an example of a cumulative distribution, and the process of finding the symbols for initializing the bisection search. The corresponding first levels of the binary-search tree are shown in Figure  2 .6. The decoder does not have to use all levels of that tree; it can use only the first level (and store one symbol), or only the first two levels (and store three symbols), or any desired number of levels. In Figure  2 .6, the complete binary-search tree is defined by applying standard bisection search following the tests shown in the figure  .  If the encoder uses up to V > 1 levels of the binary-search tree defined by the cumulative distribution, the average number of tests is near the optimal (2.19), and the worst case is not much larger than  (2.18) . \n (e) Bisection search starting from table look-up The interval search methods that use (2.16) require one division, but its advantage is that it is equivalent to rescaling the current interval to unit length before search. This means that from a quantized version of the fraction (v − b)/l, like  E(v, b, l) = K t (v − b) l , ( 2 \n Example 14 ⊳ Let us consider again the source of Example 3, with c = [ 0 0.2 0.7 0.9 1 ]. Table  2 .5 shows how each value of E(v, b, l) corresponds to an interval of possible values of (v − b)/l. By analyzing c and these intervals we can identify the range of possible decoded symbols, which correspond to ŝmin (E) and ŝmax (E). For example, if while decoding we have E(v, b, l) = 0, then we know that (v −b)/l < 0.125, and consequently ŝ(v) = 0. On the other hand, if E(v, b, l) = 5, then 0.625 ≤ (v − b)/l < 0.75, which means that we can only say that 1 ≤ ŝ(v) ≤ 2, and one more test is necessary for finishing decoding. ⊲ Note that, while the number of tests required by the original bisection search does not depend on the probabilities, when we use ŝmin (E) and ŝmax (E) to initialize the search the initial interval depends on the probabilities, and the most probable symbols are found with a smaller number of tests. For instance, we can see in Table  2 .5 that most of the table entries correspond to the most probable symbols. The main constraint for table look-up decoding is that it is practical only for static or periodically updated models (Section 2.2.5). The overhead of computing the table each time the model is updated can be small because table look-up is only for initializing the bisection search, and even small tables (e.g., 16 entries) can make it significantly more efficient. \n Cumulative Distribution Estimation Adaptive arithmetic coding requires an extra effort to update the cumulative distribution estimates after coding each symbol, or in a periodical manner. Section 2.2 covers all the important aspects of how adaptive encoders efficiently update the cumulative distributions c. We analyze two main strategies, depending on whether the cumulative distribution updating is done together or independently of the symbol search (Section 2.3.2). (a) Combined updating, coding and decoding In this case, the most efficient implementations use binary-search trees, and updating is applied to vector C, instead of C. Binary coding (Section 2.2.3) and tree-based updating (Section 2.2.4) have the same asymptotic complexity, depending on the tree being used. In addition, by comparing Algorithms 18 and 19 and Algorithms 20 and 21, we can see that the encoder and the decoder have very similar computational complexity. The worst-case effort is minimized when we use bisection search, resulting in a number of operations per coded symbol proportional to log 2 (M) (equation (2.18)). Huffman trees, that optimize the average performance, require an average number of operations proportional to No (Ω), defined in equation  (2.19) . \n (b) Independent updating When source Ω has low entropy, some symbols should occur much more frequently, and it may be efficient to update C directly, if the symbols are sorted by increasing probabilities. Implementations by Witten et al.  [25, 51]  use this strategy. However, when all symbols are equally probable, the effort for sorting and updating C is on average proportional to M/2, and in the worst-case proportional to M − 1. As explained in Section 2.2.5, with periodic updates of C we can recompute and sort symbols with reasonable worst-case complexity. We assume that the updating period is R coded symbols. One choice is keep the data symbols not sorted, and to update C using (2.10), which requires O(M) operations per update, and an average complexity of O(M/R) operations per coded symbol. Decoding can use bisection for symbol search. If we choose, for example, R = M then we have O(1) operations per update, which is quite reasonable. Sorting the symbols according to their probability during each update can be done with O(M log(M)/R) operations per symbol, in the worst-case  [30] . Since symbols are normally already sorted from previous updating passes, insertion sort  [30]  typically can be done with an average of O(M/R) operations. When the symbols are sorted, we can use the more efficient symbol search of Section 2.3.2(d). Choosing R = M results in O(log(M)) operations per symbol. With periodic updating we can both sort symbols, and find the optimal search strategy (Huffman tree), with a reasonable complexity of O(M log(M)/R) operations per symbol  [30] . However, even though the asymptotic complexity of finding the Huffman code is the same as simply sorting, it requires considerable more effort. Table  2 .6 presents a summary of the results above. Note that for the complexity analysis of optimal searches, we can use O(H(Ω) + 1), instead of the more complex and tighter bounds given by  (2.19) . The last columns in Table  2    2 .6: Computational complexity of symbol search and adaptation (cumulative distribution updating) for fixed and different adaptive arithmetic coding techniques. Typically, R = 4M minimizes the complexity without degrading performance. divisors for each coded symbol. Periodic updating, on the other hand, allows us to compute the inverse of divisor once, and use it for coding several (R) symbols. \n Arithmetic Operations Presently, additions and multiplications are not much slower than other operations, such as bit shifts and comparison. Divisions, on the other hand, have a much longer latency (number of clock cycles required to perform the operation), and cannot be streamlined with other operations (like special circuitry for multiply-add)  [58, 59, 60, 61] . First, let us consider the arithmetic required by fixed coders, and adaptive coders with periodic updating. The arithmetic coding recursion in the forms (1.9) and (1.40) require two multiplications and, respectively, one and three additions. Thus, except for processor with very slow multiplication, encoding requirements are quite reasonable. Decoding is more complex due to the effort to find the correct decoding interval. If we use (1.16) for interval search, we have one extra division, plus several comparisons (Section 2.3.2). The multiplication-only version  (1.38)  requires several multiplications and comparisons, but with reasonably efficient symbol search this is faster than  (1.16) , and eliminates the need to define division when multiplications are approximated. Adaptive coding can be considerably slower because of the divisions in (2.11) and (2.12). They can add up to two divisions per interval updating. In Algorithms 11, 12 and 13, we show how to avoid one of the divisions. Furthermore, updating the cumulative distribution may require a significant number of additions. Note that having only periodic updates of the cumulative distribution significantly reduces this adaptation overhead, because all these 2.4. Further Reading divisions and additions are not required for every coded symbol. For example, a single division, for the computation of 1/ C(M), plus a number of multiplications and additions proportional to M, may be needed per update. If the update occurs every M coded symbols, then the average number of divisions per coded symbol is proportional to 1/M, and the average number of multiplications and additions are constants. \n Further Reading We presented the coding method that is now commonly associated with the name arithmetic coding, but the reader should be aware that other types of arithmetic coding had been proposed  [6, 7] . Standard implementations of arithmetic coding had been defined in some international standards  [33, 36, 38, 62] . Several techniques for arithmetic coding complexity reduction not covered here are in the references  [11, 18, 23, 25, 27, 28, 34, 37, 42, 51, 52] . We did not mention the fact that errors in arithmetic coded streams commonly lead to catastrophic error propagation, and thus error-resilient arithmetic coding  [57]  is very important in some applications. Figure 1.1: System with typical processes for data compression. Arithmetic coding is normally the final stage, and the other stages can be modeled as a single data source Ω. \n ), m = 0, 1, . . . , M. (1.2) Note that c(0) ≡ 0, c(M) ≡ 1, and p(m) = c(m + 1) − c(m). (1.3) We use bold letters to represent the vectors with all p(m) and c(m) values, i.e., p = [ p(0) p(1) • • • p(M − 1) ], c = [ c(0) c(1) • • • c(M − 1) c(M) ]. \n Figure 1 . 2 : 12 Figure 1.2: Cumulative distribution of code values generated by different coding methods when applied to the source of Example 2. \n Figure 1 . 3 : 13 Figure 1.3: Dynamic system for updating arithmetic coding intervals. \n Φ 4 Figure 1 . 4 : 414 Figure 1.4: Graphical representation of the arithmetic coding process of Example 3: the interval Φ 0 = [0, 1) is divided in nested intervals according to the probability of the data symbols. The selected intervals, corresponding to data sequence S = {2, 1, 0, 0, 1, 3} are indicated by thicker lines. \n Example 6 ⊳ 6 ( 25 = 6625 Figure 1.6 shows rescaling applied to Example 3. It is very similar to Figure 1.4, but instead of having just an enlarged view of small intervals, in Figure 1.6 the intervals also change. The rescaling parameters δ 1 = 0.74 and γ 1 = 10 are used after coding two symbols, and δ 2 = 0 and γ 2 = 25 after coding two more symbols. The final interval is Φ6 (S) = | 0.65, 0.05 , that corresponds to Φ | 0.7426, 0.0002 , and which is exactly the interval obtained in Example 3. ⊲ \n Figure 1 . 6 : 16 Figure 1.6: Graphical representation of the arithmetic coding process of Example 3 (Figure 1.4) using numerical rescaling. Note that the code value changes each time the intervals are rescaled. \n Figure 1 . 1 Figure 1.7 shows how an interval is subdivided when using inexact multiplications. In the figure we show that there can be a substantial difference between, say, b k + c(1) • l k and b k + [[c(1) • l k ]], but this difference does not lead to decoding errors if the decoder uses the same approximation.Decoding errors occur when condition (1.19) is not satisfied. Below we show the constraints that must be satisfied by approximations, and analyze the three main causes of coding error to be avoided. \n 1aaa . . . aa 000000 . . . 2 b k = 0.aaaa . . . aa settled 0111 . . . 11 outstanding aaaa . . . aa (B=2 P b) active 000000 . . . 2 trailing zeros (2.3) \n b k+n < b k + l k ≤ 0.aaaa . . . aa settled \n Figure 2 . 1 : 21 Figure 2.1: Carry propagation process. Bold letters indicate the active bits, outstanding bits are underlined, and leftmost bits are settled. \n Example 8 ⊳ 5 ⋆ 85 We applied Algorithms 1 to 8 to the source and data sequence of Example 3, and the results are shown in Table 2.1. The first column shows what event caused the change in the interval. If a new symbol σ is coded, then we show it as s = σ, and show Algorithm 8 Procedure Decoder Renormalization (v, b, l, t, d) 1. while l ≤ 0.5 do ⋆ Renormalization loop 1.1. if b ≥ 0.Remove most significant bit then set { b ← 2(b − 0.5); ⋆ shift and scale interval base v ← 2(v − 0.5); } ⋆ shift and scale code value else set { b ← 2b; \n Table 2 . 1 : 21 Results of arithmetic encoding and decoding, with renormalization, applied to source and data sequence of Example 3. Final code value is v = 0.1011111000100 2 = 0.74267578125. \n Figure 2 . 2 : 22 Figure 2.2: Configuration for using a standard (binary output) arithmetic encoder to replace an encoder with D-symbol (ternary, decimal, etc.) output. \n Figure 2 . 2 \n Figure 2 . 3 : 23 Figure 2.3:Example of a binary search tree with the sequential decisions required for coding data from a 6-symbol alphabet using binary encoders. Leaf nodes represent data symbols, and the numbers above them represent their number of occurrences, P (s). The binary information indicated by each question mark is coded with the probability estimate shown in parenthesis. \n Figure 2 . 4 : 24 Figure 2.4: Another example of a binary-search tree with for coding data from a 6-symbol source. The number of symbol occurrences is the same as shown in Figure 2.3. This tree has been designed to minimize the average number of binary symbols coded. \n C ) = P (0) + P (1) + P (2) = 22 C(4) = P (3) = 15 C(5) = P (4) = 19 where P (s) is the number of time symbol s has occurred. From these equations we can compute the vector with cumulative distributions as = P (0) + P (1) + P (2) + P (3) + P (4) = C(3) + C(4) + C(5) = 56 C(6) = P (0) + P (1) + P (2) + P (3) + P (4) + P (5) = C(0) = 121 We can do the same with the tree of Figure 2.4, and find different sets of equations. In this case the counters are C(0) ≡ P (0) + P (1) + P (2) + P (3) + P (4) + P (5) = 121 C(1) = P (0) = 4 C(2) = P (0) + P (1) = 12 C(3) = P (0) + P (1) + P (2= P (0) + P (1) + P (2) + P (3) = C(3) + C(4) = 37 C(5) = P (0) + P (1) + P (2) + P (3) + P (4) = C(5) = 56 C(6) = P (0) + P (1) + P (2) + P (3) + P (4) + P (5) = C(0) = 121 \n Algorithm 21 Function 21 Interval Selection (v, b, l, M, C) \n Table 1 . 1 0.7 0.2 0.74267578125 2 2 1 0.74 0.1 0.21337890625 1 3 0 0.74 0.02 0.0267578125 0 4 0 0.74 0.004 0.1337890625 0 5 1 0.7408 0.002 0.6689453125 1 6 3 0.7426 0.0002 0.937890625 3 7 - - - 0.37890625 1 8 - - - 0.3578125 1 2: Arithmetic encoding and decoding results for Examples 3 and 4. The last two rows show what happens when decoding continues past the last symbol. \n .2 to show that the decoding process can continue normally after the last symbol is encoded. Below we explain what happens. ⊲ 1.5. Optimality of Arithmetic Coding \n bit counter 2. for k = 1 to N do ⋆ Encode N data symbols 2.1. Interval Update (s k , b, l, M, c); ⋆ Update interval according to symbol 2.2. if b ≥ 1 then ⋆ Check for carry 2.2.1. set { b ← b − 1; ⋆ Shift interval base Propagate Carry (t, d); } ⋆ Propagate carry on buffer 2.3. if l ≤ 0.5 then ⋆ If interval is small enough 2.3.1. Encoder Renormalization (b, l, t, d); ⋆ then renormalize interval 3. Code Value Selection (b, t, d); ⋆ Choose final code value 4. return t. ⋆ Return number of code bits • \n Procedure Arithmetic Decoder (N, M, c, d, Ŝ) 1. set n ← t; ⋆ Initialize pointer to last outstanding bit 2. while d(n) = 1 do ⋆ While carry propagation 2.1. set { d(n) ← 0; ⋆ complement outstanding 1-bit and n ← n − 1; } ⋆ move to previous bit 3. set d(n) ← 1; ⋆ Complement outstanding 0-bit 4. return. • Algorithm 4 Procedure Encoder Renormalization (b, l, t, d) 1. while l ≤ 0.5 do ⋆ Renormalization loop 1.1. set { t ← t + 1; ⋆ Increment bit counter and l ← 2l; } ⋆ scale interval length 1.2. if b ≥ 0.5 ⋆ Test most significant bit of interval base then set { d(t) ← 1; ⋆ Output bit 1 b ← 2(b − 0.5); } ⋆ shift and scale interval base else set { d(t) ← 0; ⋆ Output bit 0 b ← 2b; } ⋆ scale interval base 2. return. • Algorithm 5 Procedure Code Value Selection (b, t, d) 1. set t ← t + 1; ⋆ Increment bit counter 2. if b ≤ 0.5 ⋆ Eenormalized code value selection then set d(t) ← 1; ⋆ Choose v = 0.5: output bit 1 else set { d(t) ← 0; ⋆ Choose v = 1.0: output bit 0 and Propagate Carry (t − 1, d); } ⋆ propagate carry 3. return. • Algorithm 6 \n Table 2 . 2 1−P . Input Encoder Binary Decimal 8-bit s k Data Representation Representation Registers - b o 0.00000000 2 0 l 0 1.00000000 2 1 l 0 • c(2) 0.10110011 2 0.69921875 2 b 1 0.1011001100 2 0.69921875 l 1 0.0011001100 2 0.19921875 l 1 • c(1) 0.0000101000 2 0.0390625 1 b 2 0.10111101000 2 0.73828125 l 2 0.00011001100 2 0.099609375 l 2 • c(0) 0.00000000000 2 0 0 b 3 0.1011110100000 2 0.73828125 l 3 0.0000010100000 2 0.01953125 l 3 • c(0) 0.0000000000000 2 0 0 b 4 0.1011110100000000 2 0.73828125 l 4 0.0000000011111000 2 0.0037841796875 l 4 • c(1) 0.0000000000110001 2 0.0007476806640625 1 b 5 0.10111101001100010 2 0.7390289306640625 l 5 0.00000000011111000 2 0.00189208984375 l 5 • c(3) 0.00000000011011110 2 0.0016937255859375 3 b 6 0.10111101101000000000 2 0.74072265625 l 6 0.00000000000011001000 2 0.00019073486328125 - v 0.1011110110101 2 0.7408447265625 - 2: Results of arithmetic encoding and decoding for Example 3 using 8-bit precision for the arithmetic operations. \n Table 2 . 2 Data source Arithmetic encoder Arithmetic decoder Decoded data Arithmetic decoder Arithmetic encoder 3: Results of arithmetic encoding and decoding for Example 3, with renormalization and a 16-symbol (hexadecimal) output alphabet. Final code value is v = 0.BE20 16 = 0.1011 1110 0010 0000 2 = 0.74267578125. \n Table 2 . 4 : 24 Number binary symbols coded using trees created from different types of binary searches, applied to data source of Example 10. The trees corresponding to bisection and optimal searches are shown in Figures 2.3 and 2.4, respectively. Data Probability Number of binary symbols coded Symbol estimate Sequential search Bisection search Optimal search s p(s) N(s) p(s)N(s) N(s) p(s)N(s) N(s) p(s)N(s) 0 0.033 6 0.198 2 0.066 4 0.132 1 0.066 5 0.331 3 0.198 4 0.264 2 0.083 4 0.331 3 0.248 3 0.248 3 0.124 3 0.372 2 0.248 3 0.372 4 0.157 2 0.314 3 0.471 3 0.471 5 0.537 1 0.537 3 1.612 1 0.537 Sum 1.000 21 2.083 16 2.843 18 2.025 \n Table 2 . 5 : 25 .20) Figure 2.5: Technique to find quasi-optimal points to initialize bisection search on sorted symbols.Figure 2.6: Decision process corresponding to the quasi-optimal binary-search initialization of Figure 2.5. Values of (v − b)/l quantized to 8 integer values to allow fast decoding using table look-up. The minimum and maximum possible values of the decoded symbol (table entries) were computed using c = [ 0 0.2 0.7 0.9 1 ].we can know better initial values for the bisection search, which can be stored in tables with K t elements, enabling significantly faster decoding. Cumulative distribution c(s) 1.000 0.875 0.750 0.625 0.500 0.375 0.250 0.125 0.000 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 s 6 ✖✕ ✗✔ s < 6? Y N ❅ ❅ ❅ ■ ✒ ✖✕ 9 ✗✔ s < 9? Y N ❅ ❅ ❅ ■ ✒ ✖✕ 11 ✗✔ s < 11? Y N ❅ ❅ ❅ ■ ✒ s < 13? ❅ Y ❅ ❅ ■ ✗✔ Y ❅ ❅ ❅ ■ 14 N ✒ ✖✕ s < 14? ✗✔ ✖✕ 15 ✗✔ 15 N ✒ ✖✕ s < 15? 13 ✖✕ ✗✔ Y N ❍ ❍ ❍ ❍ ❍ ❍ ❍ ( ✟ ✟ ✟ ✟ ✟ ✟ ✟ ✯ Note that ŝmax (E) <= ŝmin (E + 1), so one table is enough for correct decoding. \n Table . 6  indicates the need to use divisions in the form (2.10). Changing the cumulative distribution every symbol requires using different Searching & updating Symbol Decoder Distribution Divisions methods encoding interval search updating Fixed code O(1) O(H(Ω) + 1) NA NA optimal search Sequential search O(1) O(M) O(M) O(1) on sorted symbols Combined updating and O(log(M)) O(1) coding, bisection tree Combined updating and O(H(Ω) + 1) O(1) coding, optimal tree Periodic update, O(1) O(log(M)) O(M/R) O(1/R) bisection search Periodic update, O(1) O(H(Ω) + 1) O(M log(M)/R) O(1/R) optimal search \n\t\t\t Published as a chapter in Lossless Compression Handbook, ed. Khalid Sayood, ©Academic Press \n\t\t\t . Arithmetic Coding Principles \n\t\t\t . The symbols in the data sequence are not independent (even if uncorrelated) [22] . \n\t\t\t . We can only estimate the probability values, the statistical dependence between symbols, and how they change in time.However, in the next sections we show that the generalization of arithmetic coding to time-varying sources is straightforward, and we explain how to address all these practical issues. \n\t\t\t .4. Arithmetic Coding \n\t\t\t .6. Arithmetic Coding Properties \n\t\t\t .1. Coding with Fixed-Precision Arithmetic \n\t\t\t . Arithmetic Coding Implementation \n\t\t\t .2. Adaptive Coding \n\t\t\t .3. Complexity Analysis', acknowledgement=None, annex='Algorithm 11 Procedure Interval Update (s, b, l, M, C) • Only a few changes to the algorithms of Section 2.1.1 are sufficient to include the ability to dynamically update the cumulative distribution. First, we may use (2.10) to change all multiplications by c(s), as follows. Similarly, the changes to the integer arithmetic version in Appendix A may be in the form. However, since divisions are much more expensive than additions and multiplications, it is better to compute γ = l/ C(M) once, and implement interval updating as shown in Algorithm 11  [50, 51] . The corresponding changes to Algorithm 7 is shown in Algorithm 12. In Algorithms 7 and 12 we may need to compute several multiplications, under the assumption they are faster than a single division (see Algorithm 21 for a more efficient search method). If this is not true, Algorithm 13 shows how to replace those multiplications by one extra division. \n Appendix A Integer Arithmetic Implementation The following algorithms show the required adaptations in the algorithms in Section 2.1.1 for use with integer arithmetic, and with a D-symbol output alphabet. Typically D is small power of two, like 2, 4, 16, or 256. As explained in Section 2.1.2, we assume that all numbers are represented as integers, but here we define B = D P b, L = D P l, and C(s) = D P c(s). In addition, we assume multiplications computed with 2P digits and results truncated to P digits. Renormalization (2.6) sets L > D P −1 , and thus the minimum probability allowed is defined by i.e., p(s) ≥ D 1−P . Algorithm 22, for integer arithmetic, is almost identical to Algorithm 1, except for the initialization of L, and the decision for renormalization. The arithmetic operations that update the interval base may overflow the integer registers. To make clear that this is acceptable, we define the results modulo D P , as in Algorithm 23. The results of the multiplications are multiplied by D −P and truncated, meaning that the least significant bits are discarded. Overflow is here detected by a reduction in the base value. For that reason, we implement carry propagation together with the interval update in  Algorithm 23.  Note that in the renormalization of Algorithm 24, we assume that D is a power of two, and all multiplications and divisions are actually implemented with bit shifts. The carry propagation with a D-symbol output, shown in Algorithm 25, is very similar to Algorithm 3. In Algorithm 5 the code value selection is made to minimize the number of bits, assuming that the decoder pads the buffer d with sufficient zeros. This inconvenience can be avoided by simply adding a proper extra symbol at the end of the compressed data. Algorithm 26 shows the required modifications. It shifts the interval base by a small amount, and resets the interval length, so that when procedure Encoder Renormalization is called, it adds the proper two last output symbols to buffer d. This way, correct decoding does not dependent on the value of the symbols that are read by the decoder (depending on register size P ) past the last compressed data symbols. With integer arithmetic the decoder does not have to simultaneously update the interval base and code value (see  Algorithm 8) . Since decoding is always based on the difference v −b, we define V = D P (v − b) and use only V and L while decoding. The only other required change is that we must subtract from V the numbers that would have been added to B. \n Algorithm 22 Function Arithmetic Encoder (N, S, M, C, d)  In Algorithm 7 we used sequential search for interval selection during decoding, which in the worst case requires testing M − 1 intervals. In Algorithm 28 we use bisection  [17, 19, 39]  for solving  (1.16) , which requires testing at most ⌈log 2 (M)⌉ intervals (see Sections 2.2.3 and 2.3.2). Finally, the decoder renormalization is shown in Algorithm 29.')
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Matteo Castiglioni', given_name='Matteo', middle_name=None, surname='Castiglioni', email=None, orcid=None, affiliation=GrobidAffiliation(institution='Politecnico di Milano', department=None, laboratory=None, address=None)), GrobidAuthor(full_name='Andrea Celli', given_name='Andrea', middle_name=None, surname='Celli', email='drea.celli2@unibocconi.it>.', orcid=None, affiliation=GrobidAffiliation(institution='Bocconi Univer-sity', department=None, laboratory=None, address=None)), GrobidAuthor(full_name='Christian Kroer', given_name='Christian', middle_name=None, surname='Kroer', email=None, orcid=None, affiliation=GrobidAffiliation(institution='Columbia University', department=None, laboratory=None, address=None))], index=None, id=None, unstructured=None, date='2023-02-02', title='Online Bidding in Repeated Non-Truthful Auctions under Budget and ROI Constraints', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2302.01203v1[cs.GT]', pii=None, ark=None, istex_id=None, url=None), pdf_md5='BA546C823A0C704A373C5E5DBD5A7E21', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='A Agarwal', given_name='A', middle_name=None, surname='Agarwal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Hsu', given_name='D', middle_name=None, surname='Hsu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Kale', given_name='S', middle_name=None, surname='Kale', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Langford', given_name='J', middle_name=None, surname='Langford', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Li', given_name='L', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Schapire', given_name='R', middle_name=None, surname='Schapire', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date='2014', title='Taming the monster: A fast and simple algorithm for contextual bandits', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1638-1646', first_page='1638', last_page='1646', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Agarwal', given_name='D', middle_name=None, surname='Agarwal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Ghosh', given_name='S', middle_name=None, surname='Ghosh', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Wei', given_name='K', middle_name=None, surname='Wei', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S You', given_name='S', middle_name=None, surname='You', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='2014', title='Budget pacing for targeted online advertisements at linkedin', book_title='Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1613-1619', first_page='1613', last_page='1619', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Agrawal', given_name='S', middle_name=None, surname='Agrawal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N R Devanur', given_name='N', middle_name='R', surname='Devanur', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date='2019', title='Bandits with global convex constraints and objective', book_title=None, series_title=None, editors=None, journal='Operations Research', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='67', issue='5', pages='1486-1502', first_page='1486', last_page='1502', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Agrawal', given_name='S', middle_name=None, surname='Agrawal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N R Devanur', given_name='N', middle_name='R', surname='Devanur', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Li', given_name='L', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None)], index=3, id='b3', unstructured=None, date='2016', title='An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives', book_title='29th Annual Conference on Learning Theory (COLT)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Ai', given_name='R', middle_name=None, surname='Ai', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Wang', given_name='C', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Li', given_name='C', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Zhang', given_name='J', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Huang', given_name='W', middle_name=None, surname='Huang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Deng', given_name='X', middle_name=None, surname='Deng', email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='2022', title='No-regret learning in repeated first-price auctions with budget constraints', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2205.14572', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Akbarpour', given_name='M', middle_name=None, surname='Akbarpour', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Li', given_name='S', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date='2018', title='Credible mechanisms', book_title=None, series_title=None, editors=None, journal='EC', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='371', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Auerbach', given_name='J', middle_name=None, surname='Auerbach', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Galenson', given_name='J', middle_name=None, surname='Galenson', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Sundararajan', given_name='M', middle_name=None, surname='Sundararajan', email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date='2008', title='An empirical analysis of return on investment maximization in sponsored search auctions', book_title='Proceedings of the 2nd International Workshop on Data Mining and Audience Intelligence for Advertising', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1-9', first_page='1', last_page='9', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Babaioff', given_name='M', middle_name=None, surname='Babaioff', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Cole', given_name='R', middle_name=None, surname='Cole', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Hartline', given_name='J', middle_name=None, surname='Hartline', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Immorlica', given_name='N', middle_name=None, surname='Immorlica', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Lucier', given_name='B', middle_name=None, surname='Lucier', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date='2021', title='Non-quasi-linear agents in quasi-linear mechanisms', book_title='12th Innovations in Theoretical Computer Science Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Badanidiyuru', given_name='A', middle_name=None, surname='Badanidiyuru', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Langford', given_name='J', middle_name=None, surname='Langford', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Slivkins', given_name='A', middle_name=None, surname='Slivkins', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date='2014', title='Resourceful contextual bandits', book_title='Conference on Learning Theory', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1109-1134', first_page='1109', last_page='1134', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Badanidiyuru', given_name='A', middle_name=None, surname='Badanidiyuru', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Kleinberg', given_name='R', middle_name=None, surname='Kleinberg', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Slivkins', given_name='A', middle_name=None, surname='Slivkins', email=None, orcid=None, affiliation=None)], index=9, id='b9', unstructured=None, date='2018', title='Bandits with knapsacks', book_title=None, series_title=None, editors=None, journal='J. ACM', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='65', issue='3', pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Balseiro', given_name='S', middle_name=None, surname='Balseiro', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Lu', given_name='H', middle_name=None, surname='Lu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Mirrokni', given_name='V', middle_name=None, surname='Mirrokni', email=None, orcid=None, affiliation=None)], index=10, id='b10', unstructured=None, date='2020', title='Dual mirror descent for online allocation problems', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='613-628', first_page='613', last_page='628', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Balseiro', given_name='S', middle_name=None, surname='Balseiro', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Kim', given_name='A', middle_name=None, surname='Kim', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Mahdian', given_name='M', middle_name=None, surname='Mahdian', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Mirrokni', given_name='V', middle_name=None, surname='Mirrokni', email=None, orcid=None, affiliation=None)], index=11, id='b11', unstructured=None, date='2021', title='Budget-management strategies in repeated auctions', book_title=None, series_title=None, editors=None, journal='Operations Research', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S R Balseiro', given_name='S', middle_name='R', surname='Balseiro', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Gur', given_name='Y', middle_name=None, surname='Gur', email=None, orcid=None, affiliation=None)], index=12, id='b12', unstructured=None, date='2019', title='Learning in repeated auctions with budgets: Regret minimization and equilibrium', book_title=None, series_title=None, editors=None, journal='Management Science', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='65', issue='9', pages='3952-3968', first_page='3952', last_page='3968', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S R Balseiro', given_name='S', middle_name='R', surname='Balseiro', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Deng', given_name='Y', middle_name=None, surname='Deng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Mao', given_name='J', middle_name=None, surname='Mao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V S Mirrokni', given_name='V', middle_name='S', surname='Mirrokni', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Zuo', given_name='S', middle_name=None, surname='Zuo', email=None, orcid=None, affiliation=None)], index=13, id='b13', unstructured=None, date='2021', title='The landscape of auto-bidding auctions: Value versus utility maximization', book_title='Proceedings of the 22nd ACM Conference on Economics and Computation', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='132-133', first_page='132', last_page='133', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S R Balseiro', given_name='S', middle_name='R', surname='Balseiro', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Lu', given_name='H', middle_name=None, surname='Lu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Mirrokni', given_name='V', middle_name=None, surname='Mirrokni', email=None, orcid=None, affiliation=None)], index=14, id='b14', unstructured=None, date='2022', title='The best of many worlds: Dual mirror descent for online allocation problems', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Operations Research', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Bigler', given_name='J', middle_name=None, surname='Bigler', email=None, orcid=None, affiliation=None)], index=15, id='b15', unstructured=None, date='2019', title='Rolling out first price auctions to google ad manager partners', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='https://tinyurl.com/mvpfc97n'), GrobidBiblio(authors=[GrobidAuthor(full_name='C Borgs', given_name='C', middle_name=None, surname='Borgs', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Chayes', given_name='J', middle_name=None, surname='Chayes', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Immorlica', given_name='N', middle_name=None, surname='Immorlica', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Jain', given_name='K', middle_name=None, surname='Jain', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Etesami', given_name='O', middle_name=None, surname='Etesami', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Mahdian', given_name='M', middle_name=None, surname='Mahdian', email=None, orcid=None, affiliation=None)], index=16, id='b16', unstructured=None, date='2007', title='Dynamics of bid optimization in online advertisement auctions', book_title='Proceedings of the 16th international conference on World Wide Web', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='531-540', first_page='531', last_page='540', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Castiglioni', given_name='M', middle_name=None, surname='Castiglioni', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Celli', given_name='A', middle_name=None, surname='Celli', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Kroer', given_name='C', middle_name=None, surname='Kroer', email=None, orcid=None, affiliation=None)], index=17, id='b17', unstructured=None, date='2022', title='Online learning with knapsacks: the best of both worlds', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='ICML', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2767-2783', first_page='2767', last_page='2783', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Castiglioni', given_name='M', middle_name=None, surname='Castiglioni', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Celli', given_name='A', middle_name=None, surname='Celli', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Marchesi', given_name='A', middle_name=None, surname='Marchesi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Romano', given_name='G', middle_name=None, surname='Romano', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Gatti', given_name='N', middle_name=None, surname='Gatti', email=None, orcid=None, affiliation=None)], index=18, id='b18', unstructured=None, date=None, title='A unifying framework for online optimization with long-term constraints', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2022', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2209.07454', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Cesa-Bianchi', given_name='N', middle_name=None, surname='Cesa-Bianchi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Lugosi', given_name='G', middle_name=None, surname='Lugosi', email=None, orcid=None, affiliation=None)], index=19, id='b19', unstructured=None, date='2006', title='Prediction, learning, and games', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Cambridge university press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Cesa-Bianchi', given_name='N', middle_name=None, surname='Cesa-Bianchi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Gaillard', given_name='P', middle_name=None, surname='Gaillard', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Lugosi', given_name='G', middle_name=None, surname='Lugosi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Stoltz', given_name='G', middle_name=None, surname='Stoltz', email=None, orcid=None, affiliation=None)], index=20, id='b20', unstructured=None, date='2012', title='Mirror descent meets fixed share (and feels no regret)', book_title="Proceedings of the 25th International Conference on Neural Information Processing Systems -Volume 1, NIPS'12", series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='980-988', first_page='980', last_page='988', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Chen', given_name='T', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G B Giannakis', given_name='G', middle_name='B', surname='Giannakis', email=None, orcid=None, affiliation=None)], index=21, id='b21', unstructured=None, date='2018', title='Bandit convex optimization for scalable and dynamic iot management', book_title=None, series_title=None, editors=None, journal='IEEE Internet of Things Journal', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='6', issue='1', pages='1276-1286', first_page='1276', last_page='1286', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Chen', given_name='T', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Q Ling', given_name='Q', middle_name=None, surname='Ling', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G B Giannakis', given_name='G', middle_name='B', surname='Giannakis', email=None, orcid=None, affiliation=None)], index=22, id='b22', unstructured=None, date='2017', title='An online convex optimization approach to proactive network resource allocation', book_title=None, series_title=None, editors=None, journal='IEEE Transactions on Signal Processing', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='65', issue='24', pages='6350-6364', first_page='6350', last_page='6364', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V Conitzer', given_name='V', middle_name=None, surname='Conitzer', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Kroer', given_name='C', middle_name=None, surname='Kroer', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Sodomka', given_name='E', middle_name=None, surname='Sodomka', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N E Stier-Moses', given_name='N', middle_name='E', surname='Stier-Moses', email=None, orcid=None, affiliation=None)], index=23, id='b23', unstructured=None, date='2021', title='Multiplicative pacing equilibria in auction markets', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Operations Research', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Despotakis', given_name='S', middle_name=None, surname='Despotakis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Ravi', given_name='R', middle_name=None, surname='Ravi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Sayedi', given_name='A', middle_name=None, surname='Sayedi', email=None, orcid=None, affiliation=None)], index=24, id='b24', unstructured=None, date='2021', title='First-price auctions in online display advertising', book_title=None, series_title=None, editors=None, journal='Journal of Marketing Research', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='58', issue='5', pages='888-907', first_page='888', last_page='907', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Dudik', given_name='M', middle_name=None, surname='Dudik', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Hsu', given_name='D', middle_name=None, surname='Hsu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Kale', given_name='S', middle_name=None, surname='Kale', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Karampatziakis', given_name='N', middle_name=None, surname='Karampatziakis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Langford', given_name='J', middle_name=None, surname='Langford', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Reyzin', given_name='L', middle_name=None, surname='Reyzin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Zhang', given_name='T', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None)], index=25, id='b25', unstructured=None, date='2011', title='Efficient optimal learning for contextual bandits', book_title='Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='169-178', first_page='169', last_page='178', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='B Edelman', given_name='B', middle_name=None, surname='Edelman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Ostrovsky', given_name='M', middle_name=None, surname='Ostrovsky', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Schwarz', given_name='M', middle_name=None, surname='Schwarz', email=None, orcid=None, affiliation=None)], index=26, id='b26', unstructured=None, date='2007', title='Internet advertising and the generalized second-price auction: Selling billions of dollars worth of keywords', book_title=None, series_title=None, editors=None, journal='AM ECON REV', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='97', issue='1', pages='242-259', first_page='242', last_page='259', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Z Feng', given_name='Z', middle_name=None, surname='Feng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Padmanabhan', given_name='S', middle_name=None, surname='Padmanabhan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Wang', given_name='D', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None)], index=27, id='b27', unstructured=None, date='2022', title='Online bidding algorithms for return-on-spend constrained advertisers', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2208.13713', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Golrezaei', given_name='N', middle_name=None, surname='Golrezaei', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Jaillet', given_name='P', middle_name=None, surname='Jaillet', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J C N Liang', given_name='J', middle_name='C N', surname='Liang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Mirrokni', given_name='V', middle_name=None, surname='Mirrokni', email=None, orcid=None, affiliation=None)], index=28, id='b28', unstructured=None, date=None, title='Bidding and pricing in budget and roi constrained markets', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2021', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2107.07725', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Golrezaei', given_name='N', middle_name=None, surname='Golrezaei', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Lobel', given_name='I', middle_name=None, surname='Lobel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Paes Leme', given_name='R', middle_name=None, surname='Paes Leme', email=None, orcid=None, affiliation=None)], index=29, id='b29', unstructured=None, date='2021', title='Auction design for roi-constrained buyers', book_title='Proceedings of the Web Conference 2021', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='3941-3952', first_page='3941', last_page='3952', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A György', given_name='A', middle_name=None, surname='György', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Kocsis', given_name='L', middle_name=None, surname='Kocsis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Szabó', given_name='I', middle_name=None, surname='Szabó', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Szepesvári', given_name='C', middle_name=None, surname='Szepesvári', email=None, orcid=None, affiliation=None)], index=30, id='b30', unstructured=None, date='2007', title='Continuous time associative bandit problems', book_title='20th Intl. Joint Conf. on Artificial Intelligence (IJCAI)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='830-835', first_page='830', last_page='835', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='E Hazan', given_name='E', middle_name=None, surname='Hazan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Seshadhri', given_name='C', middle_name=None, surname='Seshadhri', email=None, orcid=None, affiliation=None)], index=31, id='b31', unstructured=None, date='2007', title='Adaptive algorithms for online decision problems', book_title='Electronic colloquium on computational complexity (ECCC)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='14', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='E Hazan', given_name='E', middle_name=None, surname='Hazan', email=None, orcid=None, affiliation=None)], index=32, id='b32', unstructured=None, date='2016', title='Introduction to online convex optimization', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Now Publishers, Inc', institution=None, issn=None, eissn=None, volume='2', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Herbster', given_name='M', middle_name=None, surname='Herbster', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M K Warmuth', given_name='M', middle_name='K', surname='Warmuth', email=None, orcid=None, affiliation=None)], index=33, id='b33', unstructured=None, date='1998', title='Tracking the best expert', book_title=None, series_title=None, editors=None, journal='Machine learning', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='32', issue='2', pages='151-178', first_page='151', last_page='178', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Immorlica', given_name='N', middle_name=None, surname='Immorlica', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Sankararaman', given_name='K', middle_name=None, surname='Sankararaman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Schapire', given_name='R', middle_name=None, surname='Schapire', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Slivkins', given_name='A', middle_name=None, surname='Slivkins', email=None, orcid=None, affiliation=None)], index=34, id='b34', unstructured=None, date='2022-11', title='Adversarial bandits with knapsacks', book_title=None, series_title=None, editors=None, journal='J. ACM', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='69', issue='6', pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Jenatton', given_name='R', middle_name=None, surname='Jenatton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Huang', given_name='J', middle_name=None, surname='Huang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Archambeau', given_name='C', middle_name=None, surname='Archambeau', email=None, orcid=None, affiliation=None)], index=35, id='b35', unstructured=None, date='2016', title='Adaptive algorithms for online convex optimization with long-term constraints', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='402-411', first_page='402', last_page='411', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Kesselheim', given_name='T', middle_name=None, surname='Kesselheim', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Singla', given_name='S', middle_name=None, surname='Singla', email=None, orcid=None, affiliation=None)], index=36, id='b36', unstructured=None, date='2020', title='Online learning with vector costs and bandits with knapsacks', book_title='Conference on Learning Theory', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2286-2305', first_page='2286', last_page='2305', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Liakopoulos', given_name='N', middle_name=None, surname='Liakopoulos', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Destounis', given_name='A', middle_name=None, surname='Destounis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Paschos', given_name='G', middle_name=None, surname='Paschos', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Spyropoulos', given_name='T', middle_name=None, surname='Spyropoulos', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Mertikopoulos', given_name='P', middle_name=None, surname='Mertikopoulos', email=None, orcid=None, affiliation=None)], index=37, id='b37', unstructured=None, date='2019', title='Cautious regret minimization: Online optimization with long-term budget constraints', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='3944-3952', first_page='3944', last_page='3952', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Luo', given_name='H', middle_name=None, surname='Luo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C.-Y Wei', given_name='C.-Y', middle_name=None, surname='Wei', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Agarwal', given_name='A', middle_name=None, surname='Agarwal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Langford', given_name='J', middle_name=None, surname='Langford', email=None, orcid=None, affiliation=None)], index=38, id='b38', unstructured=None, date='2018', title='Efficient contextual bandits in non-stationary worlds', book_title='Conference On Learning Theory', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1739-1776', first_page='1739', last_page='1776', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Mahdavi', given_name='M', middle_name=None, surname='Mahdavi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Jin', given_name='R', middle_name=None, surname='Jin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Yang', given_name='T', middle_name=None, surname='Yang', email=None, orcid=None, affiliation=None)], index=39, id='b39', unstructured=None, date='2012', title='Trading regret for efficiency: online convex optimization with long term constraints', book_title=None, series_title=None, editors=None, journal='The Journal of Machine Learning Research', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='13', issue='1', pages='2503-2528', first_page='2503', last_page='2528', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Mahdavi', given_name='M', middle_name=None, surname='Mahdavi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Yang', given_name='T', middle_name=None, surname='Yang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Jin', given_name='Jin', middle_name=None, surname=None, email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R', given_name='R', middle_name=None, surname=None, email=None, orcid=None, affiliation=None)], index=40, id='b40', unstructured=None, date='2013', title='Stochastic convex optimization with multiple objectives', book_title='Advances in Neural Information Processing Systems (NIPS)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1115-1123', first_page='1115', last_page='1123', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Mannor', given_name='S', middle_name=None, surname='Mannor', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J N Tsitsiklis', given_name='J', middle_name='N', surname='Tsitsiklis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Y Yu', given_name='J', middle_name='Y', surname='Yu', email=None, orcid=None, affiliation=None)], index=41, id='b41', unstructured=None, date='2009', title='Online learning with sample path constraints', book_title=None, series_title=None, editors=None, journal='Journal of Machine Learning Research', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='10', issue='3', pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Nedelec', given_name='T', middle_name=None, surname='Nedelec', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Calauzènes', given_name='C', middle_name=None, surname='Calauzènes', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N El Karoui', given_name='N', middle_name=None, surname='El Karoui', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Perchet', given_name='V', middle_name=None, surname='Perchet', email=None, orcid=None, affiliation=None)], index=42, id='b42', unstructured=None, date='2022', title='Learning in repeated auctions', book_title=None, series_title=None, editors=None, journal='Foundations and Trends® in Machine Learning', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='15', issue='3', pages='176-334', first_page='176', last_page='334', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Nedić', given_name='A', middle_name=None, surname='Nedić', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Ozdaglar', given_name='A', middle_name=None, surname='Ozdaglar', email=None, orcid=None, affiliation=None)], index=43, id='b43', unstructured=None, date='2009', title='Approximate primal solutions and rate analysis for dual subgradient methods', book_title=None, series_title=None, editors=None, journal='SIAM Journal on Optimization', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='19', issue='4', pages='1757-1780', first_page='1757', last_page='1780', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M J Neely', given_name='M', middle_name='J', surname='Neely', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Yu', given_name='H', middle_name=None, surname='Yu', email=None, orcid=None, affiliation=None)], index=44, id='b44', unstructured=None, date='2017', title='Online convex optimization with time-varying constraints', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1702.04783', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G Neu', given_name='G', middle_name=None, surname='Neu', email=None, orcid=None, affiliation=None)], index=45, id='b45', unstructured=None, date='2015', title='Explore no more: Improved high-probability regret bounds for non-stochastic bandits', book_title=None, series_title=None, editors=None, journal='Advances in Neural Information Processing Systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='28', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Paes Leme', given_name='R', middle_name=None, surname='Paes Leme', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Sivan', given_name='B', middle_name=None, surname='Sivan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Teng', given_name='Y', middle_name=None, surname='Teng', email=None, orcid=None, affiliation=None)], index=46, id='b46', unstructured=None, date='2020', title='Why do competitive markets converge to first-price auctions?', book_title='Proceedings of The Web Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='596-605', first_page='596', last_page='605', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Rangi', given_name='A', middle_name=None, surname='Rangi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Franceschetti', given_name='M', middle_name=None, surname='Franceschetti', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Tran-Thanh', given_name='Tran-Thanh', middle_name=None, surname=None, email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L', given_name='L', middle_name=None, surname=None, email=None, orcid=None, affiliation=None)], index=47, id='b47', unstructured=None, date='2018', title='Unifying the stochastic and the adversarial bandits with knapsack', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1811.12253', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K A Sankararaman', given_name='K', middle_name='A', surname='Sankararaman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Slivkins', given_name='A', middle_name=None, surname='Slivkins', email=None, orcid=None, affiliation=None)], index=48, id='b48', unstructured=None, date='2018', title='Combinatorial semibandits with knapsacks', book_title='International Conference on Artificial Intelligence and Statistics', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1760-1770', first_page='1760', last_page='1770', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W Sun', given_name='W', middle_name=None, surname='Sun', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Dey', given_name='D', middle_name=None, surname='Dey', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Kapoor', given_name='A', middle_name=None, surname='Kapoor', email=None, orcid=None, affiliation=None)], index=49, id='b49', unstructured=None, date='2017', title='Safety-aware algorithms for adversarial contextual bandit', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='3280-3288', first_page='3280', last_page='3288', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L Tran-Thanh', given_name='L', middle_name=None, surname='Tran-Thanh', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Chapman', given_name='A', middle_name=None, surname='Chapman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E M De Cote', given_name='E', middle_name='M', surname='De Cote', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Rogers', given_name='A', middle_name=None, surname='Rogers', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N R Jennings', given_name='N', middle_name='R', surname='Jennings', email=None, orcid=None, affiliation=None)], index=50, id='b50', unstructured=None, date='2010', title='Epsilon-first policies for budgetlimited multi-armed bandits', book_title='Twenty-Fourth AAAI Conference on Artificial Intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L Tran-Thanh', given_name='L', middle_name=None, surname='Tran-Thanh', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Chapman', given_name='A', middle_name=None, surname='Chapman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Rogers', given_name='A', middle_name=None, surname='Rogers', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Jennings', given_name='N', middle_name=None, surname='Jennings', email=None, orcid=None, affiliation=None)], index=51, id='b51', unstructured=None, date='2012', title='Knapsack based optimal policies for budget-limited multi-armed bandits', book_title='Proceedings of the AAAI Conference on Artificial Intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='26', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H R Varian', given_name='H', middle_name='R', surname='Varian', email=None, orcid=None, affiliation=None)], index=52, id='b52', unstructured=None, date='2007', title='Position auctions', book_title=None, series_title=None, editors=None, journal='INT J IND ORGAN', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='25', issue='6', pages='1163-1178', first_page='1163', last_page='1178', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Weed', given_name='J', middle_name=None, surname='Weed', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Perchet', given_name='V', middle_name=None, surname='Perchet', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Rigollet', given_name='P', middle_name=None, surname='Rigollet', email=None, orcid=None, affiliation=None)], index=53, id='b53', unstructured=None, date='2016', title='Online learning in repeated auctions', book_title='Conference on Learning Theory', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1562-1583', first_page='1562', last_page='1583', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='X Wei', given_name='X', middle_name=None, surname='Wei', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Yu', given_name='H', middle_name=None, surname='Yu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M J Neely', given_name='M', middle_name='J', surname='Neely', email=None, orcid=None, affiliation=None)], index=54, id='b54', unstructured=None, date='2020', title='Online primal-dual mirror descent under stochastic constraints', book_title=None, series_title=None, editors=None, journal='Proceedings of the ACM on Measurement and Analysis of Computing Systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='4', issue='2', pages='1-36', first_page='1', last_page='36', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Wong', given_name='M', middle_name=None, surname='Wong', email=None, orcid=None, affiliation=None)], index=55, id='b55', unstructured=None, date='2021', title='Moving adsense to a first-price auction', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='Ac-cessed:2023-01-15'), GrobidBiblio(authors=[GrobidAuthor(full_name='X Yi', given_name='X', middle_name=None, surname='Yi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Li', given_name='X', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Xie', given_name='L', middle_name=None, surname='Xie', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K H Johansson', given_name='K', middle_name='H', surname='Johansson', email=None, orcid=None, affiliation=None)], index=56, id='b56', unstructured=None, date='2020', title='Distributed online convex optimization with time-varying coupled inequality constraints', book_title=None, series_title=None, editors=None, journal='IEEE Transactions on Signal Processing', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='68', issue=None, pages='731-746', first_page='731', last_page='746', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Yu', given_name='H', middle_name=None, surname='Yu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Neely', given_name='M', middle_name=None, surname='Neely', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Wei', given_name='X', middle_name=None, surname='Wei', email=None, orcid=None, affiliation=None)], index=57, id='b57', unstructured=None, date='2017', title='Online convex optimization with stochastic constraints', book_title=None, series_title=None, editors=None, journal='Advances in Neural Information Processing Systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='30', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Zinkevich', given_name='M', middle_name=None, surname='Zinkevich', email=None, orcid=None, affiliation=None)], index=58, id='b58', unstructured=None, date='2003', title='Online convex programming and generalized infinitesimal gradient ascent', book_title='Proceedings of the 20th international conference on machine learning (icml-03)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='928-936', first_page='928', last_page='936', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None)], abstract='Online advertising platforms typically use auction mechanisms to allocate ad placements. Advertisers participate in a series of repeated auctions, and must select bids that will maximize their overall rewards while adhering to certain constraints. We focus on the scenario in which the advertiser has budget and return-oninvestment (ROI) constraints. We investigate the problem of budget-and ROI-constrained bidding in repeated non-truthful auctions, such as firstprice auctions, and present a best-of-both-worlds framework with no-regret guarantees under both stochastic and adversarial inputs. By utilizing the notion of interval regret, we demonstrate that our framework does not require knowledge of specific parameters of the problem which could be difficult to determine in practice. Our proof techniques can be applied to both the adversarial and stochastic cases with minimal modifications, thereby providing a unified perspective on the two problems. In the adversarial setting, we also show that it is possible to loosen the traditional requirement of having a strictly feasible solution to the offline optimization problem at each round.', body='Introduction Automatic bidding systems are critical for online advertising as they allow advertisers to efficiently manage and optimize their ad campaigns. These mechanisms automatically adjust bid prices for ad placements based on real-time data and performance metrics. This allows advertisers to reach their target audience, while simplifying the interaction with the platform. In particular, advertisers usually have to specify some parameters like the overall budget for the campaign and their targeting criteria. Then a proxy bidder operated by the platform places bids on the advertiser\'s behalf. A popular autobidding strategy is value maximization subject to a set of constraints encoding the target metrics of the campaign. Typically such constraints include budget constraints reflecting the advertisers\' limited spending capabilities  (Agarwal et al., 2014b; Conitzer et al., 2021; Balseiro et al., 2021a) , and return-on-investment (ROI) constraints, which enforce a target return on the capital spent  (Golrezaei et al., 2021b; Auerbach et al., 2008) . 1   Recently, many advertising platforms have been transitioning from the second-price auction format toward a first-price format because of its simplicity and favorable properties  (Akbarpour & Li, 2018; Paes Leme et al., 2020; Despotakis et al., 2021) . This is the case, for example, for Google\'s Ad Manager and AdSense platforms  (Bigler, 2019; Wong, 2021) . While second-price auctions are a truthful mechanism, meaning that bidders can bid their true value and maximize their utility, this is not the case for first-price auctions. Much of the existing online learning literature leverages this truthfulness in their analysis. The widespread adoption of the first-price format leads us to the motivating question of the present paper: What is an appropriate online bidding strategy for a bidder in a series of non-truthful auctions, considering budget and ROI constraints? Surprisingly, it is unclear what the answer to this question is, as current approaches either concentrate on second-price auctions and heavily rely on truthfulness  (Balseiro & Gur, 2019; Golrezaei et al., 2021a; Feng et al., 2022) , or require an unrealistic amount of prior information to be available to the bidder  (Castiglioni et al., 2022b) . Contributions. At each round t up to the time horizon T , the bidder observes the type of the item being auctioned at t, and has to submit a bid b t to the auctioneer. Then, they receive reward f t (b t ) ∈ [0, 1] and a cost c t (b t ) ∈ [0, 1]. Our framework does not require any particular assumption on the structure of f t and c t , so our results apply more broadly to any single-parameter mechanism with a finite number of types (see Section 3). We present a best-of-bothworlds primal-dual framework with sublinear regret in the stochastic setting (i.e., (f t , c t ) are i.i.d. samples from a fixed but unknown distribution), and constant-factor competitive ratio in the adversarial setting (i.e., (f t , c t ) are selected by an oblivious adversary). In both settings, our framework guarantees vanishing cumulative ROI constraint violation, and cumulative expenditure less then or equal to the available budget. Unlike previous work following a similar primal-dual paradigm (see, e.g.,  Immorlica et al. (2022) ;  Balseiro et al. (2020) ), our analysis is based on the notion on interval regret  (Hazan & Seshadhri, 2007) . Our approach, which involves ensuring that primal and dual regret minimizers are weakly adaptive, improves upon previous work in three main directions: i) in the adversarial setting,  Castiglioni et al. (2022b)  assume the existence of a strictly feasible solution guaranteeing that, for each round t, all constraints are satisfied by a margin of at least α > 0. Moreover, they also assume that this parameter is known beforehand to the bidder. Our framework eliminates the need for the latter assumption, and significantly relaxes the former; ii) it focuses on strict budget constraint satisfaction as opposed to "soft" constraints; iii) best-of-both-worlds algorithm for problems with long-term constraints typically require different proof techniques for the two different input models (see, e.g.,  Castiglioni et al. (2022a) ). Our new approach unifies most of the analysis, with the only difference being in the characterization of a particular set of policies. \n Related Work The problem of online bidding in repeated auctions has been extensively studied using online learning approaches (see, e.g.,  Borgs et al. (2007) ;  Weed et al. (2016) ;  Nedelec et al. (2022) ). In particular, online bidding under resource-consumption (i.e., packing) constraints has been studied in various settings.  Balseiro & Gur (2019)  and  Ai et al. (2022)  focus on utility-maximizing agents with one resource-consumption constraint. In the context of online allocation problems with an arbitrary number of constraints,  Balseiro et al. (2020; 2022)  propose a class of primal-dual algorithms attaining asymptotically optimal performance in the stochastic and adversarial case. In their setting, at each round, the input (f t , c t ) is observed by the decision maker before they make a decision. This makes the problem substantially different from ours. In particular, their framework cannot handle non-truthful repeated auctions. Recent works have also examined settings similar to ours, involving bidders with constraints on their budget and ROI.  Golrezaei et al. (2021a)  propose a threshold-based framework that can manage "soft" budget and ROI constraints in repeated second-price auctions un-der a stochastic input model. The framework by  Feng et al. (2022)  can handle "hard" budget constraints, but crucially relies on truthfulness of second-price auctions, and on the stochasticity of the environment. Finally, the framework by  Castiglioni et al. (2022b)  allows for general "soft" constraints under both stochastic and adversarial inputs. Their framework cannot be applied in our setting for three reasons: i) we have hard budget constraints, ii) we don\'t make the stringent assumption of knowing the parameter α beforehand, and iii) we relax the assumption of having one strictly feasible solution for each round in the adversarial setting. Another related line of works is the one studying the Bandits with Knapsacks (BwK) framework, which was introduced and optimally solved by  Badanidiyuru et al. (2018) . Other regret-optimal algorithms for Stochastic BwK have been proposed by  Agrawal & Devanur (2019) , and by  Immorlica et al. (2022) . The BwK framework has been subsequently extended along many directions (see, e.g.,  Agrawal & Devanur (2019)  The Adversarial Bandits with Knapsacks setting was first studied by  Immorlica et al. (2022) , who proved a O(m log T ) competitive ratio. Recently,  Kesselheim & Singla (2020)  refined that analysis to obtain an O(log m log T ) competitive ratio for the adversarial setting, and  Castiglioni et al. (2022a)  proved a constant-factor competitive ratio in the regime B = Ω(T ). We mention that further results have been obtained in the simplified setting with one constrained resource  (Rangi et al., 2018; György et al., 2007; Tran-Thanh et al., 2010; 2012) . Another line of related work concerns online convex optimization with time-varying constraints (see, e.g.,  Mahdavi et al. (2012; 2013) ;  Jenatton et al. (2016) ;  Neely & Yu (2017) ;  Chen & Giannakis (2018) ), where it is usually assumed that the action set is a convex subset of R m , in each round rewards (resp., costs) are concave (resp., convex), and most importantly, resource constraints only apply at the last round. In contrast, in our setting, budget constraints apply in all rounds. Moreover, guarantees are usually provided either for stochastic constraints  (Yu et al., 2017; Wei et al., 2020) , or for adversarial constraints  (Mannor et al., 2009; Sun et al., 2017; Liakopoulos et al., 2019) , typically by employing looser notions of regret. In constrast, our framework will provide best-of-both-worlds guarantees. \n Preliminaries The set {1, . . . , n}, with n ∈ N, is compactly denoted as [n], and we let [0] be equal to the empty set. Moreover, given a discrete set X , we denote by ∆ X the |X |-simplex. \n Model of the Repeated Interaction We consider the problem faced by a bidder that takes part in a sequence of auctions. At each round t ∈ [T ], the bidder observes their valuation v t extracted from a finite set V ⊂ [0, 1] of n possible valuations. In ad auctions this models the fact that the auctioneer shares with the advertisers some targeting information about users. Then, the bidder chooses b t ∈ B, where B ⊂ [0, 1] is a finite set of m possible bids. Then, the bidder observes utility f t (b t ) with f t : B → [0, 1], and incurs a cost c t (b t ), with c t : B → [0, 1]. We denote as F , respectively C, the set of all the possible functions f t , respectively c t (e.g., F and C may contain all the Lipschitz-continuous functions defined over B). We assume that functions in F and C are measurable with respect to probability measures over B. This ensures that expectations are well-defined, since the functions are assumed to be bounded above, and they are therefore integrable. Moreover, we assume the existence of a void action ∅ such that, for any pair (f, c) ∈ F × C, f (∅) = c(∅) = 0. This can be achieved in many common settings by just bidding 0. The bidder has an overall budget B ∈ R + , which limits the total expenditure throughout the T rounds. We denote by ρ > 0 the per-iteration budget defined as B/T . Moreover, the bidder has a target return-on-investments (ROI) ι > 0. In order to simplify the notation, throughout the paper we will assume ι := 1. This comes without loss of generality: whenever ι > 1 we can suitably scale down values of reward functions f t . Then, the bidder has the goal of maximizing their cumulative utility T t=1 f t (b t ), subject to the following constraints: • Budget constraints: T t=1 c t (b t ) ≤ ρT . Such constraints should be satisfied "no matter what", so we refer to them as hard constraints. • ROI constraints: T t=1 (c t (b t ) − f t (b t )) ≤ 0. We say ROI constraints are soft meaning that we allow, in expectation, for a small (i.e., vanishing in the limit) cumulative violation across the T rounds. Our model can be easily instantiated to describe various single-parameter mechanisms with finite types beyond the well-studied case of second-price auctions. Example 3.1 (First-price auctions). Let β t be the highestcompeting bid at time t. The utility function of the bidder at time t is f t (b t ) := (v t − ωb t )½[b t ≥ β t ], and the cost func- tion is c t (b t ) := b t ½[b t ≥ β t ], where the indicator function ½[b t ≥ β t ] specifies whether the bidder won the auction at time t. The parameter ω ∈ [0, 1] represents the bidder\'s private capital cost which normalizes the bidder\'s accumulated valuation with the overall expenditure. This model includes the traditional quasi-linear set-up (i.e., ω = 1), as well as the value-maximizing utility model (i.e., ω = 0) (see, e.g.,  Babaioff et al. (2021) ;  Balseiro et al. (2021b) ). Example 3.2 (Generalized second-and first-price auctions). At each time t the bidder can be assigned one of s available slots. In the traditional model of separable click probabilities (see, e.g.,  Edelman et al. (2007) ;  Varian (2007) ), each slot j ∈ [s] is associated with a click-through rate r j . At each t, if the bidder does not win any slot then f t (b t ) = c t (b t ) = 0 for any b t ∈ B. If at time t the bidder is assigned slot j, then their utility function is going to be f t : b t → r j v t − c t (b t ), where c t (b t ) is equal to b t in the case of generalized first-price auctions, and to the highest bid which is less than or equal to b t in the case of generalized second-price auctions. \n Regret Minimization A regret minimizer (RM) for a set X is an abstract model for a decision maker repeatedly interacting with a blackbox environment. At each t, a RM performs two operations: (i) NEXTELEMENT(), which outputs an element x t ∈ X ; and (ii) OBSERVEUTILITY(•), which updates the internal state of the RM using the feedback received from the environment. This is either a utility function ℓ t : X → [a, b] ⊆ R (full feedback), or only the value ℓ t (x t ) (bandit feedback), with ℓ t possibly depending adversarially on x 1 , . . . , x t−1 . The objective of the RM is to output a sequence (x t ) T t=1 of points in X so that its cumulative regret, defined as sup x∈X T t=1 (ℓ t (x) − ℓ t (x t )), grows asymptotically sublinearly in T . See  Cesa-Bianchi & Lugosi (2006)  for a review of the various RM available in the literature. \n Baselines Let Π be the set of randomized bidding policies. In particular, each policy is π ∈ Π is a mapping π : V → ∆ B . We denote by π(v) b the probability of selecting b under valuation v. Given valuation v ∈ V, reward function f , and a cost function c, let g : Π ∋ π → E π [c(b)] − ρ be the expected gap between the cost for policy π and the periteration budget ρ, and h : Π ∋ π → E π [c(b) − f (b)] be the expected ROI constraint violation for policy π. We will denote by g t , resp. h t , the constraints defined for the pair (f t , c t ) observed at round t. The bid b t selected for round t is drawn from π t (v t ). Moreover, we will denote by f t (b) (equivalently, we will write g t (b), h t (b)) the value of the reward function under the deterministic bidding policy always playing b ∈ B for each valuation. Auxiliary LP. Let P be an arbitrary probability measure over the space of possible inputs F × C. Then, we define the linear program LP P as follows: OPT P := \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 sup π∈Π E f ∼P f (π) s.t. E P g(π) ≤ 0 E P h(π) ≤ 0 . (LP P ) The above LP selects the bidding policy π that maximizes expected reward according to P, while guaranteeing that constraints g and h encoded by P are satisfied in expectation (both g and h are defined by (f, c) ∼ P). The Lagrangian function L P : Π × R 2 ≥0 → R of LP P is de- fined as L P (π, λ, µ) := E (f,c)∼P [f (π) − λg(π) − µh(π)]. Baselines. Our goal is designing online algorithms that output a sequence of policies π 1 , . . . , π T such that i) the cumulative regret with respect to the performance of the baseline grows sublinearly in T , ii) the budget constraint is (deterministically) satisfied, i.e., T t=1 c t (b t ) ≤ B, and iii) the cumulative ROI constraint violation T t=1 h t (π t ) grows sublinearly in the number of rounds T . The cumulative regret of the algorithm is defined as R T := T OPT − T t=1 f t (b t ) , where the baseline OPT depends on how the input sequence γ := (f t , c t ) T t=1 is generated. We consider two settings for which we define an appropriate value of the baseline, and a suitable problem-specific parameter α ∈ R which is related to the feasibility of the offline optimization problem: • Stochastic setting: at each t ∈ [T ], the pair (f t , c t ) is independently drawn according to a fixed but unknown distribution P over F × C. The baseline is OPT P , which is the standard baseline for stochastic BwK problems since its value is guaranteed to be closed to that of the best dynamic policy  (Badanidiyuru et al., 2018, Lemma 3.1) . In this setting, let α := − inf π∈Π max{E P g(π), E P h(π)}. • Adversarial setting: the sequence of inputs γ is selected by an oblivious adversary. Given γ, we define the following distribution over inputs: for any pair (f, c) ∈ F × C, γ[f, c] = T t=1 ½[f t = f, c t = c]/T . Then, the baseline is the solution of LP γ (i.e., OPT γ ), which is the standard baseline for the adversarial setting (see, e.g.,  Balseiro et al. (2022) ;  Immorlica et al. (2022) ). Therefore, the baseline is obtained by solving the offline problem initialized with the average of the realization observed over the T rounds. Moreover, our results will also hold with respect to the best unconstrained policy. We define α as α : = − inf π max t∈[T ] max{g t (π), h t (π)}. In this setting, α represents the "worst-case feasiblity" with respect to functions observed up to T . We start by developing our analysis under the following standard assumption on the existence of a "safe" policy. In Section 8, we show how this requirement can be relaxed. Assumption 4.1. In the adversarial (resp., stochastic) setting, γ (resp., P) is such that α > 0. This means that LP P and LP γ satisfy (stochastic) Slater\'s condition. In particular, in the adversarial setting we are requiring the existence of a randomized policy that, in expectation, strictly satisfies the constraints for each t. This is a frequent assumption in works focusing on settings similar to ours (see, e.g.,  Chen et al. (2017) ;  Neely & Yu (2017) ;  Yi et al. (2020) ;  Castiglioni et al. (2022b) ). Section 8 shows how this assumption can be relaxed through our framework. When studying primal-dual algorithms, a key implication of Slater\'s condition is the existence and boundedness of Lagrange multipliers (see, e.g.,  Nedić & Ozdaglar (2009) ). Therefore, when α > 0 is known, the set of dual multipliers can be safely bounded by requiring the ℓ 1 -norm of the multiplier to be less than or equal to 1/α (see, e.g.,  Balseiro et al. (2022) ). This is the case, for example, for problems with only budget constraints, in which α = ρ > 0, which is achieved by bidding the void action ∅ at each round. However, ROI constraints complicate the problem as the bidder does not know α beforehand. Therefore, previous frameworks like the one by  Castiglioni et al. (2022b)  cannot be applied in this setting, as Lagrange multipliers cannot be easily bounded without knowledge of α. This begets the question of how to keep the size of dual multipliers under control, even without knowing α. \n Primal-Dual Framework Our framework assumes access to two regret minimizers with the following characteristics. The first one, which we denote by R P , is the primal regret minimizer which outputs policies in Π, and receives as feedback the loss ℓ P t (b t ) := −f t (b t ) + λ t g t (b t ) + µ t h t (b t ) + 1 + λ t + µ t , with b t sampled according to the bidding policy computed. Therefore, the primal regret minimizer has to work under bandit feedback. The primal loss function is obtained from the Lagrangian relaxation of the problem at time t, plus the additive term 1 + λ t + µ t to ensure ℓ P t (•) ∈ R + . The second regret minimizer, which we denote by R D , is the dual regret minimizer. It outputs points in the space of dual variables R 2 ≥0 , and receives as feedback the linear utility u D t : R 2 ≥0 → R such that, for each (λ, µ) ∈ R 2 ≥0 , u D t (λ, µ) := λg t (b t ) + µh t (b t ). The dual regret minimizer R D has full feedback by construction. In order to prove our results, our framework requires R P and R D to be weakly adaptive, that is, they should guarantee sublinear adaptive (a.k.a. interval) regret (see, e.g., ,  Hazan & Seshadhri (2007) ;  Luo et al. (2018) ). This notion of regret is stronger than "standard" external regret (see  Hazan et al. (2016, Chapter 10 )), and it will be essential in our analysis. Let t 1 , t 2 ∈ [T ], t 1 ≤ t 2 . We denote by I := [t 1 , t 2 ] the set {t 1 , t 1 + 1, . . . , t 2 }, and we call I the time interval starting from round t 1 to round t 2 . The primal regret minimizer must be such that, for δ ∈ (0, 1], with probability at least 1 − δ it holds that, for any π ∈ Π and for any interval I, t∈I ℓ P t (b t ) − ℓ P t (π(v t )) ≤ M 2 I E P T,δ , (1) where M I is the maximum absolute value of the losses ℓ P t observed in interval I, and E P T,δ is a term of order Õ( √ T ) (see Theorem 5.2). We require a similar property for the dual regret minimizer. However, since the dual regret minimizer works under full-information feedback we can use a regret minimizer with deterministic regret guarantees. In particular, R D should guarantee that, for any time interval I = [t 1 , t 2 ], and for any pair of dual variables (λ, µ) ∈ R 2 ≥0 it holds t∈I u D t (λ, µ) − u D t (λ t , µ t ) ≤ ν(T )(µ − µ t1 ) 2 + E D,B T + E D,R T , where ν(T ) ≥ 0 is such that ν(T ) = o(T ), and E D,B T (resp., E D,R T ) is a term sublinear in T related to the budget (resp., ROI) constraint. Sections 5.1 and 5.2 will provide precise bounds for our setting, and show that we can build efficient primal and dual regret minimizers with the required features. Algorithm 1 summarizes the structure of our primaldual framework. For each t, the algorithm first computes primal and dual actions at time t by invoking R P .NEXTELEMENT() and R D .NEXTELEMENT(). The bid at time t is going to be b t ∼ π t (v t ) unless the available budget B t is less than 1, in which case we set b t equal to the void action ∅. Then, ℓ P t (b t ) and u D t are observed, and the budget consumption is updated according to the realized cost c t . Finally, the internal state of the two regret minimizer is updated on the basis of the last value observed v t , and the feedback specified by ℓ P t , u D t (see the invocation of OBSERVEUTILITY()). The algorithm terminates when the time horizon T is reached. We will denote by τ ∈ [T ] the time in which the budget is fully depleted and the bidder starts playing the void action ∅. \n Design of the Primal Regret Minimizer In order to provide a suitable primal regret minimizer for Algorithm 1, we start by proving some useful properties of the EXP3-SIX algorithm by  Neu (2015)  in the simpler setting of multi-armed bandits (i.e., when the number of possible valuations is n = 1). In particular, we derive guarantees characterizing the interval regret of the algorithm. As a byproduct of this, Corollary 5.3 provides the guarantee for the case of multiple possible valuations (i.e., n > 1). The resulting primal regret minimizer is described in Algorithm 2. At each round t, the algorithm maintains a set of weights  w t ∈ [0, 1] b t ← b t ∼ π t (v t ) if B t ≥ 1 ∅ otherwise . Observe cost: observe c t (b t ) and update available resources: B t+1 ← B t − c t (b t ) Primal update: • ℓ P t (b t ) ← −f t (b t )+λ t g t (b t )+µ t h t (b t )+1+λ t +µ t • R P .OBSERVEUTILITY(ℓ P t (b t ), v t ) Dual update: • u D t : R 2 ≥0 ∋ (λ, µ) → λg t (b t ) + µh t (b t ) • R D .OBSERVEUTILITY(u D t ) end for the estimated loss lP t , where ξ > 0 is the implicit exploration term. Then, the update of weights w is inspired by the Fixed Share algorithm by  Herbster & Warmuth (1998) . In order to proceed with the analysis, let p t+1 ∈ [0, 1] m be the vector of pre-weights for time t + 1, which is defined as p t+1,b := π t (v t ) b e −η lP t (b) b ′ ∈B π t (v t ) b ′ e −η lP t (b ′ ) for all b ∈ B. Then, we have the following intermediate result (all the omitted proofs can be found in the appendix). Lemma 5.1. Let η > 0 be s.t. η E π lP t (b) < 1 for all t ∈ [T ] and π ∈ Π. Then, for any t ∈ [T ], and b ′ ∈ B, it holds E πt(vt) lP t (b) − lP t (b ′ ) ≤ 1 η log p t+1,b ′ π t (v t ) b ′ + η 2 E lP t (b) 2 . The above lemma allows us to prove the following regret guarantees when focusing on a single valuation. Theorem 5.2. Consider the case in which the set of valuations is a singleton (i.e., n = 1). Let η := 1/ √ mT , ξ := 1/(2 √ mT ), σ := 1/T , and assume that η ≤ 1/M [T ] . Then, for any δ > 0, EXP3-SIX guarantees that, with probability at least 1 − δ, for any interval I = [t 1 , t 2 ], and for any bid b ∈ B, t∈I ℓ P t (b t ) − ℓ P t (b) ≤ M 2 I E P T,δ , where E P T,δ := 3 2 + 4 M I log mT δ + log(T ) + 1 M 2 I √ mT . Algorithm 2 Primal regret minimizer. Input: parameters η > 0, ξ > 0, σ > 0 Initialization: [0, 1] n×m ∋ w 1 ← 1 for t = 1, 2, . . . , T do • Observe valuation v t ∈ V • Set π(v t ) b ← w t,vt,b / b ′ ∈B w t,vt,b ′ , ∀b ∈ B • Bid b t ∼ π • Observe loss ℓ P t (b t ) • lP t (b) ← ℓ P t (b)½[b = b t ]/(π b + ξ) for each b ∈ B • For each b ∈ B set w t+1,vt,b ← (1 − σ)w t,vt,b • e −η lP t (b) + σ m b ′ ∈B w t,vt,b ′ • e −η lP t (b ′ ) end for Theorem 6.2 will show that in our framework the assumption η ≤ 1/M [T ] is always satisfied for our choice of η. This immediately yields the following result for the case in which one independent instance of EXP3-SIX is employed for each valuation in V. Corollary 5.3. Suppose Algorithm 2 is run with the same fixed choice of parameters and assumptions of Theorem 5.2. Then, for any time interval I = [t 1 , t 2 ], the regret accumulated by the algorithm over I is upper bounded by M 2 \n I √ n E P T,δ with probability at least 1 − nδ. \n Design of the Dual Regret Minimizer As a dual regret minimizer we employ the standard online gradient descent algorithm (OGD) (see, e.g.,  Zinkevich (2003) ) on each of the two Lagrangian multipliers λ and µ. We initialize the algorithm by letting µ 1 = λ 1 = 0. We employ two separate learning rates η B and η R , which will be specified in Lemma 5.4. At each round, the dual regret minimizer updates the Lagrangian multipliers as λ t+1 = P [0,1/ρ] (λ t + η B h t (b t )), and µ t+1 = P R+ [µ t + η R h t (b t )] for each t ∈ [T ] , where P denotes the projection operator. The former update performs a gradient step and then projects the result on the interval [0, 1/ρ]. This is possible because we know that playing the void action ∅ would satisfy the budget constraints by a margin of at least ρ, and therefore we can safely consider as the set of λ the interval [0, 1/ρ]  (Castiglioni et al., 2022a) . On the other hand, the update of µ performs a grandient step and then ensures that the value is in R + . Since the bidder does not know the feasibility parameter of ROI constraints, bounding µ becomes more complex, and we show how to do it in Section 6. Given a time interval I = [t 1 , t 2 ], and δ ∈ [0, 1], we let E I T,δ := 2 (t 2 − t 1 ) log(2T /δ) if δ ∈ (0, 1] 0 if δ = 0 , and, when clear from context, we drop the dependency on I to denote E [T ] T,δ . Let E D,B T be a term of order O(T 1/2 /ρ), and E D,R T be a term of order O(T 1/2 ). The regret guarantees of the dual regret minimizer follow from standard results on OGD (see, e.g.,  Hazan et al. (2016, Chapter 10) ). Lemma 5.4. Let λ 1 = 0 and µ 1 = 0. Let η B := 1/(ρT 1/2 ), and η R := 6 + T 1/2 + E D,B T + 6E I T,δ + 16E P T,δ −1 . Then, online gradient descent guarantees that, for any interval I = [t 1 , t 2 ], it holds • t∈I µ t h t (b t ) ≤ 1 η R (µ − µ t1 ) 2 + E D,R T for all µ ∈ R + , • t∈I λ t g t (b t ) ≤ E D,B T for all λ ∈ R + . The dependency on δ in the construction of η R is resolved by Algorithm 1 taking δ in input as a parameter, the final guarantees of the framework will be parametrized on δ. Now, we prove the following simple result characterizing the growth of µ variables. Lemma 5.5. For all t 1 , t 2 ∈ [T ], µ t2 ≥ η R t ′ ∈[t1,t2−1] h t ′ (b t ′ ) + µ t1 . \n Bounding the Lagrange Multipliers Previous work on online optimization with long-term constraints usually assume to know, either exactly or via some upper bound, the parameter α characterizing a strictly feasible solution of the problem (see, e.g.,  Balseiro et al. (2020) ;  Immorlica et al. (2022) ). This information is then used to bound the magnitude of dual multipliers. In our setting, the decision maker has no knowledge of the gap that the strictly feasible solution guarantees for the ROI constraint, which renders the traditional approach not viable. In this section, we show that even without a priori information on α, our framework guarantees that, with high probability, the Lagrange multiplier µ t is bounded by 2/α throughout the entire time horizon. We start by providing a general condition that we will prove to be satisfied both in the stocastich and adversarial setting. Definition 6.1. Given δ ∈ (0, 1], a policy π • is δ-safe if, for any interval I := [t 1 , t 2 ], with t 1 , t 2 ∈ [T ], t 1 < t 2 , it holds t∈I λ t g t (π • ) + µ t h t (π • ) ≤ µ I + 1 α E I T,δ − α t∈I µ t , where µ I is the largest multiplier in the interval I. A safe policy gives to the primal regret minimizer a way to limit the realized penalties imposed by the dual regret minimizer. In particular, we can show that if the dual regret minimizer increased the value of Lagrange multipliers µ t too much, then the primal regret minimizer could "fight back" by playing the safe policy π • , thereby preventing the dual player from being no-regret. Indeed, the next result shows that whenever there exists a safe policy the Lagrange multipliers must be bounded. Theorem 6.2. Suppose that there exists a safe policy and that the primal regret minimizer has regret at most M I E P T,δ for any time interval I. Then, the Lagrange multipliers µ t are such that µ t ≤ 2/α for each t ∈ [τ ]. Then, we show that both in the stochastic and in the adversarial setting there exists a safe policy with high probability, which implies that with high probability the Lagrange multipliers are bounded under both input models. Lemma 6.3. Assume that inputs (f t , c t ) are drawn i.i.d. from P, and that there exists a policy π such that E P g(π) ≤ −α and E P h(π) ≤ −α. Then, there exists a δ-safe policy with probability at least 1 − δ. Lemma 6.4. Assume that the sequence of inputs (f t , g t ) is selected by an oblivious adversary, and that there exists a policy π such that g t (π) ≤ −α and h t (π) ≤ −α for each t ∈ [T ]. Then, there exists a δ-safe policy for any δ ∈ (0, 1]. \n Regret and Violations Guarantees In this section, we describe the guarantees provided by Algorithm 1 in both the stochastic and the adversarial inputs setting. Interestingly, we prove best-of-both-worlds guarantees through a unified argument which captures both settings. This is not the case in previous work, where the analysis of the stochastic setting typically requires to study convergence to a Nash equilibrium of the expected Lagrangian game (see., e.g.,  Immorlica et al. (2022) ; Castiglioni et al. (2022a)), which is not well defined in the adversarial setting. We introduce the following event that holds with high probability. Most of our results will hold deterministically given this event. Definition 7.1. We denote with E the event in which Algorithm 1 satisfies the following two conditions: • the primal regret minimizer has regret upper bounded by (3/α + 1)E P T,δ for all time intervals I; • the dual multipliers due to the ROI constraint are such that µ t ≤ 2/α for each t ∈ [T ]. Then, by applying a union bound to the events of Theorem 5.2 and Lemma 6.3 or Lemma 6.4, we can invoke Theorem 6.2 to obtain the following result. Lemma 7.2. Event E holds with probability at least 1 − 2δ. We start by observing that the cumulative violation of ROI constraints must be sublinear in T with high probability, under both input models. This is a direct consequence of properties of the dual regret minimizer (Section 5.2), and the bound on dual multipliers implied by Lemma 7.2. Lemma 7.3. Assume that event E holds and let τ be the round in which the budget is fully depleted. Then, it holds that t∈[τ ] h t (π t ) ≤ 1 + 2/(η R α). Then, we define the following class of policies. Definition 7.4. Given δ ∈ (0, 1), q ∈ (0, 1), a sequence of T inputs {(f t , c t )} T t=1 , and the value of a baseline OPT, we say that a policy π is (δ, q, OPT)-optimal, if • t∈[T ] f t (π) ≥ q • T • OPT − E T,δ , and • t∈[t ′ ] λ t g t (π) + µ t h t (π) ≤ µ [t ′ ] + 1 α E T,δ for each t ′ ∈ [T ], where µ [t ′ ] is the largest multiplier µ t observed up to t ′ . In other words, a (δ, q, OPT)-optimal policy guarantees a reward which is a fraction q of that of the baseline up to a sublinear term, and guarantees that the cumulative value of the penalty due to the Lagrangian relaxation of the problem is vanishing in time. First, we need the following result that holds both in the stochastic and adversarial setting. Lemma 7.5. Algorithm 1 guarantees that t∈[τ ] λ t g(b t ) ≥ T − τ − 1 ρ − E D,B T . Then, we prove that the existence of a (δ, q, OPT)-optimal policy implies the following regret bound with respect to the generic baseline OPT. Lemma 7.6. Suppose that event E holds and that there exists a (δ, q, OPT)-optimal policy. Then, t∈[τ ] f t (b t ) ≥ qT OPT − C(T, α, δ), where C(T, α, δ) := 1 α + 3 α + 1 E P T,δ + E T,δ + E D,R T + E D,B T . Next, we show that a suitable (δ, q, OPT)-optimal policy exists with high probability both under the stochastic and the adversarial input model. Lemma 7.7. In the stochastic setting, with probability at least 1 − 2δ there exists a (δ, 1, OPT P )-optimal policy (where OPT P is the optimal value of LP P ). This is saying that, fixed an underlying distribution P, there exist with high probability a policy satisfying Definition 7.4. Stochasticity of the environment (i.e., pairs (f t , c t ) are i.i.d. samples from P) is used to prove that the solution to LP P satisfies the second condition of Definition 7.4 for all t ∈ [T ]. If we tried a similar approach in the adversarial setting, the solution to LP γ would guarantee that the second condition is satisfied over the whole time horizon, but not necessarily at earlier time steps t < T . Moreover, feasibility in expectation has no implications on feasibility of a policy under the adversarial sequence (λ t , f t , µ t , c t ), in which dual variables are optimized to "punish violations". However, it is possible to show the existence of a policy satisfying Definition 7.4 even in the adversarial setting by following a different approach. In particular, we show that it is possible to build a suitable convex combination between a strictly feasible policy π • guaranteeing that all constraints are satisfied by at least a margin α > 0 for each t ∈ [T ], and the optimal unconstrained policy π * maximizing t∈[T ] f t (π). The following lemma employs a policy π such that, for all v ∈ V, b ∈ B, π(v) b := π • (v) b /(1 + α) + απ * (v) b /(1 + α). Lemma 7.8. In the adversarial setting, there always exists a (0, α/(1 + α), OPT γ )-optimal policy. Now, we provide the overall guarantees of Algorithm 1. Theorem 7.9 (Stochastic setting). For each t ∈ [T ], let (f t , c t ) be i.i.d. samples from a fixed but unknown distribution P over the set of possible inputs. For δ > 0, with probability at least 1 − 4δ, Algorithm 1 guarantees T OPT P − t∈[T ] f t (b t ) ≤ C(T, α, δ). Moreover, we have t∈[T ] h t (b t ) ≤ 1 + 2 η R α , and t∈[T ] c t (b t ) ≤ B. Proof. The regret upper bound holds since event E holds with probability at least 1 − 2δ (Lemma 7.2), and by combining Lemma 7.7 and Lemma 7.6. The ROI constrain is upper bounded by Lemma 7.3, and the budget constraint is strictly satisfied by construction of Algorithm 1. Notice that if we initialize primal and dual regret minimizers as detalied in Section 5.1 we have that the cumulative regret and the cumulative ROI constraint violation are of order Õ( √ T ), while the budget constrained is strictly satisfied. Analogously, by exploiting Lemma 7.8, we have the following guarantees for the adversarial setting, in which we show that it is possible to achieve a α/(1 + α) fraction of the cumulative reward of the baseline, while guaranteeing that constraint violations are under control. Theorem 7.10 (Adversarial setting). Suppose the sequence of inputs γ = (f t , c t ) T t=1 is selected by an oblivious adversary. Then, for δ > 0, with probability at least 1 − 2δ, Algorithm 1 guarantees α 1 + α OPT γ − t∈[T ] f t (b t ) ≤ C(T, α, δ). 2 Moreover, t∈[T ] h t (b t ) ≤ 1 + 2 η R α and t∈[T ] c t (b t ) ≤ B. Our competitive ratio matches that of  Castiglioni et al. (2022b) , and, in the case in which we only have budget constraints, it yields the state-of-the-art 1/α competitive ratio of  Castiglioni et al. (2022a) . \n Relaxing the Safe-Policy Assumption In the adversarial online setting with long-term constraints, the usual assumption for recovering Slater\'s condition is that there exist a policy guaranteeing that constraints are satisfied by at least α > 0 for each t  (Chen et al., 2017; Yi et al., 2020; Castiglioni et al., 2022b) . Our analysis, up to this point, made the same assumption (Assumption 4.1), except that, unlike those past works, we do not need to know the value of α. Now, we show that our analysis carries over with the following looser requirement. Assumption 8.1. There exists a policy π • ∈ Π such that, for each interval I = [t 1 , t 2 ] with t 2 − t 1 = k, we have t∈I g t (π • ) ≤ −αk and t∈I h t (π • ) ≤ −αk. The traditional assumption of requiring a safe policy for each t would require the bidder to have an action yielding expected ROI which is strictly above their target for each round t. This may not hold practice. For example, if we assume one ad placement is being allocated at each t then the bidder would be priced out by other bidders for at least some time steps (the traditional assumption is more natural if we consider a set of auctions per t, instead of a single one, since bidders typically have user segments for which they have good ROI). Next, we show that if the size of the intervals k is not too big (i.e., if there exists a "safe" policy frequently enough), there exist the following policies. Lemma 8.2. Suppose Assumption 8.1 holds with k < E T,δ /(2T η B ). Then, for δ > 0, there exists a δ-safe and a (δ, α/(1 + α), OPT γ )-optimal policy. This result allows us to balance the tightness of the required assumption with the final regret guarantees, by suitably choosing the learning rates rates η B and η R . When Assumption 8.1 holds for k = log T , we recover exactly the bounds of Theorems 7.9 and 7.10. As a further example, if k = T 1/4 , then we can obtain regret guarantees of order Õ(T 3/4 ) by setting η B = O(T −3/4 ) and by suitably updating the definition of E T,δ . In the context of auctions, this allows us to make the milder assumption of requiring that the bidder sees an auction with strictly positive ROI at least every k steps, instead of at every step. A. Proofs for Section 5 Lemma 5.1. Let η > 0 be s.t. η E π lP t (b) < 1 for all t ∈ [T ] and π ∈ Π. Then, for any t ∈ [T ], and b ′ ∈ B, it holds E πt(vt) lP t (b) − lP t (b ′ ) ≤ 1 η log p t+1,b ′ π t (v t ) b ′ + η 2 E lP t (b) 2 . Proof. Let y := π t (v t ) ∈ ∆ m . By the fact that, for any x ≥ 0, e −x ≤ 1 − x + x 2 /2, we have that − log E y e −η lP t (b) ≥ − log 1 − ηE y lP t (b) + η 2 2 E y ℓ P t (b) 2 log E y e −η lP t (b) ≤ −ηE y lP t (b) + η 2 2 E y lP t (b) 2 , where the second inequality holds since, by assumption, ηE y lP t (b) < 1, which implies that the argument of the logarithm is strictly greater than 0. Then, by definition of the preweights p t+1 , we have that, for any b ′ ∈ B, ηE y lP t (b) ≤ − log E y e −η lP t (b) + η 2 2 E y lP t (b) 2 = − log π t,b ′ e −η lP t (b ′ ) p t+1,b ′ + η 2 2 E y lP t (b) 2 . This yields E πt(vt) lP t (b) − lP t (b ′ ) ≤ 1 η log p t+1,b ′ π t,b ′ (v t ) + η 2 E y lP t (b) 2 , for any possible alternative bid b ′ ∈ B. Theorem 5.2. Consider the case in which the set of valuations is a singleton (i.e., n = 1). Let η := 1/ √ mT , ξ := 1/(2 √ mT ), σ := 1/T , and assume that η ≤ 1/M [T ] . Then, for any δ > 0, EXP3-SIX guarantees that, with probability at least 1 − δ, for any interval I = [t 1 , t 2 ], and for any bid b ∈ B, t∈I ℓ P t (b t ) − ℓ P t (b) ≤ M 2 I E P T,δ , where E P T,δ := 3 2 + 4 M I log mT δ + log(T ) + 1 M 2 I √ mT . Proof. In order to increase the readability, we will write π t in place of π t (v) since v ∈ V is constant throughout the proof. By definition of lP t , we have that for any b ∈ B and π, E lP t (b) ≤ E[½[b = b t ]ℓ P t (b)/π b ] = ℓ P t (b). Therefore, since by assumption we have η < 1/M [T ] , where M [T ] is the maximum range of the loss functions ℓ P t over the time horizon, the assumption of Lemma 5.1 holds. Then, for any interval [t 1 , t 2 ], with t 1 , t 2 ∈ [T ], t 1 < t 2 , by Lemma 5.1 we have that for any b ′ ∈ B, t∈[t1,t2] E πt lP t (b) − lP t (b ′ ) ≤ t∈[t1,t2] 1 η log p t+1,b ′ π t,b ′ + η 2 E πt lP t (b) 2 . Moreover we have that t∈[t1,t2] log p t+1,b ′ π t,b ′ = log 1 π t1,b ′ + t∈[t1+1,t2] log p t,b ′ π t,b ′ + log(p t2+1,b ′ ) ≤ log m σ + t∈[t1+1,t2] log 1 1 − σ . The last inequality holds since, for any t ∈ [T ] and b ∈ B, π t+1,b = (1 − σ)w t,b e −η lP t (b) + σ/m i∈B w t,i e −η lP t (i) i∈B (1 − σ)w t,i e −η lP t (i) + σ/m j∈B w t,j e −η lP t (j) = (1 − σ)w t,b e −η lP t (b) + σ/m i∈B w t,i e −η lP t (i) i∈B w t,i e −η lP t (i) ≥ (1 − σ) w t,b e −η lP t (b) i∈B w t,i i∈B w t,i i∈B w t,i e −η lP t (i) = (1 − σ) π t,b e −η lP t (b) E πt e −η lP t (i) = (1 − σ)p t+1,b , where we used the definition of π t and p t as per Algorithm 2. Then, t∈[t1,t2] E πt lP t (b) − lP t (b ′ ) ≤ 1 η log m σ + (t 2 − t 1 − 1) log 1 1 − σ + η 2 t∈[t1,t2] E i∼πt lP t (b) 2 . (2) Neu (2015, Lemma 1) states that given a fixed non-increasing sequence (ξ t ) with ξ t ≥ 0, and by letting β t,i be a nonnegative random variable such that β t,i ≤ 2ξ t for all t and i ∈ B, then with probability at least 1 − δ, t∈[T ] i∈B β t,i lP t (i) − ℓ P t (i) ≤ log(1/δ). Then, for any bid i ∈ B, by setting β t,j = 2ξ t ½[i = j] if t ∈ I 0 otherwise , and by applying a union bound we obtain that, with probability at least 1 − δ, t∈I lP t (i) − ℓ P t (i) ≤ M I log(m/δ) 2ξ , (3) where M I is the maximum range of the loss functions ℓ P t over time interval I. Moreover, from the definition of lP t (see Algorithm 2), we have that t∈I E b∼πt lP t (b) = t∈I ℓ P t (b t ) − b∈B ξ lP t (b) . (4) Finally, given t ∈ I, we observe that E b∼πt lP t (b) 2 = b∈B (π t,b lP t (b)) lP t (b) ≤ M I b∈B lP t (b). (5) Finally, we conclude by showing that, for any b ∈ B, with probability at least 1 − δ, t∈[t1,t2] ℓ P t (b t ) − ℓ P t (b) ≤ t∈[t1,t2] ℓ P t (b t ) + M I log(m/δ) 2ξ − t∈[t1,t2] lP t (b) by Equation (3) = M I log(m/δ) 2ξ + t∈[t1,t2] i∈B ξ lP t (i) + t∈[t1,t2] E πt lP t − lP t (b) by Equation (  4 ) ≤ M I log(m/δ) 2ξ + t∈[t1,t2] i∈B ξ lP t (i) by Equation (2) and Equation (  5 ) + 1 η log m σ + (t 2 − t 1 − 1) log 1 1 − σ + η 2 M I t∈[t1,t2] i∈B lP t (i) ≤ M I log(m/δ) 2ξ + (t 2 − t 1 )mξM I + 1 η log m σ + (t 2 − t 1 − 1) log 1 1 − σ + (t 2 − t 1 ) ηM 2 I m 2 . By setting η = 1 √ mT , ξ = 1 2 √ mT , with probability at least 1 − δ, t∈[t1,t2] ℓ P t (b t ) − ℓ P t (b) ≤ 3 2 M 2 I (t 2 − t 1 ) m T + √ mT M I log m δ + log m σ + (T − 1) log 1 1 − σ ≤ 3 2 M 2 I (t 2 − t 1 ) m T + √ mT 2M I log m δ − log σ(1 − σ) T −1 . By  ℓ P t (b t ) − ℓ P t (b) ≤ 3 2 M 2 I (t 2 − t 1 ) m T + √ mT 2M I log m δ + log(eT ) . By using the union bound over all possible intervals [t 1 , t 2 ] we obtain that, with probability at least 1 − δ, t∈[t1,t2] ℓ P t (b t ) − ℓ P t (b) ≤ 3 2 M 2 I (t 2 − t 1 ) m T + √ mT 4M I log mT δ + log(T ) + 1 ≤ M 2 I 3 2 + 4 M I log mT δ + log(T ) + 1 M 2 I √ mT , which proves our statement. Lemma 5.5. For all t 1 , t 2 ∈ [T ], µ t2 ≥ η R t ′ ∈[t1,t2−1] h t ′ (b t ′ ) + µ t1 . Proof. We prove the result by induction. First, it\'s easy to see that the result holds for t = 1, since µ 1 = 0. Then, suppose that the statement holds for round t. Then, µ t+1 = [µ t + η R h t (b t )] + ≥ µ t + η R h t (b t ) ≥ η R t ′ ∈[t−1] η R h t ′ (b t ′ ) + η R h t (b t ) = η R t ′ ∈[t] ηh t ′ (b t ′ ), where [x] + := max{x, 0}. This concludes the proof. \n B. Proofs for Section 6 Theorem 6.2. Suppose that there exists a safe policy and that the primal regret minimizer has regret at most M I E P T,δ for any time interval I. Then, the Lagrange multipliers µ t are such that µ t ≤ 2/α for each t ∈ [τ ]. Proof. We consider two cases. Case 1: α ≤ 10/ √ T . By construction of the dual regret minimizer, and by the choice of η R , the dual variable µ t can reach at most value η R T ≤ √ T /16. Therefore, we have µ t ≤ √ T /16 < 2/α. Case 2: α > 10/ √ T . Let I = [t 1 , t 2 ], with t 1 , t 2 ∈ [T ], t 1 ≤ t 2 . Moreover, assume that there exists a safe policy π • . We show that, if the Lagrangian multiplier µ t is greater than 2/α, we reach a contradiction. Suppose, by contradiction, that there exists a round t 2 such that µ t2 ≥ 2/α. Let t 1 be the the first round such that µ t ≥ 1/α for any t ∈ [t 1 , t 2 ]. Notice that the structure of the dual regret minimizer (see Section 5.2) implies that µ t1 ≤ 1/α + η R and µ t2 ≤ 2/α + η R , (6) since the dual losses are in [−1, 1]. Therefore, we can upperbound the primal loss function as M [T ] ≤ (1 + 4/α) (i.e., we use µ t ≤ 3/α). This implies that, for m ≥ 2, ηM [T ] = 1 √ mT 1 + 4 α < 1 √ mT 1 + 2 √ T 5 ≤ 1. Therefore, Theorem 5.2 applies and the primal regret minimizer satisfies the bound on the adaptive regret of Equation (1). Then, by the no-regret property of the primal we get: t∈I (f t (b t ) − λ t g t (b t ) − µ t h t (b t )) ≥ t∈I (f t (π • ) − λ t g t (π • ) − µ t h t (π • )) − M 2 I E P T,δ ≥ α t∈I µ t − µ I + 1 α E I T,δ − M 2 I E P T,δ (by Definition 6.1) ≥ (t 2 − t 1 ) − µ [t1,t2−1] + 3 α + η R E I T,δ − M 2 I E P T,δ (by Def. of t 1 and Equation (  6 )) ≥ (t 2 − t 1 ) − 5 α + η R E I T,δ − 1 + 2 α + 1 α 2 E P T,δ (by Def. of M I and ℓ P t ) ≥ (t 2 − t 1 ) − 5 α + η R E I T,δ − 16 α 2 E P T,δ . (7) By Lemma 5.5 it holds µ t2 ≥ η R t ′ ∈[t1,t2−1] h t ′ (b t ′ ) + µ t1 . Hence, by definition of t 2 and Equation (  6 ), t ′ ∈[t1,t2−1] h t ′ (b t ′ ) ≥ µ t2 − µ t1 η R ≥ 1 αη R − 1. Then, by the regret bound of the dual with respect to µ = µ t and λ = 0, we get t∈[t1,t2−1] (f t (b t ) − λ t g t (b t ) − µ t h t (b t )) ≤ t∈[t1,t2−1] (f t (b t ) − µ t h t (b t )) + E D,R T + E D,B T ≤ (t 2 − t 1 ) − 1 α t∈[t1,t2−1] h t (b t ) + E D,R T + E D,B T ≤ (t 2 − t 1 ) − 1 α 2 η + 1 α + E D,R T + E D,B T . (8) By putting Equation (  7 ) and Equation (  8 ) together we have that (t 2 − t 1 ) − 1 α 2 η R + 3 α + 2 + E D,R T + E D,B T ≥ (t 2 − t 1 ) − 5 α + η R E I T,δ − 16 α 2 E P T,δ . We observe that in Lemma 5.4 we set η R := 6 + E D,R T + E D,B T + 6E I T,δ + 16E P T −1 . Then, from the inequality above we have 1 α 2 η R ≤ 3 α + 2 + E D,R T + E D,B T + 5 α + 1 E I T,δ + 16 α 2 E P T,δ . However, we reach a contradiction since 1 α 2 η R ≥ 4 α + 2 + E D,R T + E D,B T + 5 α + 1 E I T,δ + 16 α 2 E P T > 3 α + 2 + E D,R T + E D,B T + 5 α + 1 E I T,δ + 16 α 2 E P T , where we used the fact that α ∈ (0, 1] by assumption (α > 0), and by boundedness of g t and h t for all t ∈ [T ]. This concludes the proof. Lemma 6.4. Assume that the sequence of inputs (f t , g t ) is selected by an oblivious adversary, and that there exists a policy π such that g t (π) ≤ −α and h t (π) ≤ −α for each t ∈ [T ]. Then, there exists a δ-safe policy for any δ ∈ (0, 1]. \n Proof. By assumption there exists a policy π such that g t (π) ≤ −α and h t (π) ≤ −α for each t ∈ [T ]. Then, for each  t 1 , t 2 ∈ [T ], with t 1 < t 2 , it holds t∈[t1,t2] (λ t g t (π) + µ t h t (π)) ≤ −α t∈[t1,t2] (λ t + µ t ) ≤ −α t∈[t1,t2] µ t , (π) + µ t h t (π)) ≤ µ I + 1 α E I T,δ + t∈I (λ t E P g(π) + µ t E P h(π)) ≤ µ I + 1 α E I T,δ − α t∈I (λ t + µ t ) ≤ µ I + 1 α E I T,δ − α t∈I µ t . This concludes the proof. C. Proofs for Section 7 Lemma 7.2. Event E holds with probability at least 1 − 2δ. Proof. By Lemmas 6.4 and 6.3, we have that in both settings there exists a safe policy with probability at least 1 − δ. Moreover, by Corollary 5.3 with probability at least 1 − δ the regret of the primal is upperbounded by M I E P T for each interval I = [t 1 , t 2 ], t 1 , t 2 ∈ [T ]. Applying a union bound sufficies to show that the two events hold simultaneously with probability at least 1 − 2δ. Then, the statement directly follows from Theorem 6.2. Lemma 7.3. Assume that event E holds and let τ be the round in which the budget is fully depleted. Then, it holds that t∈[τ ] h t (π t ) ≤ 1 + 2/(η R α). Proof. By the definition of event E we have that µ τ ≤ 2/α. Moreover, by Lemma 5.5 it holds that µ τ ≥ η R t∈[τ −1] h t (π t ). Hence, t∈[τ ] h t (π t ) ≤ µ τ /η R + 1 ≤ 2/(η R α) + 1. Lemma 7.5. Algorithm 1 guarantees that t∈[τ ] λ t g(b t ) ≥ T − τ − 1 ρ − E D,B T . Proof. We consider two cases. • If τ = T , then t∈[τ ] λ t g t (b t ) ≥ −E D,B T ≥ T − τ − 1 ρ − E D,B T . • Otherwise, if τ < T , t∈[τ ] λ t g t (b t ) ≥ 1 ρ t∈[τ ] g t (b t ) − E D,B τ = 1 ρ t∈[τ ] (c t (b t ) − ρ) − E D,B τ = 1 ρ (B − 1 − τ ρ) − E D,B τ = T − τ − 1 ρ − E D,B τ . where the first inequality follows by the no-regret guarantee of the dual regret minimizer with respect to the fixed choice of λ = 1/ρ, and then we use the definition of g t and the fact that τ is the time at which the budget is depleted, that is the round in which the available budget becomes strictly smaller than 1 (see Algorithm 1). This concludes the proof. Lemma 7.6. Suppose that event E holds and that there exists a (δ, q, OPT)-optimal policy. Then, t∈[τ ] f t (b t ) ≥ qT OPT − C(T, α, δ), where C(T, α, δ) := 1 α + 3 α + 1 E P T,δ + E T,δ + E D,R T + E D,B T . Proof. Let π * be a (δ, q, OPT)-optimal policy. Then, we have that t∈[τ ] f t (b t ) ≥ t∈[τ ] (f t (π * ) − λ t g t (π * ) − µ t h t (π * ) + λ t g t (b t ) + µ t h t (b t )) − 3 α + 1 E P τ,δ ≥ t∈[τ ] (f t (π * ) + λ t g t (b t ) + µ t h t (b t )) − 3 α E τ,δ − 3 α + 1 E P τ,δ ≥ t∈[τ ] f t (π * ) + t∈[τ ] λ t g t (b t ) − 3 α E τ,δ − 3 α + 1 E P τ,δ − E D,R τ ≥ t∈[τ ] f t (π * ) + T − τ − 1 ρ − 3 α E τ,δ − 3 α + 1 E P τ,δ − E D,R τ − E D,B T ≥ t∈[T ] f t (π * ) − 1 ρ − 3 α E τ,δ − 3 α + 1 E P τ,δ − E D,R τ − E D,B T ≥ qT OPT − 1 ρ − 3 α + 1 E P T,δ + E T,δ − E D,R T − E D,B T , where the first inequality comes from the regret bound of the primal regret minimizer, the second follows by the definition of (δ, q, OPT)-optimal policy, the third follows by the no-regret guarantee of the dual regret minimizer with respect to action µ = 0, the fourth one follows from Lemma 7.5. Finally, the fifth inequality follows from the fact that f t (•) ∈ [0, 1], and the last one is by definition of (δ, q, OPT)-optimal policy. This proves our statement. Lemma 7.7. In the stochastic setting, with probability at least 1 − 2δ there exists a (δ, 1, OPT P )-optimal policy (where OPT P is the optimal value of LP P ). Proof. Let π * be an optimal solution to LP γ . We show that, with probability at least 1 − δ, the policy π * is (δ, 1, OPT γ )optimal, proving the statement. First, by Azuma-Hoeffding inequality we have that, for t ′ ∈ [T ], with probability at least 1 − δ t∈[t ′ ] (λ t g t (π * ) + µ t h t (π * ) − λ t E P g(π * ) − µ t E P h(π * )) ≤ µ [t ′ ] + 1 α 2T log 1 δ , where µ [t ′ ] is the largest dual multiplier µ t observed up to t ′ . Notice that we cannot upper bound it righ away as 2/α because here we are not requiring event E (Definition 7.1) to hold. Then, assuming T > 2, by taking a union bound over all possible rounds t ′ , we get that the following inequality holds with probability at least 1 − δ simultaneously for all t ′ ∈ [T ], t∈[t ′ ] (λ t g t (π * ) + µ t h t (π * ) − λ t E P g(π * ) − µ t E P h(π * )) ≤ µ [t ′ ] + 1 α 2T log T δ ≤ µ [t ′ ] + 1 α E T,δ . Similarly, we can prove that Assuming T > 2 and applying an union bound, the statement follows. Lemma 7.8. In the adversarial setting, there always exists a (0, α/(1 + α), OPT γ )-optimal policy. Proof. Let π • be a strictly feasible policy such that α = − max t∈[T ] max{g t (π • ), h t (π • )}, with α > 0, and let π * ∈ arg max π∈Π t∈[T ] f t (π) be an optimal unconstrained policy. It holds t∈[T ] f t (π * ) ≥ T OPT γ since the optimal uncostrained policy is better than the optimal constrained policy, which is a solution to LP γ . Then, consider the policy π such that, for each v ∈ V, b ∈ B, π(v) b = 1 1 + α π • (v) b + α 1 + α π * (v) b , where, given a policy π, we denote by π(v) b the probability of bidding b under valuation v. At each iteration we have that both the budget and the ROI constraints are satisfied by the policy π (in expectation with respect to π). Indeed, for each t ∈ [T ], we have thath g t (π) = 1 1+α g t (π • ) + α 1+α g t (π * ) ≤ −α 1+α + α 1+α ≤ 0. Similarly, we can prove that for each t ∈ [T ] it holds h t (π) ≤ 0. Then, the policy π satisfies the condition t∈[t ′ ] (λ t g t (π) + µ t h t (π)) ≤ 0 for each t ′ ∈ [T ]. Moreover, t∈[T ] f t (π) = t∈[T ] 1 1 + α f t (π • ) + α 1 + α f t (π * ) ≥ t∈[T ] α 1 + α f t (π * ) ≥ α 1 + α OPT γ , which satisfies the first condition of Definition 7.4. This concludes the proof. \n D. Proofs for Section 8 Lemma 8.2. Suppose Assumption 8.1 holds with k < E T,δ /(2T η B ). Then, for δ > 0, there exists a δ-safe and a (δ, α/(1 + α), OPT γ )-optimal policy. Proof. First, we need to show that there exists a δ-safe policy. In particular, we show that there exists a policy π • such that, for any time interval I = [t 1 , t 2 ], it holds t∈I (λ t g t (π • ) + µ t h t (π • )) ≤ E T,δ − α t∈I (µ t + λ t ). (9) To do that, we show that the interval I can be split in smaller intervals of length k, and for each of such smaller intervals I ′ , it holds t∈I ′ (λ t g t (π • ) + µ t h t (π • )) ≤ 2k 2 η B − α t∈I ′ (µ t + λ t ). We show that this holds for any I ′ of length k in Lemma D.1. Then, the cumulative sum on the original interval I is at most t∈I (λ t g t (π where the second inequality comes from t∈I g t (π • ) ≤ −αk and t∈I h t (π • ) ≤ −αk. This concludes the proof. letting h(x) := −x log x − (1 − x) log(1 − x) be the binary entropy function for x ∈ [0, 1], we have that, for x ∈ [0, 1], h(x) ≤ x log(e/x) (see, e.g., Cesa-Bianchi et al. (2012, Corollary 1)). Then, for σ = 1/T , we have that − log σ(1 − σ) T −1 ≤ log(eT ). This yields t∈[t1,t2]     \n t∈[T ] (f t (π * ) − E P f (π * )) ≤ 2 T log 2T δ = E T,δholds with probability at least 1 − δ. Then,t∈[T ] f t (π * ) ≥ t∈[T ] E P f (π * ) − E I T,δ = OPT γ − E T,δ . \n n,m . The probability of playing b under valuation v t is proportional to the weight w t,vt,b . After drawing b t , the primal regret minimizer observes ℓ P Primal-Dual framework. Input: parameters B, T, δ; regret minimizers R P and R D Initialization: B 1 ← B; initialize R P , R D for t = 1, 2, . . . , T do Dual decision: (λ t , µ t ) ← R D .NEXTELEMENT() Primal decision: Π ∋ π t ← R P .NEXTELEMENT() Observe valuation v t and select bid as t (b t ) and builds Algorithm 1 \n Lemma 6.3. Assume that inputs (f t , c t ) are drawn i.i.d. from P, and that there exists a policy π such that E P g(π) ≤ −α and E P h(π) ≤ −α. Then, there exists a δ-safe policy with probability at least 1 − δ.Proof. By the definition of α, there exists a policy π such that E P g(π) ≤ −α and E P h(π) ≤ −α. Then, given a time interval I = [t 1 , t 2 ], t 1 , t 2 ∈ [T ], by appling the Azuma-Hoeffding inequality to the martingale difference sequence W 1 , . . . , W T with W t := λ t g t (π) + µ t h t (π) − λ t E P g(π) − µ t E P h(π), we obtain that which implies that π is δ-safe for any δ ∈ (0, 1]. t∈I W t ≤ µ I + 1 α 2(t 2 − t 1 ) log 2 δ holds with probability at least 1 − δ. By applying a union bound we get that the inequalities for each time interval I hold simultaneously with probability at least 1 − T 2 δ. Let E I T,δ := 2 (t 2 − t 1 ) log 2T δ as per Definition 6.1. Then, with probability at least 1 − δ it holds t∈I (λ t g t \n Then, we can show that a (δ, α/(1 + α), OPT γ )-optimal policy exists. In particular, by defining a policy π as in the proof of Lemma 7.8, we havet∈I (λ t g t (π) + µ t h t (π)) = 1 1 + α t∈I (λ t g t (π • ) + µ t h t (π • )) + (λ t g t (π * ) + µ t h t (π * ))where the first inequality is by Equation (9). The first condition of Definition 7.4 can be shown to hold with the same steps of Lemma 7.8. This concludes the proof.Lemma D.1. For any time interval I of length k, there exist a policy π • ∈ Π for which it holdst∈I (λ t g t (π • ) + µ t h t (π • )) ≤ 2k 2 η B − α t∈I (µ t + λ t ).Proof. Given an interval I of lenght k, let ( λ, μ) be the largest Lagrangian multipliers in the interval I, and let (λ, µ) be the smallest Lagrangian multipliers in such interval. Let G := max λ − λ, μ − µ . Then, we have G ≤ kη B since η B is more aggressive than η R (see Section 5.2), and there are at most k gradient updates in the interval. Then, \uf8eb \uf8f6 α 1 + α \uf8ed t∈[t ′ ] \uf8f8 ≤ E T,δ − α 1 + α t∈I (µ t + λ t ) + α 1 + α t∈I (λ t + µ t ) ≤ E T,δ ≤ 3 α E T,δ , • ) + µ t h t (π • )) ≤ |I| k \uf8eb \uf8ed 2k 2 η B − α t∈ Î (µ t + λ t ) \uf8f6 \uf8f8 ≤ 2T kη B − α t∈I (µ t + λ t ) ≤ E T,δ − α t∈I (µ t + λ t ),where we set Î ∈ arg max I ′ :|I ′ |=k t∈I ′ (µ t + λ t ). This shows that Equation (9) holds for any interval I. t∈I(λ t g t (π • ) + µ t h t (π • )) ≤ t∈I:gt(π • )>0 λg t (π • ) + t∈I:gt(π • )≤0 λg t (π • ) + t∈I:ht(π • )>0 μh t (π • ) + t∈I:ht(π • )≤0 µg t (π • ) ≤ − λ\uf8eb \uf8ed t∈I:gt(π • )≤0 g t (π • ) + αk \uf8f6 \uf8f8 + t∈I:gt(π • )≤0 λg t (π • ) − μ\uf8eb \uf8ed t∈I:ht(π • )≤0 h t (π • ) + αk \uf8f6 \uf8f8 + t∈I:ht(π • )≤0 µg t (π • ) ≤ kG − αk λ + kG − αk μ ≤ 2kG − α t∈I (λ t + µ t ) ≤ 2k 2 η B − t∈I (λ t + µ t ), \n\t\t\t We observe that the same guarantees would hold with respect to the optimal unconstrained policy maximizing ft(π).', acknowledgement=None, annex=None)
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Indradumna Banerjee', given_name='Indradumna', middle_name=None, surname='Banerjee', email='indradumna.banerjee@oracle.com', orcid=None, affiliation=None), GrobidAuthor(full_name='Prateek Katageri', given_name='Prateek', middle_name=None, surname='Katageri', email='prateek.katageri@oracle.com', orcid=None, affiliation=None), GrobidAuthor(full_name='Atin Modi', given_name='Atin', middle_name=None, surname='Modi', email='atin.modi@oracle.com', orcid=None, affiliation=None)], index=None, id=None, unstructured=None, date=None, title='MLOps with enhanced performance control and observability', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), pdf_md5='AB40BA5399978138A6AFF015134429BF', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='Andrei Paleyes', given_name='Andrei', middle_name=None, surname='Paleyes', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Raoul-Gabriel Urma', given_name='Raoul-Gabriel', middle_name=None, surname='Urma', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Neil D Lawrence', given_name='Neil', middle_name='D', surname='Lawrence', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date='2020', title='Challenges in deploying machine learning: a survey of case studies', book_title=None, series_title=None, editors=None, journal='ACM Computing Surveys (CSUR)', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Philipp Ruf', given_name='Philipp', middle_name=None, surname='Ruf', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Manav Madan', given_name='Manav', middle_name=None, surname='Madan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Christoph Reich', given_name='Christoph', middle_name=None, surname='Reich', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Djaffar Ould-Abdeslam', given_name='Djaffar', middle_name=None, surname='Ould-Abdeslam', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='2021', title='Demystifying mlops and presenting a recipe for the selection of open-source tools', book_title=None, series_title=None, editors=None, journal='Applied Sciences', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='11', issue='19', pages='8861', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Yue Zhou', given_name='Yue', middle_name=None, surname='Zhou', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Yue Yu', given_name='Yue', middle_name=None, surname='Yu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Bo Ding', given_name='Bo', middle_name=None, surname='Ding', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date='2020', title='Towards mlops: A case study of ml pipeline platform', book_title='2020 International conference on artificial intelligence and computer engineering (ICAICE)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='IEEE', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='494-500', first_page='494', last_page='500', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Sasu Mäkinen', given_name='Sasu', middle_name=None, surname='Mäkinen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Henrik Skogström', given_name='Henrik', middle_name=None, surname='Skogström', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Eero Laaksonen', given_name='Eero', middle_name=None, surname='Laaksonen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Tommi Mikkonen', given_name='Tommi', middle_name=None, surname='Mikkonen', email=None, orcid=None, affiliation=None)], index=3, id='b3', unstructured=None, date='2021', title='Who needs MLOps: What data scientists seek to accomplish and how can MLOps help?', book_title='IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='IEEE', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='109-112', first_page='109', last_page='112', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Sridhar Alla', given_name='Sridhar', middle_name=None, surname='Alla', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Suman Kalyan', given_name='Suman', middle_name=None, surname='Kalyan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Adari', given_name='Adari', middle_name=None, surname=None, email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='2021', title='What is mlops?', book_title='Beginning MLOps with MLFlow', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Apress', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='79-124', first_page='79', last_page='124', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Matei Zaharia', given_name='Matei', middle_name=None, surname='Zaharia', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Andrew Chen', given_name='Andrew', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Aaron Davidson', given_name='Aaron', middle_name=None, surname='Davidson', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Ali Ghodsi', given_name='Ali', middle_name=None, surname='Ghodsi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Sue', given_name='Sue', middle_name=None, surname=None, email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Ann Hong', given_name='Ann', middle_name=None, surname='Hong', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Andy Konwinski', given_name='Andy', middle_name=None, surname='Konwinski', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Siddharth Murching', given_name='Siddharth', middle_name=None, surname='Murching', email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date='2018', title='Accelerating the machine learning lifecycle with MLflow', book_title=None, series_title=None, editors=None, journal='IEEE Data Eng. Bull', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='41', issue='4', pages='39-45', first_page='39', last_page='45', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Sridhar Alla', given_name='Sridhar', middle_name=None, surname='Alla', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Suman Kalyan', given_name='Suman', middle_name=None, surname='Kalyan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Adari', given_name='Adari', middle_name=None, surname=None, email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date='2021', title='What is mlops?', book_title='Beginning MLOps with MLFlow', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Apress', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='79-124', first_page='79', last_page='124', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Ricardo Bianchini', given_name='Ricardo', middle_name=None, surname='Bianchini', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Marcus Fontoura', given_name='Marcus', middle_name=None, surname='Fontoura', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Eli Cortez', given_name='Eli', middle_name=None, surname='Cortez', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Anand Bonde', given_name='Anand', middle_name=None, surname='Bonde', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Alexandre Muzio', given_name='Alexandre', middle_name=None, surname='Muzio', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Ana-Maria Constantin', given_name='Ana-Maria', middle_name=None, surname='Constantin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Thomas Moscibroda', given_name='Thomas', middle_name=None, surname='Moscibroda', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Gabriel Magalhaes', given_name='Gabriel', middle_name=None, surname='Magalhaes', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Girish Bablani', given_name='Girish', middle_name=None, surname='Bablani', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Mark Russinovich', given_name='Mark', middle_name=None, surname='Russinovich', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date='2020', title='Toward ml-centric cloud platforms', book_title=None, series_title=None, editors=None, journal='Communications of the ACM', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='63', issue='2', pages='50-59', first_page='50', last_page='59', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Yanling Zhao', given_name='Yanling', middle_name=None, surname='Zhao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Ye Li', given_name='Ye', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Xinchang Zhang', given_name='Xinchang', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Guanggang Geng', given_name='Guanggang', middle_name=None, surname='Geng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Wei Zhang', given_name='Wei', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Yanjie Sun', given_name='Yanjie', middle_name=None, surname='Sun', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date='2019', title='A survey of networking applications applying the software defined networking concept based on machine learning', book_title=None, series_title=None, editors=None, journal='IEEE Access', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='7', issue=None, pages='95397-95417', first_page='95397', last_page='95417', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None)], abstract='The explosion of data and its ever increasing complexity in the last few years, has made MLOps systems more prone to failure, and new tools need to be embedded in such systems to avoid such failure. In this demo, we will introduce crucial tools in the observability module of a MLOps system that target difficult issues like data drfit and model version control for optimum model selection. We believe integrating these features in our MLOps pipeline would go a long way in building a robust system immune to early stage ML system failures.', body='Introduction Machine Learning has been widely adopted by the industry in the last few years, leading to scalability, innovativeness, efficiency, and sustainability of multiple businesses. However, the complex evolving nature of data means there are new challenges to solve, with a majority of ML proof-ofconcept models failing when taken to production systems. One of the main reasons for these failures is lesser emphasis of coordinating the complex ML system components and infrastructure, in an automated way across many industrial applications.  [1, 2]  So, our efforts in building automatic, productive, and operationalized ML systems in production, is about tackling three main essential challenges. These challenges are preprocessing large amounts of data, setting up tracking and versioning for experiments and model training runs, and setting up observability in the deployment pipelines for models running in production. The need for solution to these challenges led to the evolution of MLOps, which is built from DevOps, Data Engineering, and Machine Learning. As an approach and tool for ML lifecycle management, there has been a few reports in the past,  [3, 4]  where end to end MLOps systems has been demonstrated. In this demo, we will present an improved end to end MLOps system, where novel tools developed for addressing issues such as data drift, concept drift and model version control, have been integrated into the system. We will also be discussing these issues in detail in our demo and explain why they are crucial problems worth solving in any MLOps system. \n System Design An MLOps system needs to be designed with an understanding that the system is the actual product, not the ML model and it components.  [5, 6, 7]  With this perspective, we have listed down the main components of an MLOps system in Fig.  1 , that includes an Orchestrator, User Interface (UI), Data Source, ML components, Data Version Control (DVC), Metric collectors, database (DB). We also introduce a new module on observability and enable notifications that work as an alert monitoring system. The MLOps Orchestrator, helps in streamlining and enforcing a robust architecture, and works towards ensuring high system availability. Having a well-designed ML user interface, that captures all aspects of the features developed in code is extremely crucial, as it helps in better explainability of what the system is doing and explain its unique features. Machine Learning Components in the system includes several features such as data preprocessing, model training and scoring, model inference, and these For ML systems in production, we often rely on cloud resources, components, and services, that provide log and metrics.  [8, 9]  The monitoring metrics provide an efficient mechanism to perform root cause analysis in troubleshooting production and development issues. Finally, we would be demonstrating an enhanced Observability module and we introduce two key features in this module: Model Experimentation System (MES) and a data validation module. \n Model Experimentation System (MES): We have added a unique model experimentation system, that will allow users to pick out the best versions of a particular model, and analyse which changes in the parameters made the performance better or worse. This feature is intended to help users recreate the results of a particular model version because a model lineage will be stored for each model version. The model lineage will also aid users in understanding how and why the different model versions were made. The MES will be useful during A/B/n testing, multivariate testing and statistical analysis of test results to determine which model version performs the best. It also offers transparency on the models itself and more details about its feature inputs and performance outputs. Data Validation module: Machine Learning models over time degenerate in their performance and accuracy, primarily due to a shift in the patterns of the data the model uses for it predictions. This shift in data patterns is known as Data Drift, which is important to identify at an early stage. Identification of Data Drift is the main feature of Data Validation module, that also performs basic schema level checks and high level feature wise checks. Drift detection module consists of five main sub-modules: services, data format identifier, summary generator, benchmark and Output Interpreter. All the sub-module work together towards a bigger goal of maintaining the model performance over time, and generating real-time alerts. Services sub module uses the configurable parameters in all other submodules, and updates the configurable parameters. Data format identifier uses the dataset and user config json file, and generates the categorisation of features based on user provided input thresholds. Summary generator sub-module generates summary for multiple types of features that includes for example number of records in the schema, and features related to numerical cardinality features amongst others. The benchmark sub-module compares the various metrics generated in the summary module, and monitors the relative change from baseline value to the overall percentage data drift accepted. Finally, the output interpreter submodule generates alerts and other insights for the benchmark sub-module, in a UI presentable format. \n Conclusion With an increased impetus on ML innovation coupled with data availability and analytical capabilities, the need for machine learning products working as a time tested system is extreme. The translation of this need into effect, however, needs a lot of integrated efforts for firstly progressing ML models into deployment and production, and then, more importantly maintaining them at a constant performance level in production. Machine Learning Operations (MLOps) is a promising solution on this front, and in this demo, we address some key challenges towards building a robust MLOps system. We demonstrate our entire system, and its key unique features, that should promote better monitoring and performance control of MLOps systems in production. Figure 1 : 1 Figure 1: Components of a designed MLOps system, with enhanced and novel observability module that includes crucial components such as data and concept drift monitoring, and model experimental tracking. components are the core of any standard MLOps platform. Data Version Control (DVC) is about managing data and ML experiments, and it leverages existing versioning and engineering toolset such as Git and CI/CD. While Git is used for storing and version code, DVC on similar lines stores the data and model files.', acknowledgement='The demo aims to build a common understanding of an MLOps system and its associated concepts, and hopefully should assist businesses and individuals in setting up nondegenerating MLOps systems, by using the novel features related to robust MLOps systems that we introduce.', annex=None)
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Gus Schrader', given_name='Gus', middle_name=None, surname='Schrader', email=None, orcid=None, affiliation=GrobidAffiliation(institution='NORTHWESTERN UNIVERSITY', department='DEPARTMENT OF MATHEMATICS', laboratory=None, address=None)), GrobidAuthor(full_name='Linhui Shen', given_name='Linhui', middle_name=None, surname='Shen', email=None, orcid=None, affiliation=GrobidAffiliation(institution='NORTHWESTERN UNIVERSITY', department='DEPARTMENT OF MATHEMATICS', laboratory=None, address=None)), GrobidAuthor(full_name='Eric Zaslow', given_name='Eric', middle_name=None, surname='Zaslow', email=None, orcid=None, affiliation=GrobidAffiliation(institution='NORTHWESTERN UNIVERSITY', department='DEPARTMENT OF MATHEMATICS', laboratory=None, address=None))], index=None, id=None, unstructured=None, date='2023-02-01', title='THE CHROMATIC LAGRANGIAN: WAVEFUNCTIONS AND OPEN GROMOV-WITTEN CONJECTURES', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2302.00159v1[math.RT]', pii=None, ark=None, istex_id=None, url=None), pdf_md5='9C6C003F9EA88E5323D133F70DCDFE31', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='M Aganagic', given_name='M', middle_name=None, surname='Aganagic', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Dijkgraaf', given_name='R', middle_name=None, surname='Dijkgraaf', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Klemm', given_name='A', middle_name=None, surname='Klemm', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Mariño', given_name='M', middle_name=None, surname='Mariño', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date='2005', title='Topological Strings and Integrable Hierarchies', book_title=None, series_title=None, editors=None, journal='Commun. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='261', issue=None, pages='451-516', first_page='451', last_page='516', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='hep-th/0312085', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Agaganagic', given_name='M', middle_name=None, surname='Agaganagic', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Ekholm', given_name='T', middle_name=None, surname='Ekholm', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Ng', given_name='L', middle_name=None, surname='Ng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='2014', title='Topological Strings, D-Model, and Knot Contact Homology', book_title=None, series_title=None, editors=None, journal='Adv. Theor. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='18', issue=None, pages='827-956', first_page='827', last_page='956', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Agaganagic', given_name='M', middle_name=None, surname='Agaganagic', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Klemm', given_name='A', middle_name=None, surname='Klemm', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date='2002', title='Disk Instantons, Mirror Symmetry and the Duality Web', book_title=None, series_title=None, editors=None, journal='Z. Naturforsch', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='57', issue=None, pages='1-28', first_page='1', last_page='28', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='hep-th/0105045', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Aganagic', given_name='M', middle_name=None, surname='Aganagic', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=3, id='b3', unstructured=None, date=None, title='Mirror symmetry, D-branes and counting holomorphic discs', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='hep-th/0012041', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Berenstein', given_name='A', middle_name=None, surname='Berenstein', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Zelevinsky', given_name='A', middle_name=None, surname='Zelevinsky', email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='2005', title='Quantum cluster algebras', book_title=None, series_title=None, editors=None, journal='Adv. Math', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='195', issue='2', pages='405-455', first_page='405', last_page='455', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Casals', given_name='R', middle_name=None, surname='Casals', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Zaslow', given_name='E', middle_name=None, surname='Zaslow', email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date=None, title='Legendrian Weaves: N -Graph Calculus, Flag Moduli and Applications', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='to appear in Geometry & Topology', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Cecotti', given_name='S', middle_name=None, surname='Cecotti', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Córdova', given_name='C', middle_name=None, surname='Córdova', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date=None, title='Braids, walls, and mirrors', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1110.2115', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C Córdova', given_name='C', middle_name=None, surname='Córdova', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Espahbodi', given_name='S', middle_name=None, surname='Espahbodi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Haghighat', given_name='B', middle_name=None, surname='Haghighat', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Rastogi', given_name='A', middle_name=None, surname='Rastogi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date=None, title='Tangles, generalized Reidemeister moves, and thre-dimensional mirror symmetry', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1211.3730', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Dimofte', given_name='T', middle_name=None, surname='Dimofte', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date=None, title='Quantum Riemann Surfaces in Chern-Simons Theory', book_title=None, series_title=None, editors=None, journal='Adv. Theor. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='17', issue=None, pages='479-599', first_page='479', last_page='599', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Dimofte', given_name='T', middle_name=None, surname='Dimofte', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Gabella', given_name='M', middle_name=None, surname='Gabella', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Goncharov', given_name='A', middle_name=None, surname='Goncharov', email=None, orcid=None, affiliation=None)], index=9, id='b9', unstructured=None, date='2016', title='K-Deompositions and 3d Gauge Theories', book_title=None, series_title=None, editors=None, journal='J. High Energ. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='2016', issue=None, pages='151', first_page=None, last_page=None, note=None, doi='10.1007/JHEP11(2016)151', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Dimofte', given_name='T', middle_name=None, surname='Dimofte', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Gaiotto', given_name='D', middle_name=None, surname='Gaiotto', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Gukov', given_name='S', middle_name=None, surname='Gukov', email=None, orcid=None, affiliation=None)], index=10, id='b10', unstructured=None, date='2014', title='Gauge Theories Labeled by Three-Manifolds', book_title=None, series_title=None, editors=None, journal='Commun. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='325', issue=None, pages='367-417', first_page='367', last_page='417', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1108.4389', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Dimofte', given_name='T', middle_name=None, surname='Dimofte', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Gukov', given_name='S', middle_name=None, surname='Gukov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Hollands', given_name='L', middle_name=None, surname='Hollands', email=None, orcid=None, affiliation=None)], index=11, id='b11', unstructured=None, date='2011', title='Vortex Counting and Lagrangian 3-manifolds', book_title=None, series_title=None, editors=None, journal='Lett. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='98', issue=None, pages='225-287', first_page='225', last_page='287', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1006.0977', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D.-E Diaconescu', given_name='D.-E', middle_name=None, surname='Diaconescu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Shende', given_name='V', middle_name=None, surname='Shende', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=12, id='b12', unstructured=None, date='2013', title='Large N duality, lagrangian cycles, and algebraic knots', book_title=None, series_title=None, editors=None, journal='Comm. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='319', issue='3', pages='813-863', first_page='813', last_page='863', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Derksen', given_name='H', middle_name=None, surname='Derksen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Weyman', given_name='J', middle_name=None, surname='Weyman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Zelevinsky', given_name='A', middle_name=None, surname='Zelevinsky', email=None, orcid=None, affiliation=None)], index=13, id='b13', unstructured=None, date=None, title='Quiver with potential and representations I: Mutations', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='0704.0649', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A I Efimov', given_name='A', middle_name='I', surname='Efimov', email=None, orcid=None, affiliation=None)], index=14, id='b14', unstructured=None, date='2012', title='Cohomological hall algebra of a symmetric quiver', book_title=None, series_title=None, editors=None, journal='Compositio Math', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='148', issue=None, pages='1136-1146', first_page='1136', last_page='1146', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Ekholm', given_name='T', middle_name=None, surname='Ekholm', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Kucharski', given_name='P', middle_name=None, surname='Kucharski', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Longhi', given_name='P', middle_name=None, surname='Longhi', email=None, orcid=None, affiliation=None)], index=15, id='b15', unstructured=None, date='2020', title='Physics and Geometry of Knots-Quivers Correspondence', book_title=None, series_title=None, editors=None, journal='Commun. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='379', issue=None, pages='361-415', first_page='361', last_page='415', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1811.03110', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Ekholm', given_name='T', middle_name=None, surname='Ekholm', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Shende', given_name='V', middle_name=None, surname='Shende', email=None, orcid=None, affiliation=None)], index=16, id='b16', unstructured=None, date=None, title='Colored HOMFLYPT counts holomorphic curves', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2101.00619', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Evens', given_name='S', middle_name=None, surname='Evens', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J.-H Lu', given_name='J.-H', middle_name=None, surname='Lu', email=None, orcid=None, affiliation=None)], index=17, id='b17', unstructured=None, date='2007', title='Poisson geometry of the Grothendieck-Springer resolutions of a complex semisimple group', book_title=None, series_title=None, editors=None, journal='Mosc. Math. J', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='7', issue='4', pages='613-642', first_page='613', last_page='642', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='math/0610123', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L D Faddeev', given_name='L', middle_name='D', surname='Faddeev', email=None, orcid=None, affiliation=None)], index=18, id='b18', unstructured=None, date='2008', title='Discrete series of representations for the modular double of the quantum group Uq(sl2)', book_title=None, series_title=None, editors=None, journal='Functional Analysis and Its Applications', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='42', issue='4', pages='330-335', first_page='330', last_page='335', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V V Fock', given_name='V', middle_name='V', surname='Fock', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A B Goncharov', given_name='A', middle_name='B', surname='Goncharov', email=None, orcid=None, affiliation=None)], index=19, id='b19', unstructured=None, date='2006', title='Moduli spaces of local systems and higher Teichmuller theory', book_title=None, series_title=None, editors=None, journal='Publ. Math. IHES, n', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='103', issue=None, pages='1-212', first_page='1', last_page='212', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='math/0311149', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V V Fock', given_name='V', middle_name='V', surname='Fock', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A B Goncharov', given_name='A', middle_name='B', surname='Goncharov', email=None, orcid=None, affiliation=None)], index=20, id='b20', unstructured=None, date=None, title='Cluster ensembles, quantization and the dilogarithm', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='math/0311245v7', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V V Fock', given_name='V', middle_name='V', surname='Fock', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A B Goncharov', given_name='A', middle_name='B', surname='Goncharov', email=None, orcid=None, affiliation=None)], index=21, id='b21', unstructured=None, date=None, title='Dual Teichmüller and lamination spaces', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='math/0510312v1', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L Faddeev', given_name='L', middle_name=None, surname='Faddeev', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Kashaev', given_name='R', middle_name=None, surname='Kashaev', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Volkov', given_name='A', middle_name=None, surname='Volkov', email=None, orcid=None, affiliation=None)], index=22, id='b22', unstructured=None, date='2001', title='Strongly Coupled Quantum Discrete Liouville Theory I: Algebraic Approach and Duality', book_title=None, series_title=None, editors=None, journal='Communications in Mathematical Physics', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='219', issue='1', pages='199-219', first_page='199', last_page='219', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='B Fang', given_name='B', middle_name=None, surname='Fang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C C M Liu', given_name='C', middle_name='C M', surname='Liu', email=None, orcid=None, affiliation=None)], index=23, id='b23', unstructured=None, date='2013', title='Open Gromov-Witten Invariants of Toric Calabi-Yau 3-Folds', book_title=None, series_title=None, editors=None, journal='Commun. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='323', issue=None, pages='285-328', first_page='285', last_page='328', note=None, doi='10.1007/s00220-013-1771-5.pdf', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Fomin', given_name='S', middle_name=None, surname='Fomin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Zelevinsky', given_name='A', middle_name=None, surname='Zelevinsky', email=None, orcid=None, affiliation=None)], index=24, id='b24', unstructured=None, date='2007', title='Cluster algebras IV: Coefficients', book_title=None, series_title=None, editors=None, journal='Compositio Mathematica', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='142', issue=None, pages='112-164', first_page='112', last_page='164', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W M Goldman', given_name='W', middle_name='M', surname='Goldman', email=None, orcid=None, affiliation=None)], index=25, id='b25', unstructured=None, date='1986', title='Invariant functions on Lie groups and Hamiltonian flows of surface group representations', book_title=None, series_title=None, editors=None, journal='Invent. Math', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='85', issue=None, pages='263-302', first_page='263', last_page='302', note=None, doi='10.1007/BF01389091.pdf', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A B Goncharov', given_name='A', middle_name='B', surname='Goncharov', email=None, orcid=None, affiliation=None)], index=26, id='b26', unstructured=None, date=None, title='Ideal webs, moduli spaces of local systems, and 3d Calabi-Yau categories', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1607.05228v1', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A B Goncharov', given_name='A', middle_name='B', surname='Goncharov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Kenyon', given_name='R', middle_name=None, surname='Kenyon', email=None, orcid=None, affiliation=None)], index=27, id='b27', unstructured=None, date=None, title='Dimers and cluster integrable systems', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1107.5588', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A B Goncharov', given_name='A', middle_name='B', surname='Goncharov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Shen', given_name='L', middle_name=None, surname='Shen', email=None, orcid=None, affiliation=None)], index=28, id='b28', unstructured=None, date='2018', title='Donaldson-Thomas transformations of moduli spaces of G-local systems', book_title=None, series_title=None, editors=None, journal='Adv. in Math', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='327', issue=None, pages='225-348', first_page='225', last_page='348', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1602.06479', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A B Goncharov', given_name='A', middle_name='B', surname='Goncharov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Shen', given_name='L', middle_name=None, surname='Shen', email=None, orcid=None, affiliation=None)], index=29, id='b29', unstructured=None, date=None, title='Quantum geometry of moduli spaces of local systems and representation theory', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1904.10491', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Grassi', given_name='A', middle_name=None, surname='Grassi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Hatsuda', given_name='Y', middle_name=None, surname='Hatsuda', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Mariño', given_name=None, middle_name=None, surname='Mariño', email=None, orcid=None, affiliation=None)], index=30, id='b30', unstructured=None, date='2016', title='Topological Strings from Quantum Mechanics', book_title=None, series_title=None, editors=None, journal='Ann. Henri Poincaré', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='17', issue=None, pages='3177-3235', first_page='3177', last_page='3235', note=None, doi='10.1007/s00023-016-0479-4.pdf', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Gukov', given_name='S', middle_name=None, surname='Gukov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Su Lkowski', given_name='P', middle_name=None, surname='Su Lkowski', email=None, orcid=None, affiliation=None)], index=31, id='b31', unstructured=None, date='2012', title='A-polynomial, B-model, and Quantization', book_title=None, series_title=None, editors=None, journal='JHEP', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='1202', issue=None, pages='70', first_page=None, last_page=None, note=None, doi='10.1007/JHEP02(2012)070', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Harvey', given_name='J', middle_name=None, surname='Harvey', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Moore', given_name='G', middle_name=None, surname='Moore', email=None, orcid=None, affiliation=None)], index=32, id='b32', unstructured=None, date='1996', title='Algebras, BPS States and Strings', book_title=None, series_title=None, editors=None, journal='Nucl. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='463', issue=None, pages='315-368', first_page='315', last_page='368', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Hausel', given_name='T', middle_name=None, surname='Hausel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Rodriguez Villegas', given_name='F', middle_name='Rodriguez', surname='Villegas', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='; N Katz', given_name=';', middle_name='N', surname='Katz', email=None, orcid=None, affiliation=None)], index=33, id='b33', unstructured=None, date='2008', title='Mixed Hodge polynomials of character varieties', book_title=None, series_title=None, editors=None, journal='Invent. Math', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='174', issue=None, pages='555-625', first_page='555', last_page='625', note='with an appendix', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Hausel', given_name='T', middle_name=None, surname='Hausel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Letellier', given_name='E', middle_name=None, surname='Letellier', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Rodriguez Villegas', given_name='F', middle_name='Rodriguez', surname='Villegas', email=None, orcid=None, affiliation=None)], index=34, id='b34', unstructured=None, date='2013', title='Positivity for Kac polynomials and DT-invariants of quivers', book_title=None, series_title=None, editors=None, journal='Annals of Math', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='177', issue=None, pages='1147-1168', first_page='1147', last_page='1168', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Hua', given_name='J', middle_name=None, surname='Hua', email=None, orcid=None, affiliation=None)], index=35, id='b35', unstructured=None, date=None, title='A refinement of Kac polynomials for quivers with enough loops', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2207.09839', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V Iacovino', given_name='V', middle_name=None, surname='Iacovino', email=None, orcid=None, affiliation=None)], index=36, id='b36', unstructured=None, date=None, title='Open Gromov-Witten Invariants and Boundary States', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1807.08786', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Iqbal', given_name='A', middle_name=None, surname='Iqbal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Kozçaz', given_name='C', middle_name=None, surname='Kozçaz', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=37, id='b37', unstructured=None, date='2009', title='The refined topological vertex', book_title=None, series_title=None, editors=None, journal='J. High Energ. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='10', issue=None, pages='69', first_page=None, last_page=None, note=None, doi='10.1088/1126-6708/2009/10/069/pdf', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='X Jin', given_name='X', middle_name=None, surname='Jin', email=None, orcid=None, affiliation=None)], index=38, id='b38', unstructured=None, date='2015', title='Holomorphic Branes Correspond to Perverse Sheaves', book_title=None, series_title=None, editors=None, journal='Geometry & Topology', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='19', issue=None, pages='1685-1735', first_page='1685', last_page='1735', note=None, doi='10.2140/gt.2015.19.1685.full', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Kashaev', given_name='R', middle_name=None, surname='Kashaev', email=None, orcid=None, affiliation=None)], index=39, id='b39', unstructured=None, date='2001', title='Integrable Structures of Exactly Solvable Two-Dimensional Models of Quantum Field Theory', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='211-221', first_page='211', last_page='221', note='The Quantum Dilogarithm and Dehn Twists in Quantum Teichmüller Theory', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Katz', given_name='S', middle_name=None, surname='Katz', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C.-C M Liu', given_name='C.-C', middle_name='M', surname='Liu', email=None, orcid=None, affiliation=None)], index=40, id='b40', unstructured=None, date='2001', title='Enumerative Geometry of Stable Maps with Lagrangian Boundary Conditions and Multiple Covers of the Disc', book_title=None, series_title=None, editors=None, journal='Adv. Theor. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='5', issue=None, pages='1-49', first_page='1', last_page='49', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='B Keller', given_name='B', middle_name=None, surname='Keller', email=None, orcid=None, affiliation=None)], index=41, id='b41', unstructured=None, date=None, title='Quiver mutation and combinatorial DT-invariants', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Rinat M Kashaev', given_name='Rinat', middle_name='M', surname='Kashaev', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Tomoki Nakanishi', given_name='Tomoki', middle_name=None, surname='Nakanishi', email=None, orcid=None, affiliation=None)], index=42, id='b42', unstructured=None, date='2011', title='Classical and quantum dilogarithm identities', book_title=None, series_title=None, editors=None, journal='SIGMA. Symmetry, Integrability and Geometry: Methods and Applications', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='7', issue=None, pages='102', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Soibelman Kontsevich', given_name='Soibelman', middle_name=None, surname='Kontsevich', email=None, orcid=None, affiliation=None)], index=43, id='b43', unstructured=None, date='2011', title='Cohomological Hall algebra, exponential Hodge structures and motivic Donaldson-Thomas invariants', book_title=None, series_title=None, editors=None, journal='CNTP', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='5', issue=None, pages='231-352', first_page='231', last_page='352', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Kucharski', given_name='P', middle_name=None, surname='Kucharski', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Reineke', given_name='M', middle_name=None, surname='Reineke', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Stošic', given_name='M', middle_name=None, surname='Stošic', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Su Lkowski', given_name='P', middle_name=None, surname='Su Lkowski', email=None, orcid=None, affiliation=None)], index=44, id='b44', unstructured=None, date='2019', title='Knots-quivers correspondence', book_title=None, series_title=None, editors=None, journal='Adv. Theor. Math. Phys. bf', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='23', issue=None, pages='1849-1902', first_page='1849', last_page='1902', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1707.04017', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Labastida', given_name='J', middle_name=None, surname='Labastida', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Mariño', given_name='M', middle_name=None, surname='Mariño', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=45, id='b45', unstructured=None, date='2000', title='Knots, links and branes at large N', book_title=None, series_title=None, editors=None, journal='J. High Energ. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='11', first_page=None, last_page=None, note=None, doi='10.1088/1126-6708/2000/11/007/pdf', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='I Le', given_name='I', middle_name=None, surname='Le', email=None, orcid=None, affiliation=None)], index=46, id='b46', unstructured=None, date='2019', title='C luster structures on higher Teichmuller spaces for classical groups', book_title=None, series_title=None, editors=None, journal='Forum of Math, Sigma', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='7', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W Luo', given_name='W', middle_name=None, surname='Luo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Zhu', given_name='S', middle_name=None, surname='Zhu', email=None, orcid=None, affiliation=None)], index=47, id='b47', unstructured=None, date=None, title='Integrality structures in topological strings I: framed unknot', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1611.06506', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='David Nadler', given_name='David', middle_name=None, surname='Nadler', email=None, orcid=None, affiliation=None)], index=48, id='b48', unstructured=None, date='2009', title='Microlocal branes are constructible sheaves', book_title=None, series_title=None, editors=None, journal='Selecta Math. (N.S.)', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='15', issue=None, pages='563-619', first_page='563', last_page='619', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='math/0612399', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='David Nadler', given_name='David', middle_name=None, surname='Nadler', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Eric Zaslow', given_name='Eric', middle_name=None, surname='Zaslow', email=None, orcid=None, affiliation=None)], index=49, id='b49', unstructured=None, date='2009', title='Constructible Sheaves and the Fukaya Category', book_title=None, series_title=None, editors=None, journal='J. Amer. Math. Soc', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='22', issue=None, pages='233-286', first_page='233', last_page='286', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Nakatsu', given_name='T', middle_name=None, surname='Nakatsu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Takasaki', given_name='K', middle_name=None, surname='Takasaki', email=None, orcid=None, affiliation=None)], index=50, id='b50', unstructured=None, date='2016', title='Open string amplitudes of closed topological vertex', book_title=None, series_title=None, editors=None, journal='J. Phys. A: Math. Theor', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='49', issue=None, pages='25201', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1507.07053', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Ooguri', given_name='H', middle_name=None, surname='Ooguri', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Vafa', given_name='C', middle_name=None, surname='Vafa', email=None, orcid=None, affiliation=None)], index=51, id='b51', unstructured=None, date='2000', title='Knot invariants and topological strings', book_title=None, series_title=None, editors=None, journal='Nucl. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='577', issue=None, pages='419-438', first_page='419', last_page='438', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='hep-th/9912123', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='F Rodriguez Villegas', given_name='F', middle_name='Rodriguez', surname='Villegas', email=None, orcid=None, affiliation=None)], index=52, id='b52', unstructured=None, date=None, title='A refinement of the A-polynomial of quivers', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1102.5308', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L Shen', given_name='L', middle_name=None, surname='Shen', email=None, orcid=None, affiliation=None)], index=53, id='b53', unstructured=None, date='2022-09', title='Duals of Semisimple Poisson-Lie Groups and Cluster Theory of Moduli Spaces of G-local Systems', book_title=None, series_title=None, editors=None, journal='International Mathematics Research Notices', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue='18', pages='14295-14318', first_page='14295', last_page='14318', note=None, doi='10.1093/imrn/rnab094', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Solomon', given_name='J', middle_name=None, surname='Solomon', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Tukachinsky', given_name='S', middle_name=None, surname='Tukachinsky', email=None, orcid=None, affiliation=None)], index=54, id='b54', unstructured=None, date='2021', title='Point-like Bounding Chains in Open Gromov-Witten Theory', book_title=None, series_title=None, editors=None, journal='Geom. Funct. Anal', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='31', issue=None, pages='1245-1320', first_page='1245', last_page='1320', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1608.02495', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='V Shende', given_name='V', middle_name=None, surname='Shende', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Treumann', given_name='D', middle_name=None, surname='Treumann', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Williams', given_name='H', middle_name=None, surname='Williams', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Zaslow', given_name='E', middle_name=None, surname='Zaslow', email=None, orcid=None, affiliation=None)], index=55, id='b55', unstructured=None, date='2019', title='Cluster Varities from Legendrian Knots', book_title=None, series_title=None, editors=None, journal='Duke Math. J', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='168', issue=None, pages='2801-2871', first_page='2801', last_page='2871', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1512.08942', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Takasaki', given_name='K', middle_name=None, surname='Takasaki', email=None, orcid=None, affiliation=None)], index=56, id='b56', unstructured=None, date='2014', title='Remarks on partition functions of topological string theory on generalized conifolds', book_title=None, series_title=None, editors=None, journal='RIMS Kokyuroku No', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='1913', issue=None, pages='182-201', first_page='182', last_page='201', note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1301.4548', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Treumann', given_name='D', middle_name=None, surname='Treumann', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Zaslow', given_name='E', middle_name=None, surname='Zaslow', email=None, orcid=None, affiliation=None)], index=57, id='b57', unstructured=None, date='2018', title='Cubic Planar Graphs and Legendrian Surface Theory', book_title=None, series_title=None, editors=None, journal='Adv. Theor. Math. Phys', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='22', issue=None, pages='1289-1345', first_page='1289', last_page='1345', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Y Volkov', given_name='A', middle_name='Y', surname='Volkov', email=None, orcid=None, affiliation=None)], index=58, id='b58', unstructured=None, date='2005', title='Noncommutative hypergeometry', book_title=None, series_title=None, editors=None, journal='Communications in mathematical physics', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='258', issue='2', pages='257-273', first_page='257', last_page='273', note='Commun. Math. Phys.', doi=None, pmid=None, pmcid=None, arxiv_id='math/0312084', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Zagier', given_name='D', middle_name=None, surname='Zagier', email=None, orcid=None, affiliation=None)], index=59, id='b59', unstructured=None, date='2007', title='The Dilogarithm Function', book_title='Frontiers in Number Theory, Physics, and Geometry II', series_title=None, editors=[GrobidAuthor(full_name='P Cartier', given_name='P', middle_name=None, surname='Cartier', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Moussa', given_name='P', middle_name=None, surname='Moussa', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Julia', given_name='B', middle_name=None, surname='Julia', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Vanhove', given_name='P', middle_name=None, surname='Vanhove', email=None, orcid=None, affiliation=None)], journal=None, journal_abbrev=None, publisher='Springer', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='also available here', doi='10.1007/978-3-540-30308-4_1/fulltext.pdf', pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='E Zaslow', given_name='E', middle_name=None, surname='Zaslow', email=None, orcid=None, affiliation=None)], index=60, id='b60', unstructured=None, date=None, title='Wavefunctions for a Class of Branes in Threespace', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1803.02462', pii=None, ark=None, istex_id=None, url=None)], abstract='Inside a symplectic leaf of the cluster Poisson variety of Borel-decorated P GL2 local systems on a punctured surface is an isotropic subvariety we will call the chromatic Lagrangian. Local charts for the quantized cluster variety are quantum tori defined by cubic planar graphs, and can be put in standard form after some additional markings giving the notion of a framed seed. The mutation structure is encoded as a groupoid. The local description of the chromatic Lagrangian defines a wavefunction which, we conjecture, encodes open Gromov-Witten invariants of a Lagrangian threefold in threespace defined by the cubic graph and the other data of the framed seed. We also find a relationship we call framing duality: for a family of "canoe" graphs, wavefunctions for different framings encode DT invariants of symmetric quivers.', body='Introduction This paper exploits cluster theory to compute wavefunctions for Lagrangian branes in threespace and to make explicit conjectures about their all-genus open Gromov-Witten invariants. For certain branes, these numbers also relate to the cohomologies of twisted character varieties and Donaldson-Thomas invariants of quivers. Two structural tools in the schema are the behavior under mutation and the dependence of quantities on phases and framings. Let P be the symplectic cluster variety of Borel-decorated, P GL 2 local systems on a punctured sphere S with unipotent monodromy around the punctures. There is a Lagrangian subvariety M ⊂ P of decorated local systems with trivial monodromy at the punctures. Cluster charts P Γ of P are labeled by cubic graphs Γ on S, or dually ideal triangulations. They are algebraic tori, and can be identified with a torsor over rank-one local systems on a genus-g Legendrian surface S Γ in P M P Γ P Γ P Γ neck Ψ Γ neck ≡ 1 M Γ M Γ framed seed Figure  1 .0.1. The cluster variety of decorated P GL 2 -local systems P and the chromatic Lagrangian M (in blue). Each framed seed (teal dot) identifies the chart P Γ with a quantum torus, in which the ideal M Γ is described by a cyclic vector or wavefunction, Ψ Γ . Arrows in the framed seed groupoid allow us to determine Ψ Γ from Ψ Γ . Any seed connected to the necklace graph Γ neck by admissible mutations has a computable wavefunction, conjectured to be the generating function of allgenus open Gromov-Witten invariants of the corresponding Lagrangian. the five-sphere: after choosing a base point, we can write the chart as P Γ ∼ = H 1 (S Γ ; C * ) ∼ = (C * ) 2g . Then M Γ := M ∩ P is a subspace of a torus closely related to the space of graph colorings of Γ, so we call M the chromatic Lagrangian. The explicit description of M Γ will lead to enumerative predictions, but will also depend on further choices: a phase, a framing and a cone. Central to the strategy for calculation is to understand the effects of a mutation Γ Γ , which is dual to a flip of a triangulation, and to understand its interaction with phases, framings and cones. The whole story has a quantization, conjecturally related to higher-genus open Gromov-Witten invariants. The entire structure is captured by the framed seed groupoid, an enhancement of the cluster groupoid, whose arrows are either mutations or changes of the various decorations -see Figure  1 .0.1. 1.1. Framed Seed Groupoid. In a bit more detail, the edge lattice Λ := Z E Γ of a cubic graph on an oriented surface (for us, the sphere) has a natural skew form ( * , * ) defined from the cyclic structure on edges meeting at a vertex. Quotienting by its kernel Λ c defines a symplectic lattice Λ. Roughly, a framed seed is an identification of this lattice with the standard symplectic lattice Z g ⊕ Z g . More formally, it is a tuple (i, K, t, f ), where i is a cluster seed (i.e. a basis for a lattice Λ equipped with an integral skew form), K ⊂ Λ is a maximal isotropic sublattice, t : Λ → Z is a character of Λ, and f = (σ, {a i }) is a pair of a splitting σ : K ∨ → Λ of 0 → K → Λ → K ∨ → 0 together with a basis {a i } of K ∨ . Note we have K ∨ ∼ = Λ/K via the symplectic structure on Λ, and we thereby obtain a dual basis {b i }. In total, the data of the framed seed provides an identification of Λ with the standard symplectic lattice Z g ⊗ Z g with symplectic form i du i ∧ dv i . We are interested in the set-up detailed in  [TZ] , i.e. the construction of a Legendrian surface S Γ ⊂ S 5 from the data of Γ ⊂ S, and a singular exact Lagrangian filling L 0 of S Γ as defined by an ideal foam, F, the combinatorial dual of a tetrahedronization of a ball. A smoothing L can be defined by studying the local model of the Harvey-Lawson special Lagrangian smoothing of the singular Harvey-Lawson cone, and amounts to local choice of one of the three possible facematchings at each tetrahedron. This geometry gives rise to a framed seed as follows: the group H 1 (S Γ , Z) is identified with Λ, with its intersection form, and K is the kernel of the homology push-forward of inclusion of the boundary S Γ → L. Then {a i } is a basis for H 1 (L); the dual basis {b i } for H 1 (L) and these give coordinates U i and V j for H 1 (L, C * ) ∼ = P Γ , respectively. The quantization then leads to an isomorphism of P Γ with the quantum torus V i U j = q 2δ i,j U j V i . Each edge is then labeled by a monomial X e in the U i and V j with q-dependent coefficient. 1.2. Wavefunctions. After quantization in each chart P Γ , the Lagrangian subvariety M Γ ⊂ P Γ becomes a left ideal I Γ , and we can identify the left P Γ -module P Γ /I Γ with the principal ideal defined by cyclic vector Ψ Γ ∈ C[[{X i }]], satisfying I Γ Ψ Γ = 0 in the standard representation defined by exponentiating the Weyl representation: (U i • f )(X) = X i f (x), (V i • f )(X) = f (q 2 X i ), where q = e πi 2 . The generators for I Γ are relations determined by the faces of Γ, giving us concrete q-difference equations for Ψ Γ . For example, in the case where Γ is the tetrahedron graph, S Γ is a genus-one surface and the quantum torus has generators U and V obeying V U = q 2 U V . For a certain choice of framed seed (see Figure  6 .2.1 and Lemma 6.3), the face equations are all equivalent to (1 − U − V )Ψ = 0, and the unique power-series solution is Ψ = Φ(−q −1 X), where Φ(x) = n≥0 (1 + q 2n+1 x) −1 is a quantum dilogarithm. The equations for I Γ are compatible with mutations Γ Γ , meaning generators of I Γ are related to generators of I Γ by a cluster coordinate transformation, and these are effected (up to a known basis change) by conjugation by a quantum dilogarithm. The upshot is that graph mutations change the wavefunction by the action of the quantum dilogarithm, and as long as can make sense of this action on the ring of power series, we may compute the resulting wavefunction. We call such mutations admissible. What is more, we can effect changes of other aspects of a framed seed (phase, framing, basis) by known operators, as well. Moreover, the necklace graph Γ neck (see Figure  1 .5.1) is a distinguished base point for the framed seed groupoid, with known wavefunction Ψ Γ neck ≡ 1. So we can find any wavefunction for any point on the framed seed groupoid connected to this basepoint by an admissible path. One must check that the resulting wavefunction is independent of path, and this amounts to checking that the cluster modular group (the automorphisms of the standard quantum torus determined by loops in the groupoid) acts trivially on the necklace wavefunction. This can be verified explicitly by observing that the necklace wavefunction is uniquely determined by the defining equations for the ideal. In this way, the cluster structure of the cluster modular groupoid can be exploited to find wavefunctions. Some have conjectural interpretations. 1.3. Open Gromov-Witten Conjectures. The cubic planar graphs Γ that label cluster charts P Γ also describe Legendrian surfaces S Γ , which form asymptotic boundary conditions for categories of A-branes, by which we mean categories of constructible sheaves with singular support on S Γ  [N, NZ] . Non-exact Lagrangian fillings L ⊂ C 3 asymptotic to S Γ have open Gromov-Witten invariants which we conjecture, following the pioneering work of Aganagic-Vafa  [AV] , are predicted by the geometry of the brane moduli space M Γ ⊂ P Γ . The classical geometry conjecturally leads to open Gromov-Witten invariants. The subvariety M Γ ⊂ P Γ is Lagrangian. Choosing a framed seed A and lifting to the universal cover, we get M Γ ⊂ C 2g , and any connected component determines a potential W Γ so that M Γ is the graph of dW Γ . The instanton part of W Γ is conjectured to be the open Gromov-Witten generating function. \n Conjecture: W (A) Γ is the generating function of disk invariants and obeys Ooguri-Vafa integrality: W (A) Γ (X) = d∈Z ≥0 \\{0} n (A) d Li 2 (X d ), with n (A) d ∈ Z. This conjecture appeared in essentially the same form in  [TZ, Section 1.2] . The cluster variety P has a quantization, each chart P Γ of which can be identified, through a framing, with a quantum torus, D: V i U i = q 2 U i V i , where q = e iπ 2 . Then M Γ quantizes as an ideal I, and the left D-module D/ID is cyclic for a vector Ψ Γ . Thanks to general results of Kontsevich-Soibelman  [KS] , it follows that the wavefunctions Ψ Γ we construct satisfy the Ooguri-Vafa integrality property  [OV] : namely, they admit factorizations (1.3.1) Ψ (A) Γ = d∈Z g ≥0 k∈Z Φ((−q) k X d ) n (A) d,k , where n The conjecture implies the one above from  [TZ]  since Ψ (A) Γ ∼ e −W (A)  Γ /gs and Φ(x) ∼ e −Li 2 (x)/gs as the string coupling constant g s = 2πi 2 tends to 0, and then n (A) d = k n (A) d,k . Since all ideal triangulations are related by flips, every cubic planar graph of genus g (meaning it has 2g + 2 vertices) can be obtained from Γ neck g through a sequence of mutations. Our rubric therefore leads to conjectures for Lagrangian fillings for many Legendrian surfaces. Remark 1.1. As explained in  [FG2] , the symplectic form on P arises as the image under the regulator map f ∧ g → dlog(f ) ∧ dlog(g) of a canonical element W ∈ K 2 (Q(P)) in the Milnor K 2group of the field of rational functions on P. In  [DGGo]  it is shown that the chromatic Lagrangian M is in fact a K 2 -Lagrangian: the image of the K-theory class W under the restriction map K 2 (Q(P)) → K 2 (Q(M )) vanishes. In fact, this K 2 Lagrangianicity of M is formally implied by the Ooguri-Vafa integrality (1.3.1) of the wavefunction. 1 1.4. Analytic Aspects. A quantization in the physical sense would require that we construct, in addition to wavefunctions for each seed of the cluster modular groupoid, a Hilbert space with arrows acting by unitary isomorphisms. Fock and Goncharov constructed such a quantization depending on a parameter ∈ R, a central character for the kernel of the skew form, with reality being crucial for each logarithmic cluster variable x to act in a unitary way, and for mutations to be effected by a unitary action of the Faddeev (noncompact quantum) dilogarithm ϕ(x). Such an approach cannot work for us, as the unipotency condition defining our cluster variety requires the central character to act as an imaginary number, ruling out self-adjointness in the naïve sense. Nevertheless, in Section 7 we present what we think of as good evidence for the existence of a quantization in the analytic sense, and for a well-defined wavefunction at each seed. Solutions are symmetric in ↔ −1 , reflecting the symmetry of the "squashed three-sphere" in the physical set-up (see, e.g.,  [CEHRV, Equation (2.16 )]). In this set-up, all seed arrows would be admissible. For example, mutating at all three strands of the genus-two necklace graph Γ 2 neck would not be admissible in the algebraic set-up of Section 1.2, but leads to an analytic wavefunction. Indeed, in Section 7 we show in this and several other cases that different paths to the same framed seed lead to the same wavefunction. The identities needed to establish this path-independence (e.g. (7.1)) are consequences of the analytic properties of the Faddeev dilogarithm and its Fourier self-duality. As an illustration of the analytic set-up, in Section 7.2 we show how it reproduces the all-genus analog of the proposal in  [TZ]  for the superpotential associated to the g = 3 cubic graph given by the 1-skeleton of the cube. 1.5. Framing Duality. We notice a curious identity between wavefunctions and quiver invariants. A special role is played by the Legendrian Clifford torus and its higher-genus generalizations. These Clifford surfaces of genus g arise from "canoe" graphs (see Figure  1 .5.1). The Clifford surfaces 1 For example, locally in a cluster chart in the two-dimensional case, the regulator map sends X ∧Y to dlogX ∧dlogY.  We can now state framing duality in the following way. Let A be a g×g symmetric, integral matrix with non-negative entries. Let Q A be the symmetric quiver with g nodes and adjacency matrix A. Recall that the DT series is the generating function for cohomologies of quiver representation spaces M d (see Section 8.2 for a precise definition) over different dimension vectors, d. Explicitly, \n Ooguri-Vafa integrality of the potential W = d n (A) d Li2(X d ), says that Y = d (1 − X d ) −dn (A) d . To see that this implies K2 Lagrangianicity, note X d ∧ (1 − X d ) DT A = d∈Z ≥0 s∈Z (−1) k H s (M d )X d t k/2 . Then we have, after setting t 1 2 = −q: (1.5.1) The wavefunction is the DT series of Q A : Ψ (A) Γ g canoe = DT A . Further, explicit formulas show that the invariants n (A) d relate to refined Kac polynomials of quivers, as defined in  [RV] . Recall that the Kac polynomial A d (q) of a quiver Q counts the number of isomorphism classes of absolutely indecomposable representations of Q. The refined Kac polynomials A λ (q) are labeled by partitions λ, and satisfy A d (q) = |λ|=d A λ (q). A special role will be played by λ = 1 d = (1, 1, ..., 1). In Proposition 8.5, we show that when Q is the quiver with one node and h ≥ 1 arrows, and if A is the one-by-one matrix (2 − 2h), then A 1 d (1) = n (A) d . Remark 1.2. Many of the results which establish this equality were performed by Kontsevich-Soibelman in  [KS] . For the genus-one case studied by Aganagic and Vafa, the connection between DT invariants and open GW invariants in different framings was observed also in  [LZ] . As for other Legendrians, also in genus-one, wavefunctions for knot and link conormals were considered in  [AENV] . Finding quiver duals for knot conormals is known as the Knot-Quiver Correspondence  [KRSS] . The relationship (1.5.1) suggests that the quiver invariants arise from an effective quiver quantum-mechanical theory described by the capping data for the noncompact threefolds we construct from Harvey-Lawson components -see, e.g.  [CEHRV, Section 5.1.1] . Framing duality is thus in the spirit as the knots-quiver correspondence of  [KRSS] , whose geometric and physical interpretations were proposed in  [EKL] . It is however more general, in the following sense. The Legendrian surfaces considered here are higher genus and not tori, giving rise to all symmetric quivers and DT invariants depending on all g variables. In contrast, framings of a fixed knot are labeled by a single integer, corresponding to a one-parameter set of quivers, with DT invariants determined by specializing the g variables to a one-dimensional slice -see  [KRSS, Equation (4.2) ]. It would be interesting to pursue a geometric interpretation of framing duality along the lines of  [EKL] . Remark 1.3. One wonders if the above relations extend to other cubic graphs and/or nonsymmetric quivers. 1.6. Seminal Prior Works. Very similar constructions were considered from related physical perspectives in prior works. In  [CEHRV]  and  [DGGo]  the authors consider an M5-brane on S 3 × L, where L is a Lagrangian submanifold of a compactifying space. (Those authors call this Lagrangian M .) They describe the partition function of the effective 3d theory on S 3 as a quantum-mechanical state. The M-theory set-up expresses this partition function as an integer combination of dilogarithms. The partition function can also be computed by reduction to L. It is a general property of quantum field theory that the path integral on a manifold with boundary always defines a state in the Hilbert space defined by the boundary. In the present case, the boundary Hilbert space is a quantization of the space of flat U(1) connections on the genus-g Legendrian boundary surface S Γ (or a torsor over such -see Section 4.1). The wavefunction Ψ should be understood as the wavefunction of this quantum state. On top of all this, many of the results of this paper have also appeared in important previous works, to which we owe a debt of gratitude. In  [CCV]  and  [CEHRV] , the authors studied the behavior of these wavefunctions under symplectic transformations, although not via cluster theory and without relating the results to Gromov-Witten invariants. The papers  [DGGo]  and  [DGGu]  overlap with the present paper, as well as  [CEHRV] , in considering Lagrangian double covers branched over tangles, and studied the corresponding Lagrangian moduli space. The paper  [KS]  studied quiver representations and preservation of integrality under changes of framings, providing many of the key formulas that we use. The idea of quantizing mirror curves goes back to  [ADKMV]  and has been integral to the spectral approach of  [GHM] , applications to knot polynomials in  [GS] , and difference equations for partition functions in  [T, NT] . Finally, open Gromov-Witten conjectures appeared previously in  [TZ]  and  [Za] , as well as in  [ES] . Further citations are made in the text. Acknowledgements. We dedicate this paper to Steve Zelditch, our late colleague and friend. A generous giant of a mathematician, Steve clarified several analytical and representation-theoretic issues we confronted in preparing this paper. We are greatly endebted to David Treumann, who was involved in a significant part of this collaboration. It is a pleasure to thank Roger Casals, Lenny Ng, Piotr Su lkowski and Boris Tsygan for helpful conversations. We thank Peng Zhou for asking about mutations very early in this project. L.S. has been supported by NSF grant DMS-2200738. E.Z. has been supported by NSF grants DMS-1406024, DMS-1708503 and DMS-2104087. \n Cluster Poisson Varieties and Quantizations For the convenience of the reader, we briefly recall the needed background on cluster Poisson varieties and their quantizations. Within this paper, we focus on the cluster Poisson varieties that are skew-symmetric and without frozen variables. A more general definition of cluster Poisson varieties can be found in  [FG2] . \n Cluster Poisson varieties. Definition 2.1. A seed is a pair i = ({x 1 , . . . , x n }, W ), where {x 1 , . . . , x n } is a collection of commuting algebraically independent variables, and W = i,j a ij x i ∂ ∂x i ∧ x j ∂ ∂x j is a bi-vector encoded by an integer skew-symmetric matrix A = (a ij ). Correspondingly, we get a quiver Q A such that its vertices are labelled by 1 through n and the number of arrows from i to j is [a ij ] + := max{0, a ij }. Let i be a seed. Every k ∈ {1, . . . , n} creates a new seed µ k (i) = ({x 1 , . . . , x n }, W ) such that x i = \uf8f1 \uf8f2 \uf8f3 x −1 k if i = k, x i (1 + x −sgn(a ik ) k ) −a ik if i = k. In terms of {x i }, the bi-vector W can be presented as i,j a ij x i ∂ ∂x i ∧ x j ∂ ∂x j , where a ij = \uf8f1 \uf8f2 \uf8f3 −a ij if i = k or j = k; a ij + |a ik |a kj +a ik |a kj | 2 otherwise. The process of obtaining the new seed µ k (i) is called a cluster mutation in the direction k. The cluster mutation µ k in the same direction is involutive: µ 2  k (i) = i. Let σ be a permutation of {1, . . . , n}. It gives rise to a seed σ(i) = ({x 1 , . . . , x n }, W ) such that x i = x σ −1 (i) , i ∈ {1, . . . , n}. A composition τ = σ • µ i 1 • • • • • µ im of cluster mutations and permutations taking a seed i to i is called a cluster transformation. Definition 2.2. Let X be a rational variety over C equipped with a rational bi-vector W . A cluster chart of X is a birational map π = (x 1 , . . . , x n ) : X −→ C n such that i π := ({x 1 , . . . , x n }, π * (W ) ) forms a seed. Two cluster charts are called equivalent if their corresponding seeds are related by a cluster transformation. The equivalence class of a cluster chart π is denoted by |π|. Abusing notation 2 , a variety X equipped with a pair (|π|, W ) is called a cluster Poisson variety. Let C(X ) be the field of rational functions on X . For a cluster chart π = {x 1 , . . . , x n }, let T π = C[x ±1 1 , . . . , x ±1 n ] ⊂ C(X ) denote the ring of Laurent polynomials in x 1 , . . . , x n . The cluster Poisson algebra is the intersection (2.1.1) L X := π ∈|π| T π . Note that the bivector W induces a natural Poisson bracket on L X : {•, •} : L X × L X −→ L X , {f, g} := W (f, g). Let p be a birational automorphism of X . We say p is a cluster automorphism if • p preserves the bi-vector: p * (W ) = W , • p preserves the equivalence class of cluster charts: π • p ∈ |π|. The set of cluster automorphisms forms a group. Denote it by G X and call it the cluster modular group of X . The group G X acts by Poisson automorphisms on the algebra L X . 2.2. Quantization. Let X be a cluster Poisson variety. Let A = (a ij ) be the n × n integer skewsymmetric matrix appearing in an initial seed defining X as in Definition 2.1. To A is associated a triple (Λ, Π, ( * , * )), where Λ is a rank n lattice, Π = {e 1 , . . . , e n } ⊂ Λ is a basis, and ( * , * ) is a bilinear form on Λ such that (e i , e j ) = a ij . We also set Λ + = n i=1 Z ≥0 e i , Λ − = n i=1 Z ≤0 e i . Let C[q ±1 ] be the ring of Laurent polynomials in q. Let T q be the quantum torus algebra over C[q ±1 ] with the generators X v (v ∈ Λ), subject to the relations (2.2.1) X v X w = q (v,w) X v+w . Denote by Frac(T q ) the non commutative field of fractions of T q (cf.  [BZ, Appendix] ). The positive cone Λ + determines a formal completion of the algebra T q . We will consider the group of formal power series with leading term 1 R Π = v∈Λ + a v (q)X v a 0 (q) = 1, a v (q) ∈ C(q) . Now let us consider the mutations of the basis Π = {e 1 , . . . , e n }. Let Π * = {α 1 , . . . , α n } ⊂ Λ * be the dual basis of Π. Let k ∈ {1, . . . , n}. For an n-tuple S = {v 1 , . . . , v n } of elements in Λ, the mutated µ k (S) = {v 1 , . . . , v n } consists of elements (2.2.2) v i = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 −v k if i = k, v i + n l=1 max{0, (v i , v k )α l (v k )} • sgn(α l (v k ))e l if i = k. Remark 2.3. There is a slightly more general version of mutations, which we will consider in Section 3. Let (k 1 , . . . , k m ) be a sequence of indices in {1, . . . , n}. Let us start with the set Π = S. Applying the mutations (2.2.2) recursively, we obtain a sequence of bases of Λ (2.2.3) Π = Π 1 µ k 1 −→ Π 2 µ k 2 −→ . . . µ km −→ Π m+1 = Π , where Π j = {e (j) 1 , . . . , e (j) n }. A basis Π obtained this way is said to be equivalent to Π. Let |Π| consist of bases equivalent to Π. The elements e i in (2.2.3) are called c-vectors by Fomin-Zelevinsky  [FZ4] . The sign coherence of c-vectors asserts that each e (j) i lies either in Λ + or Λ −  [DWZ] . Hence there is a unique sequence of signs (ε 1 , . . . , ε m ) such that (2.2.4) f j = ε j e (j) k j ∈ Λ + , j = 1, . . . , m. We define the formal power series Φ Π = Φ(X f 1 ) ε 1 Φ(X f 2 ) ε 2 • • • Φ(X fm ) εm ∈ R Π . where we recall that Φ(X) = ∞ n=0 (1 + q 2n+1 X) −1 is the (compact) quantum dilogarithm function. The formal power series Φ(X) is a close relative of the infinite q-Pochhammer symbol (x; q 2 ) ∞ : = ∞ n=0 (1 − q 2n x) (2.2.5) = 1 + ∞ k=1 (−1) k q k(k−1) k i=1 (1 − q 2i ) x k = exp ∞ k=1 x k k(q 2k − 1) = Φ(−q −1 x) −1 ∈ Z((q))[[x]]. The latter is the unique formal power series starting from 1 and satisfying the difference relation (2.2.6) (x; q 2 ) ∞ = (1 − x) • (q 2 x; q 2 ) ∞ . For m ∈ Z, we define the finite q-Pochhammer symbol by (x; q 2 ) m := (x; q 2 ) ∞ (q 2m x; q 2 ) ∞ We have the following fundamental result, which guarantees that the series Φ Π is a well-defined function of the set Π : Theorem 2.4 (  [K, Th.4.1] ). The power series Φ Π only depends on the set Π , not on the mutation sequences that take Π to Π . Associated with each Π = {e 1 , . . . , e n } ∈ |Π| is a quantum torus algebra T q Π over C[q ±1 ] with generators X v = Ad Φ Π (X v ) ∈ Frac(T q ), v ∈ Λ. The generators X v satisfy the relations (2.2.1). In particular, the variables X e 1 , . . . , X e n are called quantized cluster X -variables. The pair (Π , T q Π ) is called a quantum cluster seed. The quantum cluster algebra is the intersection (2.2.7) L q X = Π ∈|Π| T q Π ⊂ Frac(T q ). The quasiclassical limit q → 1 of (2.2.7) recovers the Poisson algebra (2.1.1). The cluster modular group G X acts on L q X via quantum cluster automorphism, constructed as follows. Every element in G X one-to-one corresponds to a linear automorphism τ of the lattice Λ such that τ preserves the bilinear form on Λ and maps the initial basis set Π to Π := τ (Π) ∈ |Π|. Each τ gives rise to an algebra isomorphism g τ : T q Π ∼ −→ T q Π , X v −→ X τ −1 (v) . The restriction of g τ on L q X induces an algebra automorphism of L q X , called a quantum cluster automorphism. 2.3. Casimirs. The bilinear form ( * , * ) on Λ gives rise to a linear map c from Λ to its dual Λ * ∀v ∈ Λ, c(v)( * ) = (v, * ). The kernel of c forms a sub-lattice Λ c of Λ. The quotient Λ/Λ c is a symplectic lattice. If v ∈ Λ c , then X v commutes with every generator X w by (2.2.1). For every Π ∈ |Π|, we have X v = Ad Φ Π (X v ) = X v . Therefore X v (v ∈ Λ c ) are contained in L q X and are called Casimirs. It is easy to see that the center Z(L q X ) of L q X is the torus algebra generated by Casimirs. Definition 2.5. Let t be a homomorphism from Z(L q X ) to C[q ±1 ]. The quotient algebra L q X ,t of Z(L q X ) is obtained by modulo the relations X v = t(X v ), where v goes through Λ c . 2.4. Moduli space of G-local systems. Let G be a split semisimple algebraic group over Q with trivial center. Let S be an oriented compact topological surface with n many punctures p 1 , . . . , p n removed. Denote by χ(S) the Euler characteristic of S. We require that n > max{0, χ(S)} so that S admits a triangulation whose vertices are the punctures. The Fock-Goncharov moduli space X G,S , introduced in  [FG1] , provides an important class of cluster Poisson varieties. Below we briefly recall the definition and several basic properties of X G,S for later use. We start with a local model. The flag variety B parametrizes the Borel subgroups of G. Recall the Grothendieck-Springer resolution G := {(g, B) ∈ G × B | g ∈ B} . The projection from G to B makes G a smooth B-bundle over B. Let H be the Cartan subgroup of G. For each Borel subgroup B ∈ B, there is a canonical group homomorphism (2.4.1) π B : B −→ B/[B, B] ∼ −→ H. Consequently, we get a regular map π : G −→ H, (g, B) −→ π B (g). The variety G carries a Poisson structure such that π is a symplectic fiberation. For example, see  [EL]  for more details on the Poisson geometry of G. An element g ∈ G is unipotent if and only if π B (g) = 1. The subvariety N := π −1 (1) ⊂ G is the usual Springer resolution of the unipotent cone N ⊂ G. Note that N is naturally isomorphic to the cotangent bundle T * B. Therefore it admits a symplectic structure, although we caution the reader that this is not the same as the one determined by the cluster structure associated with the model of once-punctured disk. Its zero section consists of elements (1, B) for all B ∈ B, and is a Lagrangian subvariety of N . Now we generalize the above construction to the moduli space of G-local systems. Definition 2.6. A framed G-local system over S consists of the data (L, {B 1 , . . . , B n }) where • L ∈ Hom(π 1 (S), G) is a G-local system over S; • B i is a flat section of the associated bundle L × G B over the loop around the puncture p i . The moduli space X G,S consists of the framed G-local systems modulo the conjugation of G. Theorem 2.7. The space X G,S is a cluster Poisson variety. The mapping class group of S acts on X G,S via cluster Poisson transformations. Remark 2.8. The cluster Poisson structure on X G,S has been constructed by  Fock and Goncharov [FG1, §9]  for G = P GL r+1 , by Le  [Le]  for G being a classical group, and finally by Goncharov and the Shen  [GS2]  for an arbitrary semisimple group. Theorem 1 of  [S]  further shows that the ring of regular functions O(X G,S ) is a cluster Poisson algebra and therefore admits a quantization. Example 2.9. Let G = P GL 2 and let T be an ideal triangulation of S, i.e., a triangulation whose vertices are the punctures. For simplicity, we shall avoid self-folded triangles. We place a vertex at the center of every edge in T . Within each triangle in T , we add three arrows in the counter-clockwise orientation, as shown in Figure  2 .4.1. In this way, we obtain a quiver Q T . Note a d c b • x • t • u • w • v a d c b • x • t • u • w • v Figure 2.4.1. A cluster structure associated with X P GL 2 ,S that B P GL 2 = P 1 . Each framed local system in X P GL 2 ,S assigns a quadruple a, b, c, d ∈ P 1 to the vertices of each quadrilateral in T . We define the cluster variable placed on the diagonal of the quadrilateral to be the cross ratio x = − (a − b)(c − d) (b − c)(d − a) . In this way, we obtain a cluster seed i T for X P GL 2 ,S . As in Figure  2 .4.1, a flip of each edge gives rise to a cluster mutation, whose new variables become x = x −1 , t = t(1 + x), u = u(1 + x −1 ) −1 , v = v(1 + x), w = w(1 + x −1 ) −1 , and the rest variables are invariant. For general G, following (2.4.1), the flat section B i chosen for each puncture p i gives rise to a map from X G,S to the Cartan subgroup H. Therefore we get a map (2.4.2) π = (π 1 , . . . , π n ) : X G,S −→ H n . By Theorem 2.10 of  [GS2] , the fibers of π are symplectic varieties. In particular, for each simple positive root α of G, the regular function α • π i is a Casimir of X G,S . Let us set X un G,S := π −1 (1). We have dim X un G,S = dim X G,S − n dim H = 2n dim B − χ(S) dim G. \n 2.5. Example: the sphere cases. Within this subsection, we assume that S is a sphere with n punctures. As illustrated by Figure  2 .5.1, we have (2.5.1)  X G,S = ((u 1 , B 1 ), (u 2 , B 2 ), . . . , (u n , B n )) (u i , B i ) ∈ N , n i=1 u i = 1 G • B 1 • B 2 • B 3 • B 4 u 1 u 2 u 3 u 4 A r B r C r D r E r (r = 6, 7, 8) F 4 G 2 d r + 1 2 2 4 9 − r 1 1 . It is known that the order of Z(G sc ) is d. Proposition 2.10. Let S be a sphere with at least three punctures. The space X un G,S has d many top dimensional irreducible components. Proof. Every unipotent element u ∈ G sc has a unique lift to a unipotent element ũ ∈ G sc . Then the product condition in (2.5.1) becomes n i=1 ũi ∈ Z  (G) . Accordingly, we obtain a decomposition X un G,S = c∈Z(G) X un G,S (c), where X un G,S (c) consists of the points such that n i=1 ũi = c. Now we show that every X un G,S (c) contains a unique top dimensional irreducible component. Let D be a disk with n − 2 punctures and 2 marked points on its boundary. Following  [FG1, Definition 2.4 ], the moduli space A Gsc,D parametrizes the decorated twisted unipotent G-local systems on D. Each boundary interval of D corresponds to an invariant in the Cartan subgroup of G sc , denoted by h and h respectively as in the following figure. • 1 • 2 • n − 1 . . . • n h h u 1 u 2 u n−1 u n As constructed in  [GS2] , the space A Gsc,D carries a cluster K 2 structure, with 2rk(G) many frozen variables, given by ω i (h) and ω i (h ) respectively, where ω i are the fundamental weights of G sc . We impose an extra condition that h = 1 and h ∈ Z(G sc ), obtaining a subspace A Gsc,D ⊂ A Gsc,D . Depending on the value of h, we get a decomposition A Gsc,D = h∈Z(Gsc) A Gsc,D (h). Here every component A Gsc,D (h) is rational, with the usual cluster coordinates for the mutable ones, and a specialization on the frozen ones. Since h = 1 and h ∈ Z(G sc ), when passing from G sc to G, one may identify the pinnings given by the two boundary intervals, obtaining a map π : A Gsc,D −→ X un G,S . More precisely, recall the central element s Gsc as in Corollary 2.1 of  [FG1] . By comparing the geometric meanings of both spaces, we see that π maps A Gsc,D (s n Gsc • c) to X un G,S (c). Now we fix a simple path γ on the sphere S connecting the puncture 1 and n. Given a generic point in X un G,S (c), we may choose a decoration for each of the flags B 1 , ...B n−1 . Let us cut along the path γ, obtaining the disk D. Finally, we choose a decoration for B n such that h = 1. In this way, we obtain a lift of the generic point in X un G,S (c) to A Gsc,D (s n Gsc • c). Through the process, we see that the map π is dominant, with a fiber isomorphic to H n−1 sc for every generic point in X un G,S (c). As a consequence, we get the desired dimension dim X un G,S (c) = dim A Gsc,D − dim H n−1 sc = 2n dim B − 2 dim G. Now let i be a reflection of S that fixes the punctures. For example, if S is a sphere, then one can put all the punctures on the equator, and i exchanges the two hemispheres. Note that i changes the orientation of S. Therefore i induces an anti-Poisson involution of X G,S . Let s be the inverse map of H n which takes (h 1 , ..., h n ) to (h −1 1 , . . . , h −1 n ). By definition, the following maps commute X G,S H n π π i s X G,S H n Therefore i maps X un G,S to X un G,S . Taking all the fixed points of the map i, we get a subvariety M i of X un G,S . Theorem 2.11. M i is a Lagrangian subvariety of X un G,S . Proof. Let w be the symplectic form on X un G,S . Note that i * (w) = −w. Since i is the identity map on M i , the restriction of w to M i is trivial. It remains to check the dimension: dim M i = 1 2 dim X un G,S . Example 2.12. Let G = P GL 2 . The following triangulations show an example of involution for a sphere with 4 punctures. • • • • w x y z u v reflection • • • • w −1 x −1 y −1 z −1 u −1 v −1 cluster mutations • • • • τ (w) τ (x) τ (y) τ (z) v u Here τ (u) = v; τ (v) = u. τ (w) = w −1 (1 + v −1 )(1 + u −1 ); τ (y) = y −1 (1 + v −1 )(1 + u −1 ); τ (x) = x −1 (1 + v) −1 (1 + u) −1 ; τ (z) = z −1 (1 + v) −1 (1 + u) −1 ; Note that the mapping class group of punctured sphere acts on P by symplectomorphisms. The mapping class group preserves M , but it interchanges the other components of M i . In general, M in Theorem 4.1 is a connected component of M i . Therefore Theorem 4.1 is a special case of Theorem 2.11. \n Groupoids of polarized and framed seeds In this section we define the groupoid of framed seeds, an enhancement of the standard cluster modular groupoid that we shall use to describe concrete models for representations of the corresponding cluster variety. 3.1. Polarizations and framings for seeds. Suppose the rank of the skew-form ( * , * ) associated to the seed i is 2g, and write Λ c ⊂ Λ for its kernel. In what follows, we will write Λ := Λ/Λ c for the corresponding rank-2g symplectic lattice, which fits into the short exact sequence 0 → Λ c → Λ → Λ → 0. (3.1.1) A polarization for i is the choice of an isotropic sublattice K ⊂ Λ of maximal rank g, such that the skew form induces a short exact sequence of lattices 0 → K → Λ → K ∨ → 0. (3.1.2) We consider two polarized seeds (i, K) and (i , K ) to be equivalent if the canonical map Λ i → Λ i is an isometry which sends K to K . If (i, K) is a polarized seed and i is a seed related to i by a signed mutation or permutation, then the induced isomorphism of symplectic lattices ι : Λ i Λ i determines a polarization K = ι(K) for i . Our reason for introducing the additional data of polarizations is that they define representations of the symplectic torus T q Λ associated to the seed i. Indeed, a polarization K for i determines a commutative subalgebra T q K ⊂ T q Λ . The subalgebra T K is identified with the coordinate ring of a split algebraic torus of rank g, and let us write 1 K for its 1-dimensional representation given by evaluation at the identity element. From the latter we may construct an induced representation of T q Λ : V K : = Ind T q Λ T q K (1 K ) = T q Λ ⊗ T q K 1 K . The representation V K is a Z-module of infinite rank. In order to give a concrete model for it, it is necessary to equip the polarized seed (i, K) with another piece of additional data, which we now describe. Definition 3.1. A framing f for a polarized seed (i, K) is the following data: (1) a basis {a i } for K ∨ ; (2) a splitting s : K ∨ → Λ of the short exact sequence (3.1.2), such that the image of K ∨ in Λ is isotropic; and (3) a group homomorphism t : Λ → Z. Let us now reformulate the notion of a framing for a seed in concrete terms. Consider the standard quantum torus D 2g := Z[q ±1 ] U ± 1 , . . . , U ± n , V ± 1 , . . . , V ± n with the relations U i U j = U j U i , V i V j = V j V i , V i U j = q 2δ ij U j V i The choice of a framing f for a polarized seed (i, K) determines an isomorphism ι f : T q Λ −→ D 2g which is uniquely characterized by the requirement that the element X s(a i ) of T q Λ is mapped to the generator U i of D 2g . The generators V i then correspond under the inverse isomorphism to elements of the basis {b i } of K dual to the basis {a i } for K ∨ . Additionally, the data (3) of the homomorphism t in the definition of a framing determines a surjection of quantum tori T q Λ −→ T q Λ , X λ → (−q) t(λ) X λ+Λc which factors through the central quotient of T q Λ by the double sided ideal X z − (−q) t(z) |z ∈ Λ c . Putting everything together, we see that a framing f gives rise to a surjection of quantum tori ι f : T q Λ −→ D 2g , and that all the data of the framing and polarization can be uniquely recovered from that of the surjection ι f . Now let R := Z[q ± ][X ± 1 , . . . , X ± g ] be the ring of Laurent polynomials in g variables. Then there is a representation of D 2g on R such that (3.1.3) ∀F ∈ R, U i • F = X i F, V i • F = F (X 1 , . . . , q 2 X i , . . . , X n ), and we obtain an isomorphism of T q Λ -modules V K ι * f R, thus providing the promised model for the induced representation V K . A framed seed i is the data (i, K, f ) of a seed i together with a polarization K and framing f . We consider two framed seeds to be equivalent if the isomorphism of quantum tori T q Λ i → T q Λ i induced by canonical map of lattices Λ i → Λ i fits into a commutative diagram T q Λ i D 2g T q Λ i ι f a ι f 3.2. Operations on framed seeds. Suppose that seeds i, i are related by a signed mutation in direction k, so that we have an isometry of lattices ν ± k : Λ i → Λ i . If K, K and f , f are polarizations and framings for i, i , we say that the framed seeds (i, K, f ) and (i , K , f ) are related by the signed mutation in direction  ) , and similarly all pieces of framing data for f in Definition 3.1 are identified with those for f under the lattice isomorphism ν ± k . In particular, for any pair of framed seeds related by a signed mutation, there is a unique monomial map ν ± : Λ → Λ such that the following diagram commutes: k if K = (ν ± k ) * (K T q Λ i T q Λ i T q Λ i T q Λ i ν ± ν ± Recall that a framed seed i gives rise to a symplectic basis {s(a i ), b i } for Λ, where we again write b i for the elements of the basis for K dual to the basis {a i } for K ∨ . We say that two framed seeds are related by a framing change morphism if all pieces of the framing data are identical except for the datum (2) given by the splitting s of Λ. The space of framing change morphisms based at a given framed seed is naturally identified with the space of g × g symmetric integer matrices Ω = (ω ij ), where the new splitting s is related to the original by s (a i ) = s(a i ) + g j=1 ω ij b j , i = 1, . . . , g. Remark 3.2. We recall that if a i = C ij a j is another basis of K ∨ , then the corresponding dual basis is given by b i = (C −1 ) ji a j . Hence the symmetric matrix Ω transforms under such a change of basis C as Ω −→ CΩC T . Given a vector d = (d 1 , . . . , d g ) ∈ Z g , consider the algebra automorphism σ d of D 2g defined by σ d (U i ) = (−q) d i U i , σ d (V i ) = V i . (3.2.1) We say that two framed seeds with identical underlying lattice Λ are related by a coordinate rescaling if the surjections ι f , ι f : T q Λ −→ D 2g , are related by ι f = σ d • ι f for some d ∈ Z 2g . The framed seed groupoid is a category whose objects are equivalence classes of framed seeds. The arrows are generated by those of four elementary kinds: signed mutations, permutations, framing change morphisms, and coordinate rescalings. Each arrow a : (i, K, f ) → (i , K , f ) induces a birational automorphism of D 2g : those corresponding to permutations, changes of framing, and coordinate rescalings induce the natural biregular automorphisms, and a signed mutation in direction k induces a birational automorphism via the monomial isomorphism ν ± k and conjugation by Φ(ι f (X ±e k )) ± . We put a relation on the arrows in the framed seeds groupoid by identifying arrows with the same source and target which induce identical birational automorphisms of D 2g . \n 3.3. Framed seeds and representations. Suppose that i is a framed seed, and recall the corresponding representation V i Z[q ± ][X ± 1 , . . . , X ± g ] of the quantum torus T q Λ . The embedding of the Laurent series ring into the ring K := Z((q))((X 1 , . . . , X g )) of formal Laurent series also gives rise to a representation of T q Λ which we denote by V i . For the purposes of constructing wavefunctions, it will be necessary to consider the action of a somewhat larger algebra on the representation V i . Write D 2g for the \'complete quantum torus\' associated to D 2g , which may be regarded as the ring of non-commutative formal Laurent series in U i , V i . Inside D 2g , consider the subalgebra A 2g := Z((q))((U 1 , . . . , U g )) V ±1 1 , . . . V ±1 g consisting of formal Laurent series in the U i whose coefficients are Laurent polynomials in the V i . Unlike in the case of D 2g , there is a well-defined action of the algebra A 2g on V i . Indeed, under (3.1.3) each V i acts on the \'vacuum vector\' 1 ∈ V i by V i • 1 = 1, and so the action of a arbitrary Laurent polynomial in the V i , being a finite Z((q))-linear combination of such, is also well-defined. Recall that the space of change of framing morphisms based at a given framed seed can be identified with the additive group p g of g × g symmetric matrices Ω = (ω ij ) with ω ij ∈ Z. Its group algebra Zp g is generated by symbols T Ω , Ω ∈ p g satisfying T Ω T Ω = T Ω+Ω . The group p g acts on A 2g by automorphisms called changes of framing: (3.3.1) T Ω : A 2g ∼ −→ A 2g , V j −→ V j , U j −→ q ω jj U j g k=1 V ω jk k , and we may form the semi-direct product algebra A 2g = A 2g ⊗ Z Zp g . Given U w = j U w j j , it follows from (3.3.1) that we have T Ω (U w ) = q w t Ωw U w V Ωw . Remark 3.3. The reader may find the following interpretation of the framing shift automorphisms useful. Consider the topological Heisenberg algebra H g over C[[ ]] generated by {u j , v j } subject to the relations [u j , v k ] = δ j,k 2πi , and set q = e πi 2 . The algebra A 2g embeds into this Heisenberg algebra via U k → e 2π u k , V k → e 2π v k . Now given a g × g symmetric matrix Ω ∈ p g , consider the associated quadratic form Q(v) = g j,k=1 ω jk v j v k , and write e −πiQ (v) for the corresponding element of the group algebra Zp g . Note that the e −πiQ (v)  are not elements of the Heisenberg algebra H g , but one can nonetheless formally compute the result of conjugating the generators of H g by them using the Baker-Campbell-Hausdorff formula: Ad e −πiQ(v) (u j ) = u j − πi[Q(v), u j ] = u j + k ω jk v k , so that Ad e −πiQ(v) (U j ) = Ad e −πiQ(v) (e 2π u j ) = e 2π (u j + k ω jk v k ) = q ω jj e 2π u j e 2π k ω jk v k = q ω jj U j g k=1 V ω jk k , recovering (3.3.1). The extended algebra A 2g also acts in the representation V i K: given F = w C w (q)X w ∈ K, we define T Ω • F := w q w t Ωw C w (q)X w . (3.3.2) That (3.3.2) indeed defines a representation of the extended algebra A 2g follows easily from the considerations of Remark (3.3), or can be readily verified directly. Finally, let us remark that the coordinate-rescaling operators σ d defined in (3.2.1) also act naturally in the representation V i via σ d • F := w (−q) d•w C w (q)X w . (3.3.3) 3.4. Admissible and primitive mutations. Suppose that i is a framed seed, and e k is an element of the basis Π for Λ associated to the underlying seed i. Recall that the data of the framing f allows us to associate to ±e k a monomial ι f (X ± k ) ∈ D 2g of the form ι f (X ± k ) = (−q) r exp \uf8eb \uf8ed 2π g j=1 m j u j + n j v j \uf8f6 \uf8f8 , m j , n j , r ∈ Z, where we adopt the notations of Remark 3.3. We say that a mutation of the framed seed i in direction e k with sign is admissible if in the monomial ι f (X k ) we have m j ≥ 0 for all j = 1, . . . , g, and in addition there is at least one j for which m j = 0. Let us make a few simple remarks about this definition. Remark 3.4. If two framed seeds i, i are related by a change of framing, then evidently a signed mutation is admissible with respect to i if and only if it is admissible with respect to i . Remark 3.5. Let a be an admissible mutation of framed seed i in direction k with sign , and let i = a(i) be the resulting framed seed. Then the mutation of i in direction k with sign − , which is the inverse of a in the framed seed groupoid, is also an admissible mutation. It follows from these remarks there is a sub-groupoid of the framed seeds groupoid whose morphisms are generated by framing shifts and admissible mutations. Our reason for introducing the notion of admissibility of mutations is the following: a mutation of a framed seed in direction e k with sign is admissible (if and) only if the quantum dilogarithm formal power series Φ (ι f (X k )) is an element of the algebra A 2g . Suppose that a = (a 1 , . . . , a l ) is a morphism in the framed seed groupoid, i.e. a sequence of mutations, framing shifts and coordinate rescalings. Let us say that such a morphism is admissible if each signed mutation in the corresponding sequence is. Then to each admissible morphism we may associate an invertible element Φ a of the extended algebra A 2g . This element Φ a determines a birational automorphism of D 2g (by conjugation), along with an automorphism of K (via the representation (3.1.3), (3.3.2).) Lemma 3.6. Suppose that two chains of a 1 , a 2 of admissible mutations and framing shifts induce the same birational automorphism of D 2g . Then Φ a 1 = Φ a 2 . Proof. The Lemma is proved by the following standard argument, cf.  [KN] . If the Φ a i induce the same birational automorphism of D 2g , then the element Φ −1 a 1 Φ a 2 ∈ A 2g commutes with all generators U i , V i . An easy calculation shows that this implies that Φ −1 a 1 Φ a 2 must be an element of the ground ring Z((q)). But since each quantum dilogarithm corresponding to an admissible mutation is a formal power series in U i starting from 1, we see that Φ −1 a 1 Φ a 2 = 1, and the Lemma is proved. For the purposes of understanding the integrality properties of wavefunctions, we introduce the following strengthening of the notion of admissible mutations. Let us say that an admissible mutation in direction e k is primitive if in the monomial ι f (X k ) = (−q) r exp \uf8eb \uf8ed 2π g j=1 m j u j + n j v j \uf8f6 \uf8f8 , m j , n j , r ∈ Z the vector m = (m 1 , . . . , m g ) (3.4.1) is a primitive vector in Z g . \n The Chromatic Lagrangian We begin this section by reviewing the constructions and results of  [TZ] . 4.1. Cubic Planar Graphs and Fukaya Moduli. Let Γ ⊂ S 2 be a cubic planar graph. There is an integer g such that Γ has v = 2g + 2 vertices, e = 3g + 3 edges, and f = g + 3 faces. As in  [TZ] , one may associate the following objects to Γ. (1) A Legendrian surface S Γ ⊂ T ∞ R 3 ⊂ S 5 of genus g [TZ, Def. 2.1]. The surface S Γ is a branched double cover of S 2 , branched over the vertices of Γ. It is defined by its front projection, which is taken to be a two-sheeted cover of S 2 with crossing locus over the edges of Γ and looking like the following near vertices: (2) A period domain whenever e 1 , . . . , e n label the edges of a face of Γ. In these coordinates, the map M Γ → P Γ is given parametrically by the cross ratio (4.1.1). But it is also given by equations, as a complete intersection, in the following way. Let F be the set of faces of Γ. If e 1 , . . . , e n are the edges around a face f ∈ F taken counterclockwise, then the expression (4.1.4) P Γ ∼ = H 1 (S Γ , C * ), V f := 1 + x e 1 + x e 1 x e 2 + • • • + x e 1 • • • x e n−1 is independent of which edge is called e 1 . M Γ is cut out by the equations V f = 0, f ∈ F . Now let Γ denote the dual planar graph, with vertex set V ( Γ). Since Γ is cubic, Γ is a triangulation of S 2 , and we regard its g + 3 vertices as punctures on the sphere in the sense of Section 2.4. Now let S be a sphere with g + 3 punctures, and recall the corresponding moduli space X P GL 2 ,S of decorated P GL 2 local systems on S. Theorem 4.1. Let P be the symplectic subvariety of the cluster Poisson moduli space X G,S cut out by equations (4.1.3). There is a canonical algebraic Lagrangian subvariety M ⊂ P with the following property: for every cubic planar graph Γ with 2g + 2 vertices, there is a cluster chart P Γ ⊂ P such that the embedding M Γ → P Γ is isomorphic to M ∩ P Γ → P Γ . Proof. The canonical Lagrangian subvariety M is given by the subvariety of decorated local systems whose underlying local system is trivial. We show that the intersection of M with P Γ coincides with M Γ using the prescription for constructing a decorated local system corresponding to a point in a cluster torus described in  [FG1, Lemma 9.6 ]. We first note that the conditions (4.1.3) generate the defining ideal of the intersection of the unipotent subvariety X un G,S with the cluster chart corresponding to the triangulation Γ, and that the equation (4.1.2) cuts out the component of X un G,S containing all decorated local systems whose underlying local system is trivial. Recalling that the g + 3 punctures on the sphere correspond to the faces of Γ, a local calculation shows that the triviality of the holonomy of the underlying unipotent local system around such a puncture is equivalent to the vanishing of the corresponding expression (4.1.4). It remains to prove that M is Lagrangian. But since any decorated local system with trivial underlying local system is a fixed point of the involution i described in Section 2.5, the Lagrangianicity follows from Theorem 2.11 -see Remark 2.12. Because of item (3) above, we call M the chromatic Lagrangian. It was defined from a dual perspective in earlier work of Dimofte-Gabella-Goncharov  [DGGo] . \n Mutation and quantization. We define T Γ := Λ Γ ⊗ Z C * , a Poisson torus. It has a canonical quantization T q Γ , generated by coordinates X v , v ∈ Λ, with relations (4.2.1) X v X w = q (v,w) X v+w . Let i be a framed seed with underlying cubic graph Γ, and let Γ be the graph obtained from Γ by flipping a single edge e 0 . Then the positive and negative lattive mutation maps ν ± : Λ Γ → Λ Γ deliver isometries of edge lattices Λ Γ ∼ = Λ Γ , and so define framed seeds ν ± 0 (i). The corresponding isometries of lattices are illustrated below: (4.2.2) Γ X e0 X e1 X e4 X e2 X e3 ν + 0 o o Γ X −e0 X e1 X e4+e0 X e2+e0 X e3 X e0 X e1 X e4 X e2 X e3 ν − 0 o o X −e0 X e1+e0 X e4 X e2 X e3+e0 Also associated to each flip of triangulation is a cluster transformation, i.e. a birational map of tori T Γ T Γ . As explained in Section 2, these maps admit quantizations T q Γ T q Γ , which in our case take the form (4.2.3) X e0 X e1 X e4 X e2 X e3 µ 0 o o Γ X −e0 X e1 (1 + qX e0 ) X e4 (1 + qX −e0 ) −1 X e2 (1 + qX −e0 ) −1 X e3 (1 + qX e0 ) The map µ can be factored in one of two ways, corresponding to the choice of sign in the lattice isomorphism ν ± . Indeed, one easily verifies that the quantum cluster transformation µ k corresponding to the flip at edge k can be written as µ k = Ad Φ(Xe k ) • ν + k = Ad Φ(X −e k ) −1 • ν − k . Now consider a morphism in the framed seed groupoid represented by a sequence of n signed edge mutations a(k) : i → i : i = i 0 → k 1 i 1 → k 2 • • • → kn i n = i , where the jth mutation takes place at edge k j and has sign j . It gives rise to an isomorphism of quantum tori ν k : T q i → T q i given by ν k = ν 1 k 1 • • • • • ν n kn . Moreover, if we write M j for the image in T i of the quantum torus element X j e k j ∈ T i j−1 under the isomorphism ν 1 k 1 • • • • • ν j−1 k j−1 : T q i j−1 → T q i 0 , then we have µ k := µ k 1 • • • • • µ kn = Ad Φ(M 1 ) 1 • • • • • Ad Φ(Mn) n • ν k . Such a sequence of mutations of framed seeds gives rise to a birational automorphism µ D k := ι i • µ k • ι −1 i of D 2g , which evidently factors as µ D k = Ad Φ(ι 1 (M 1 )) 1 • • • • • Ad Φ(ιn(Mn)) n • ι * ν k , where we have set ι * ν k := ι 0 • ν k • ι −1 n . The reader may find it convenient to visualize the automorphism ι * ν k as follows. Recall that the data of a framing for a seed gives rise to a decoration of the edges of its cubic graph by monomials D 2g . Then the automorphism ι * ν k is characterized by the property that is maps the monomial sitting on edge e of Γ in framed seed i n to the monomial sitting on the corresponding edge of Γ in framed seed i 0 . Now let us suppose that each signed mutation in the sequence k is admissible, so that under the framing isomorphism ι j from i j the monomial M j is mapped to an element of the algebra A 2g . Then we may form the product Φ a(k) := Φ(ι n (M n )) − n • • • • • Φ(ι 1 (M 1 )) − 1 ∈ A 2g . Recall the representation K Z((q))((X 1 , . . . , X g )) of the algebra A 2g . The action of Φ a(k) defines an automorphism a(k) : K → K, f −→ Φ a(k) • f, and for all A ∈ D 2g , we have the following identity of operators on K: Φ a(k) • µ D k (A) = ι * ν k (A) • Φ a(k) . (4.2.4) In particular, if u ∈ T q i 0 and u ∈ T q i n are related by u = µ k (u ), then we have ι 0 (u) • Φ a = ι n (u ) • Φ a (4.2.5) as operators on K. The torus T Γ associated to a cubic graph Γ, or its quantization T q Γ , is the cluster chart P Γ of P, described in Sections 4.1. In the next section we show that the global Lagrangian submanifold M ⊂ P is compatible with this chart-wise quantization. \n 4.3. Quantizing the Chromatic Lagrangian. We begin by discussing the quantization of the relevant connected component of the moduli space of framed P GL 2 local systems with unipotent monodromy on the punctured sphere. Fix a cubic graph Γ of genus g, and as in the previous section let T q Γ be the associated quantum torus. Suppose that e 1 , . . . , e n are the edges around a face f of Γ, listed in counterclockwise cyclic order around the face; note that this means that each e i+1 precedes e i in the counterclockwise order with respect to their common vertex, so that we have X e i X e i+1 = q −2 X e i+1 X e i . Then the relation (4.1.3), which imposes unipotency of the monodromy around the puncture dual to corresponding f , is quantized as (4.3.1) X e 1 +...+en = q −2 . Note that the relation 4.3.1 can be equivalently formulated as X e 1 • • • X en = q −n . In order to pick out the required component, let s = e i ∈E e i ∈ Λ Γ be the sum of the edges. We then further impose the relation that (4.3.2) X s = (−q) g+3 . After quotienting by these relations, we obtain a symplectic quantum torus algebra T q Γ . We now proceed to the quantization of the additive face relations that are equivalent to the triviality of the underlying unipotent local system at a point of P. To this end, set R f = q −1 + X e 1 + X e 1 +e 2 + . . . + X e 1 +e 2 +•••e n−1 (4.3.3) = q −1 + X e 1 + qX e 1 X e 2 + q 2 X e 1 X e 2 X e 3 + • • • + q n−2 X e 1 X e 2 • • • X e n−1 . Remark 4.2. It follows from the multiplicative face relation (4.3.1) that multiplying (4.3.3) by qX en yields X en + X en+e 1 + X en+e 1 +e 2 + . . . + q −1 , so we see that the ideal in the quantum torus T q Γ generated by R f is independent of our arbitrary linearization of the cyclic order on the edges around the face f implicit in (4.3.3). \n Let I Γ be the left ideal in T q Γ generated by all (4.3.1) along the global relation (4.3.2) and the relations R f for all faces f . As the quantization of a Lagrangian subvariety, the D-module V Γ := D Γ /I Γ is holonomic. Now suppose that two regular cubic graphs Γ and Γ are related by mutation at edge e 0 . Let us write T Γ,Γ for the localization of the quantum torus T Γ at the Ore set {(1 + q 2k+1 X e 0 )} k∈Z , and similarly write T Γ ,Γ for the localization of T Γ at {(1 + q 2k+1 X e 0 )} k∈Z . Then the quantum mutation map µ 0 in (4.2.3) defines an isomorphism µ 0 : T Γ ,Γ → T Γ,Γ . Let us write I Γ,Γ for the ideal in T Γ ,Γ generated by the quantized chromatic ideal I Γ , and I Γ ,Γ for the ideal in T Γ,Γ generated by I Γ . Theorem 4.3. The system of quantized chromatic ideals {I Γ } is compatible with quantum cluster mutations: if Γ, Γ are regular cubic graphs related by a flip at edge e 0 as in Figure  4 .2.3, then we have µ 0 (I Γ ,Γ ) = I Γ,Γ . Proof. Consider the generator R f,Γ of I Γ ,Γ associated to the left face of the graph Γ in Figure  4 .2.3, as defined in (4.3.3). We show that it is mapped to the corresponding to a generator R f,Γ of I Γ,Γ under µ 0 . As explained in Remark 4.2, by multiplying R f,Γ by a unit in T Γ we may assume that the edge e 0 at which we mutate is neither e 1 nor e n−1 in the notations of (4.3.3). Then reading counterclockwise around the left face of the right graph in Figure  4 .2.3, we see that µ 0 (X e 2 + qX e 2 X e 0 + q 2 X e 2 X e 0 X e 1 ) = X e 2 (1 + qX −e 0 ) −1 + qX e 2 X −e 0 (1 + qX −e 0 ) −1 + q 2 X e 2 X −e 0 (1 + qX −e 0 ) −1 X e 1 (1 + qX e 0 ) = X e 2 + qX e 2 X e 1 , where we used that X e 0 X e 1 = q 2 X e 1 X e 0 by the relation (4.2.1) applied to the graph on the left of Figure  4 .2.3. From this computation, we see that µ 0 (R f,Γ ) = R f,Γ . The intertwining of the generators of the form (4.3.1) and (4.3.2) follows in exactly the same way. We now illustrate the constructions of this section in the following simple but fundamental example. Example 4.4. Consider the framed seed i 0 for the g = 1 necklace graph Γ 0 shown in Figure  4 .3.1. The additive face relation R = q −1 + X e 2 corresponding to its left bead is mapped under the corresponding framing isomorphism ι 0 : T q Γ 0 → D 2 to the element ι 0 (R) = q −1 (1 − V ). Let us now perform a positive mutation at the edge e 3 of Γ 0 to obtain the framed seed for the canoe graph Γ 1 shown in Figure  4 .3.2. Then we see that R = µ 3 R , R = q −1 + X e 2 + X e 2 +e 3 , where R is the additive face relation associated to the face of Γ bounded by (e 1 , e 2 , e 3 ). Under the new framing isomorphism ι 1 : T q Γ 1 → D 2 , the element R is mapped to (4.3.4) ι 1 (R ) = q −1 + q −1 U V − q −1 V. The element Φ µ + 3 ∈ A 2 is given by Φ µ + 3 = Φ −q −1 U −1 = (U, q 2 ) ∞ , and hence the operators associated to the face relations R, R are indeed intertwined under by the action of Φ a : we have ι 0 (R) • Φ µ + 3 = ι 1 (R ) • Φ µ + 3 . −q −1 U 3 −qU −1 6 −q −1 V −1 1 2 −q −1 V −q −1 V 4 5 −q −1 V −1 Figure 4.3.1. The standard necklace framed seed i 0 for g = 1 −qU −1 3 −qU −1 6 −q −1 V −1 1 q −1 U V 2 q −1 U V 4 −q −1 V −1 5 Figure 4.3.2. The framed seed i 1 = µ + 3 (i 0 ) for the canoe graph. \n Foams, Phases and Framings We have shown that the moduli space of constructible sheaves with singular support on a Legendrian surface Λ is a (quantum) Lagrangian subvariety (ideal) of a symplectic leaf in a (quantized) cluster Poisson variety. This ideal is defined by a "wavefunction." The purpose of this section is to describe the combinatorics of non-exact Lagrangian fillings L ⊂ C 3 of the Legendrian. The geometric/combinatoric set-up will allow us to make conjectures about open Gromov-Witten invariants of the pair (C 3 , L). Here are the constructions we describe. We begin with a Legendrian surface S Γ defined by a cubic planar graph Γ ⊂ S 2 , as described in previous sections. • A singular exact Lagrangian filling L 0 is constructed from an ideal foam, F. • A deformed foam F determines a non-exact Lagrangian filling, L. • L is a branched double cover of the three-ball, branched over a tangle, also defined by F . • A deformation is described by a short arc between strands of the tangle at each vertex. • The map τ : H 1 (Λ) H 1 (L) is determined combinatorially from F and the arcs. • A splitting of the map τ gives a phase and framing. • We further require a maximal cone of H 1 (L). • These constructions allow us to make open Gromov-Witten predictions about (C 3 , L). • All these notions can be carried through allowed mutations of the deformed foam F . The upshot is that we get open Gromov-Witten predictions from the wavefunction at all points of the framed seed groupoid accessed by allowed mutations from the necklace foam. This is a large class of Lagrangian fillings and framings. We now proceed as outlined above. 5.1. Foams. A cubic graph Γ on the sphere S is dual to a triangulation of S. If Γ is three-connected, then by Steinitz\'s theorem it is the edge graph of a polyhedron. A foam F is the dual structure to a tetrahedronization of the polyhedron: it is a polyhedral decompsition of the three-ball B with ∂B = S. The data of F includes the quadruple (R, F, E, V ) of regions, faces, edges and vertices. A face or edge is called external if it intersects the boundary, and internal if it does not. The foam is ideal if it is dual to an ideal tetrahedronization of B, i.e one with no internal vertices. Even if Γ is not dual to a polyhedron, the notion of ideal foam makes sense -see  [TZ, Definition 3.1] . For example, if there is a bigon between two vertices, then there is a single edge of the foam whose boundary is those vertices -see Example 5.1. Example 5.1 (Foam filling for Γ neck g ). The necklace graph has a distinguished foam filling, that we in fact believe to be unique. This foam has no vertices: F 1 is already smooth -in other words there is a unique phase. See Figure  5 .1.1 below. In fact, using the local construction at the left of Figure  5 .1.1, we can construct similar foam fillings of any iterated sequence of bigon additions (handle attachments for the Legendrian surface), starting from the genus-zero necklace (theta graph). We refer to these as necklace-type graphs, and equip them with these canonical foam fillings. Note that while generic foams are dual to tetrahedronizations, these foams are dual to somewhat degenerate tetrahedronizations. For that reason, we will mainly focus on foams and not their duals. The singular, exact Harvey-Lawson Lagrangian L 0 in (C 3 , ω std = dθ std ) is a branched 2 : 1 cover of R 3 , branched over the over edges. L 0 is a cone over S 1 × S 1 with parametrization (r, s, t) → (re is , re it , re −i(s+t) ) ∈ C 3 , where r ∈ R ≥0 and (s, t) ∈ S 1 × S 1 . The covering map is the restriction to L 0 of C 3 → R 3 sending a complex triple to its real part: explicitly (r, s, t) → (r cos(s), r cos(t), r cos(s + t)) ∈ C 3 . The map is 1 : 1 over the four rays with (s, t) = (0, 0), (π, 0), (0, π), (π, π), which we think of as a singular tangle. The six sheets of the foam are defined by s = 0, s = π, t = 0, t = π, s + t = 0, s + t = π. The primitive function f obeying df = θ std | L is f = 1 4 r 2 (sin(2s) + sin(2t) − sin(2s + 2t)) . Note that f is odd under the hyperelliptic-type involution (s, t) ↔ (−s, −t) and f = 0 along the preimages of the sheets of the foam. Thus f allows us to label the branches of L 0 on the regions R.  sheets drawn. Warning: diagonally opposite vertices lie in different half-spaces, so despite appearances the two corresponding triangular sheets do not intersect outside the origin. At right is the deformed foam of its smoothing. Two sheets (pink) are smoothed to have hyperbolas as boundaries, while the boundaries of the other four consist of two halves of different hyperbolas, as well as the arc (green). 5.1.2. Foams and singular exact Lagrangians. From a foam F we can define a singular exact (not necessarily special) Lagrangian L 0 locally modeled on the Harvey-Lawson cone and foam -see  [TZ, Section 3.2] . As with the Harvey-Lawson cone and foam, we can define a multi-valued function f whose sign labels the branches of L 0 in the regions of the foam. \n Deformation of the Harvey-Lawson foam. There are three distinct families of smoothings of L 0 corresponding to the three matchings of the four edges. We will describe the one for the matching 0 ↔ 1, 2 ↔ 3; the others are similar and are related by a permutation of coordinates. The smoothing L has the topology of R 2 × S 1 and has a parametrization in polar coordinates (r, s, t) → ( √ r 2 + 2 e is , re it , re −is−it ) ∈ C 3 , which maps to ( √ r 2 + 2 cos(s), r cos(t), r cos(s + t)) ∈ R 3 . These are all diffeomorphic for = 0, so when we are interested in topological questions, we can restrict to L 1 without loss of generality. The branched cover is 1 : 1 over the points with (s, t) = (0, 0), (π, 0), (0, π), (π, π), and these parametrize four rays E i which trace out two hyperbola components (E 0 ∪ E 1 = {x 2 − y 2 = 1, y = z, x > 0} and E 2 ∪ E 3 = {x 2 − y 2 = 1, y = −z, x < 0}), a smoothing of the singular tangle of L 0 . ) There is also the line segment a ⊂ R 3 between (−1, 0, 0) and (1, 0, 0) which we call an arc -it is the image of r = 0. Note that L 1 → R 3 is 2 : 1 over the arc. The six sheets F ij now bound either a smooth edge E i ∪ E j if (ij) = (  01 ) or (  23 ), or otherwise the union E i ∪ E j ∪ a. This will be our local model of a deformed foam. More generally, let s i be the matching of edges of F HL which pairs v 0 and v i . We write F HL,s i for the local deformed foam of L 1 Its arc is the line segment between −e i and e i . We write F HL, ,s i for the deformed foam of L . Away from the origin and the preimage of the arc, the Harvey-Lawson cone and its smoothing are homeomorphic: L 0 | r =0 ∼ = L 1 | r =0 . As a result, the same primitive function f can be used to label regions of the foam and of its deformation, at least away from the arc. The local geometry of L and the deformed foam near an arc is shown in Figure  5  The neighborhood of an arc (the green line segment) and its lift to the Lagrangian (green oval). Four sheets, forming two surfaces (gray) meet at the arc. The cross-sectional planes are shown, along with the sign of the primitive function f on L. The red dots are where the sheets s = 0, π meet the arc, so the sign of f changes as they are crossed. The purple dot is the cross section of the oriented loop γ a -see Definition 5.9. 5.1.4. Deformed Foams. Given a foam F, we will define a deformed foam F to be a structure locally modeled near each vertex on a Harvey-Lawson deformed foam. Definition 5.2. Let F be a foam with vertex set V consisting of n := #V vertices. Write S for the set of matchings of half edges at each vertex, so #S = 3 n . Let s ∈ S. We define a deformed foam F s to be any set of vertices, edges, arcs, faces and regions which agrees with F outside some 3 -neighborhood of V , is homeomorphic to F outside of a 2 -neighborhood of V , and which is linearly equivalent to the local deformed foam of F HL, ,s of Section 5.1.3 within a 2 -ball of each vertex. The smoothed Lagrangian L is branched over a tangle T ⊂ B, i.e. T is a one-manifold. The construction of L from a deformed foam identities a particular set of branch cuts we call arcs. Definition 5.3. Let s i be the smoothing of the Harvey-Lawson foam which matches the ray R ≥0 •v 0 with R ≥0 • v i , where v 0 and v i are as in Section 5.1.1. The arc of the deformed Harvey-Lawson foam F HL,s i is the line segment from −e i to e i . An arc of a deformed foam F is the locus in B corresponding to the arc of the Harvey-Lawson foam under the local identification of F with F HL,s i . We write A for the set of arcs of a deformed foam F . Given an arc a of a deformed foam F and its associated branched double cover π : L → B, define λ a = π −1 (a). Since π is 2 : 1 over the interior of the arc and 1 : 1 at its edges, λ a is a circle. Note that λ a does not (yet) have a distinguished orientation. π a λ a Remark 5.4. If F is a foam, a deformed foam F is defined by choosing a matching of the four internal edges meeting at each vertex. In the case of the necklace graph Γ g neck , the foam F has no internal vertices, and therefore F = F. In particular, F is already deformed. In fact, we will learn that the g + 1 strands of Γ g neck can be thought of as the arcs of F = F, and the face of F which they bound gives rise to a single relation among them -see Definition 5.12 and Proposition 5.13. Similar considerations apply whenever Γ has a bigon. \n Phases and Framings. Definition 5.5. Let H ∼ = Z 2g be a rank-2g lattice with a non-degenerate, antisymmetric pairing ω. A phase is a rank-g isotropic subgroup K ⊂ H. A framing of K is a transverse isotropic subspace. We call the combination of phase and framing an isotropic splitting, or sometimes just splitting. We will be studying phases and framings when H = H 1 (Λ) is the homology of a genus-g surface, Λ and ω is the intersection pairing. So let L be an orientable three-manifold with boundary a genusg surface Λ = ∂L, with H 2 (L) = 0. Then it follows from the long exact sequence in homology together with the Poincaré-Lefschetz duality isomorphisms H 1 (L) H 2 (L, Λ), H 1 (L, Λ) H 2 (L) that b 1 (L) = g, 3  so that we obtain the short exact sequence (5.2.1) 0 / / H 1 (L) / / H 1 (Λ) τ / / H 1 (L) / / 0 The notion of phases and framings will apply to above geometic setting. Definition 5.6. Suppose H = H 1 (Λ) is the first homology of a genus-g oriented surface Λ, and L is an orientable three-fold with ∂L = Λ. We have the short exact sequence of Equation  5 .2.1. We say that a phase K ⊂ H is geometric if K = Ker(τ ) ∼ = H 1 (L). An accompanying framing is an ω-isotropic splitting τ : H 1 (L) → H 1 (Λ) of the short exact sequence (5.2.1). In the context of open Gromov-Witten theory, Λ is Legendrian in a contact manifold and L is Lagrangian in a symplectic filling. Remark 5.7. In  [TZ] , the above geometric phases were called "OGW framings" to connote open Gromov-Witten theory. The definition was generalized from  [AKV] , where mirror symmetry was used to make conjectures in open Gromov-Witten theory.  4  The terminology stems from the connection to Chern-Simons theory through large-N duality, where Lagrangians are knot conormals and framing relates to the framing of knots. We describe the connection to open Gromov-Witten theory later in this section. Remark 5.8. We need phases and framings to define a framed seed as in Definition 3.1, from which we will construct wavefunctions and conjectural enumerative inormation -see Conjecture 6.8. The geometry behind this is as follows. Let X = X P GL 2 ,S 2 be the cluster variety of framed local systems on a sphere. Let P be the symplectic leaf of unipotents, and let M be the Lagrangian subvariety defined by trivial monodromy. Let Γ be a cubic graph on the sphere S = S 2 and S Γ ⊂ J 1 (S) the associated a Legendrian surface up to isotopy. We write P Γ = H 1 (S Γ ; C * ) for the corresponding cluster chart. 5 A splitting allows us to write H 1 (S Γ ; C * ) as T * (H 1 (L; C * ))/H 1 (L). When we lift M to T * H 1 (L; C * ) we can write it locally as the graph of the differential of a function W Γ on H 1 (L; C * ), from which we will extract enumerative information -see Section 8.5. Recall from  [TZ]  the combinatorial model of the first homology of a Legendrian Λ := S Γ defined from a cubic planar graph Γ on a sphere, S. The faces of Γ define a relation ∼ Γ on the edge lattice Z E Γ , namely e∈∂f e ∼ Γ 0. We then have H 1 (S Γ ) ∼ = Z E Γ / ∼ Γ . We have an antisymmetric pairing ω on Z E Γ , depending only on the orientation of S, defined by ω(e, e ) = ±1 if e and e are adjacent to a vertex v with e preceding/following e in the cyclic ordering at the vertex e v e , and zero otherwise. Since e∈∂f e generates the kernel of this pairing, ω descends to a nondegenerate, antisymmetric intersection pairing ω on (5.2.2) H 1 (S Γ ) ∼ = Z E Γ / ∼ Γ . We now have a combinatorial model of H 1 (S Γ ). We next build combinatorial models of H 1 (L) for L arising from a deformed foam, and of the map H 1 (S Γ ) → H 1 (L). 5.3. Combinatorics of Tangles from Deformed Foams. We continue our study of smooth Lagrangians arising from deformed foams. Cutting to the chase, the loops defined by the edge set E and arc set A will generate H 1 (S Γ ) and H 1 (L), with relations determined by faces. In total, we find Z E Γ ∼ Γ / / ι H 1 (S Γ ) τ Z E Γ ∪A ∼ F / / H 1 (L) . Here ι is induced by the inclusion E Γ → E Γ ∪ A, and the top line was defined in the previous section. The bottom line will be defined in this section. Let Γ ⊂ S be a cubic graph on the sphere, let F be an ideal foam on the three-ball B, whose regions, faces and edges respectively bound the faces, edges and vertices of Γ. Let S be the discrete set of smoothings of F, i.e. the set of matchings of edges incident to each vertex of F -so #S = 3 #V . Let s be a smoothing whose resulting tangle T has no circle components. Let L be a smooth Lagrangian corresponding to the deformed foam F s . Recall that for an arc a we werite λ a := π −1 (a). We now define an orientation on λ a , thus definining an element γ a ∈ H 1 (L). Since the construction of the smoothing is local, we need only look at the Harvey-Lawson smoothing L 1 and its unique arc a, which we can lift to the parametrized curve (e it , 0, 0) and take the induced orientation. This is the orientation induced from the unique holomorphic disk in C 3 bounding L, i.e. |z 1 | ≤ 1. We can also give a more combinatorial construction that does not require an explicit local model, as follows. 5 As explained in Section 4.1, the cluster charts PΓ of P are spaces of rank-one local systems with fixed monodromy −1 around the critical points of the branched double cover SΓ → S 2 . Since PΓ is a torsor over H 1 (SΓ; C * ), its tangent space at any point is canonically H 1 (SΓ; C) and its Poisson structure is determined by the intersection form on SΓ, independent of choice of base point. Hereafter, we often omit the distinction and refer to cluster charts as the tori H 1 (SΓ; C * ). Definition 5.9. We choose a canonical orientation for λ a by orienting the arc arbitrarily and taking a push-off λa of the path along the arc that has some combinatorial properties, using the primitive function, f . We require that near the start of the push-off, in the chosen orientation, that f has a negative value and lies in one of the two fat regions (see Figure  5 .1.4) -in particular, outside of two sheets which meet at the arc\'s origin -then crosses once at the midpoint of the arc in a counterclockwise direction (in the induced orientation of the transverse plane). The remainder of λa traverses the arc backwards after crossing the origin of the transverse plane at the arc\'s endpoint, and has the same combinatorial recipe as the first half of λa . This completes the description of the push-off, λa . There are actually two such push-offs, but the resulting paths are homotopic. Likewise, the opposite orientation of the arc leads to a homotopic path (just shifted). For an arc a, write γ a for the resulting element of H 1 (L). Remark 5.10. The prescription in Definition 5.9 is similar to how unoriented edges of a cubic planar graph lead to an oriented loop of the associated branched cover -see  [TZ, Section 4.6 ]albeit somewhat more intricate. For each face of the deformed foam F , we will define a relation among the edges e and arc loops γ a along its boundary. Together these relations will characterize H 1 (L) as Z E Γ ∪A / ∼ F . To define the relation, we need a careful discussion of the sign of an arc relative to a face. Definition 5.11. Let F be a face of a deformed foam F bounding an arc a ∈ A. Then we have a homeomorphism of a neighborhood of a with a neighborhood of the lone arc of the Harvey-Lawson deformed foam F HL,s i defined by some smoothing s i which pairs the edges containing vectors v 0 and v i -see Section 5.1.3. Let F ij be the face of F HL,s i corresponding to F , which deforms the face F ij of F HL containing v i and v j (note i, j ∈ {0, 1, 2, 3}). Let us orient the arc from the end bounding the strand of the tangle deforming the edge of F HL with v i to the end bounding the tangle strand deforming the edge with v j . Call a vector along the arc in this orientation v. We define the sign of the arc relative to the face by σ(F, a) := sgn det(v, v i , v j ) = sgn det(−v, v j , v i ) v −v F a v i v j Note that the opposite orientation on the arc leads to sgn det(−v, v j , v i ), which is the same. The definition therefore only depends on the orientation of B. Definition 5.12. Let F be foam with boundary Γ, and let F be a deformed foam with arc set A. We define a relation ∼ F on Z E Γ ∪A by setting H 1 (L). Here we want to understand this combinatorially when L arises from a deformed foam F , in terms of its arcs and the edges of Γ. Proposition 5.13. Let F be an ideal foam filling a cubic graph Γ with edge set E Γ , and let L be a smoothing associated to a deformed foam F with arc set A, such that the corresponding tangle has no circle components. Let ∼ F be as in Definition 5.12. Then H 2 (L) = 0, and we have an isomorphism H 1 (L) ∼ = Z E Γ ∪A / ∼ F such that the homology pushforward H 1 (S Γ ) → H 1 (L) is identified with the map induced by the inclusion Z E Γ → Z E Γ ∪A . Before the proof, a remark. Remark 5.14. If Γ has no bigons, then each edge e is equivalent to a sum of arcs under Equation  5 .3.1 by the external face of F containing e in its boundary. Then after taking the partial quotient of Z E Γ ∪A → Z A by the external faces of F , we may think of H 1 (L) as Z A / ∼ F . Proof. We prove the proposition by induction on the number of internal vertices of a foam F. The base cases (no internal vertices) then consist of any of the canonical foam filling of necklacetype graphs of any genus, as in Example 5.1. Each such graph is itself obtained from the genus-0 necklace (theta graph) by bigon addition, or Legendrian one-handle attachment of the corresponding Legendrian surface -see  [CZ, Theorem 4.10(1) ]) -so we treat the base cases themselves by induction on the genus. The genus-0 foam consists of the three filled semicircles in the unit ball at azimuthal angles 0, 2π/3, 4π/3. The edge lattice modulo face relations is zero, as is H 1 (L) for the filling, and the proposition is true. Now we induct on the genus of the base case by adding bigons. Each bigon addition adds three edges and one face to the boundary, as seen here,  e  e − e + → thereby increasing H 1 of the Legendrian by 2 and the genus by 1. Two faces are added to the foam, which end in the two edges of the bigon. The bigon edges sum to zero in homology of the Legendrian, by the relation from the bigon face. The foam face relations then show that these edges are trivial in H 1 of the filling, thus in the kernel of the homology map corresponding to inclusion of the boundary -see Figure  5 .1.1. The difference e + − e − is in no boundary and therefore is an additional nontrivial class in H 1 of the Lagrangian filling the new Legendrian. This establishes the base case of no internal vertices, for every genus. We now induct on the number of internal vertices by attaching a Harvey-Lawson foam. We can attach at a single vertex, along an edge, or a face. To verify the inductive step in the first case, let F be a smoothed ideal foam, whose boundary is a cubic graph Γ F of genus g. Now suppose ∆ is a single tetrahedron together with a smoothed Harvey-Lawson foam F ∆ in it. Let us choose a vertex v of Γ F along with a vertex w of the cubic graph Γ ∆ on the boundary of the tetrahedron. Let us write e 1 , e 2 , e 3 for the three edges of Γ F incident to the vertex v listed in cyclic order determined by the orientation, and similarly write 1 , 2 , 3 for the edges of Γ ∆ incident to w, but listed in opposite cyclic order. Each of these edges determines an external face of the corresponding foam, which we denote by f e i or f i . We glue a neighborhood of the vertex w to F by identifying the tetrahedron (dual) face corresponding to w with the boundary (dual) face of F corresponding to v as indicated in Figure  5 .4.1, so that each edge e i is glued to the corresponding i to form a new edge e i . As a result of this gluing, we obtain a new ideal foam F . The set of faces of the new deformed foam F may be described as follows. The internal faces of F are the same as those of F . The set of external faces of F consists of all those external faces of F and F ∆ that correspond to edges of Γ F or Γ ∆ not incident to v, w, along with three faces f 1 , f 2 , f 3 obtained by gluing each f e i to the corresponding f i . We now turn our attention to the effect of this gluing at the level of the double covers of the ball. Let us write π : L → B, π : HL → B for the branched double covers corresponding to F and  i * + ι * : H 1 (L) ⊕ H 1 (HL ) → H 1 (L ), such that [e i ] = i * ([e i ]) + ι * ([ i ] ). Hence all that remains is to verify the face relations for the faces f 1 , f 2 , f 3 of F obtained by gluing faces of F to those of F ∆ . But recall from 5.11 that the definition of the sign of an arc a relative to a face f is entirely local, depending only on the tangent vectors v i , v j to the two edges of the deformed foam that meet a and bound f . So if a 0 is the unique arc at the vertex of F that is connected to v by an edge, and a 1 the corresponding arc in F ∆ (connected to w), the sign of a 0 with respect to face f e i in F is identical to its sign with respect to face f i of the glued foam F . Similarly, the sign of a 1 with respect to f i coincides with its sign with respect to f i . The face relation for f i is therefore obtained as the sum of those for f i and f e i under the isomorphism (5.4.1). We next consider the case of gluing in a Harvey-Lawson cone along an edge. We have a foam F with boundary Γ F and a Harvey-Lawson foam F ∆ with boundary a tetrahedron graph Γ ∆ . Suppose that we fix an edge e 0 of Γ F connecting two vertices v 1 , v 2 , and correspondingly fix an edge 0 of Γ ∆ connecting vertices w 1 , w 2 of ∆. Let us denote the edges of Γ F incident to v 1 by e 0 , e 1 , e 2 , cyclically ordered in accordance with the orientation of ∂B, and similarly write e 0 , e 3 , e 4 for the edges incident to v 2 . We denote by 0 , 1 , 2 the edges incident to w 1 but ordered with respect to the opposite of the orientation on ∆, and similarly write 0 , 3 , 4 . We write 5 for the remaining edge of Γ ∆ which is incident to neither w 1 nor w 2 . We now glue the foam F ∆ to F by identifying the edges so that each edge e i is glued to the corresponding i . We denote by F the ideal foam produced as a result of this gluing. Note that the cubic graphs Γ F and Γ F have the same genus: indeed, the two are related by a single diagonal exchange/edge mutation, as illustrated in Figure  5 .4.3. (We will return to this point in Proposition 5.22.) The set of external faces of F is thus in natural bijection with that of F: the latter contains the external faces {f i , i = 1, . . . 4} obtained by gluing each face f e i to the corresponding f i , along with the external face with boundary f 5 . On the other hand, we now have a new internal face f 0 created by gluing f e 0 to f 0 . By assumption, the smoothing of the foam in ∆ is chosen such that the tangle in the glued deformed foam F has no circle components; this is equivalent to requiring that at least one of the faces f e 0 , f 0 contains an arc as part of its boundary. We now consider the gluing of double covers π : L → B and π : HL → B. We have L = L∪ π −1 (σ) HL , where σ is a neighborhood in B of the edge 0 , or dually the quadrilateral along which the dual tetrahedron ∆ is glued to B. As shown in Figure  5 .4.4 the space π −1 (σ) is homeomorphic to a cylinder C. We fix the isomorphism H 1 (C) Z{γ}, where we take the generator γ to be Since the opposite pairs of blue boundary edges are identified, the space π −1 (σ) is homeomorphic to a cylinder. the oriented loop on ∂HL given by canonical lift of the edge 0 of Γ ∆ . The relevant part of Mayer-Vietoris sequence then reads (5.4.2) 0 → H 2 ( L) → Z{γ} → H 1 (L) ⊕ H 1 (HL ) → H 1 ( L) → 0. By our assumption that at least one of the faces f e 0 , f 0 contains an arc as part of its boundary, we see that the map i * ⊕ (−ι * ) : Z{γ} → H 1 (L) ⊕ H 1 (HL ) is injective. Hence H 2 ( L) = 0, and H 1 ( L) H 1 (L) ⊕ H 1 (HL ) Z{γ} . The face relations for all external faces of F now follow from this description of H 1 ( L) exactly as in the case of the single-triangle gluing. Finally, since (i * ⊕ (−ι) * ) (γ) = ([e 0 ], [ 0 ]), we see from the isomorphism (5.4.2) that the relation in H 1 ( L) corresponding to the new internal face f 0 is also obtained as the sum of the relation corresponding to f e 0 in H 1 (L) with that corresponding to f 0 in H 1 (HL ). It remains to consider attaching along a triangular face. The proof is very similar to the above case, so we only comment briefly. In this case, the attachment is along a punctured torus, so H 1 (π −1 (σ)) has rank two. It still injects into H 1 (L) ⊕ H 1 (HL ), and otherwise the exact sequence looks the same. Therefore H 1 ( L) has rank one less than H 1 (L). The rest of the proof is as above. This completes the proof of the Proposition. 5.5. Example -triangular prism. Let Γ be the edge graph of a triangular prism and let F be the deformed foam pictured here: b c a T 1 T 2 T 3 B 1 B 2 B 3 L 1 L 2 L 3 Write G for the gray face and P for the pink face. Then σ(P, a) = 1, and from Equation  5 .12, P gives the relation τ (T 1 ) + γ a = 0. In total, the external face relations give τ (T 1 ) + γ a = 0 τ (L 1 ) + γ b = 0 τ (B 1 ) − γ b − γ a = 0 τ (T 2 ) − γ a − γ c = 0 τ (L 2 ) = 0 τ (B 2 ) + γ a = 0 τ (T 3 ) − γ b = 0 τ (L 3 ) + γ c = 0 τ (B 3 ) − γ c = 0 We also have the internal (gray) face relation, and since σ(G, b) = σ(G, c) = 1, we see γ b + γ c = 0. The relations are consistent with the face relations from Γ. For example, the sum T 1 +T 2 +T 3 ∼ Γ 0, and this implies τ (T 1 + T 2 + T 3 ) = 0, or γ b + γ c = 0, and this is true by the internal gray face relation of F . The other face relations are consistent, as well. So τ indeed descends from a map from Z E Γ ∪A to one from H 1 (S Γ ) ∼ = Z E Γ / ∼, giving a map to H 1 (L) = Z A / ∼. H 1 (S Γ ) is rank-4 and we can take Darboux generators T 1 , T 2 ; B 2 , B 1 (careful about the cyclic order on the back side of the prism: ω(B 2 , B 1 ) = 1). H 1 (L) is rank-2 and we can take generators γ a , γ b . With these generators, τ (T 1 ) = −γ a , τ (T 2 ) = γ a − γ b , τ (B 2 ) = −γ a , τ (B 1 ) = γ a + γ b . We can see that the kernel of τ is generated by µ 1 := −(T 1 + T 2 + B 1 + B 2 ) and µ 2 := T 1 − B 2 , and is indeed isotropic. A framing H 1 (L) → H 1 (S Γ ) must send γ a to −T 1 + αµ 1 + βµ 2 and γ b to B 1 + B 2 + γµ 1 + δµ 2 The image is isotropic if β = γ, so the different framings for this phase are parametrized by symmetric 2 × 2 integer matrices α β β δ . 5.6. Associated Cones, Geometric Cones. To formulate open Gromov-Witten conjectures, we want to express a wavefunction in a power series about a limit point of the moduli space. A phase and framing define an algebraic torus, but pinning down a limit point for the expansion requires the notion of an associated cone, which we define below after setting notation. We write Γ for the underlying cubic graph, S Γ for the associated Legendrian surface, F for the deformed foam, and L for the corresponding Lagrangian. Definition 5.15. Given a splitting, i.e. a phase K = Ker( τ : H 1 (S Γ H 1 (L)) and a framing F ⊂ H 1 (S Γ ) ω-isotropic and transverse to K (so τ : F ∼ −→ H 1 (L)), an associated cone (or just cone) is an open integral convex cone C F ⊂ F ∼ = H 1 (L) containing no lines. With Remark 5.14 in mind, if Γ is simple (in particular has no bigons) and we are given a splitting, then we can specify an associated cone by choosing a spanning set of arcs. Definition 5.16. Define a geometric cone of a deformed foam F with ∂F = Γ to be the Z ≥0 span of a spanning set of arcs and edges in Z E Γ ∪A / ∼ F . When Γ is simple, without loss of generality we take a spanning set of arcs. Example 5.17. Let F = F be the foam for the necklace graph Γ g neck (see Remark 5.4), and L the corresponding Lagrangian. Label the beads 0 through g in clockwise order from some chosen starting point, and let b i , i = 0, ..., g, be edges along the outer edges of the corresponding bead. Label the strands of the necklace a i , i = 0, ..., g, so that the ith strand succeeds the ith bead in clockwise order and ω(a i , b i ) = 1. The b i span the kernel of τ : H 1 (S Γ g neck ) → H 1 (L), so define a phase. (The strands a i function as arcs, albeit there are no vertices, as they connect tangle components.) The map a i → a i defines a splitting H 1 (L) → H 1 (S Γ g neck ). We note the following relations in H 1 (S Γ g neck ): a i = 0, b i = 0. So any g-element subset of {a i } determines a geometric cone, and by symmetry we may as well take this to be a 1 , ..., a g . The necklace therefore has a unique (up to symmetry) phase, framing and geometric cone. Example 5.18. A Harvey-Lawson smoothing has a single arc and therefore a unique geometric cone. The blue edges are equivalent under the face relations and span the kernel (phase) of π : H 1 (S Γ ∆ ) → H 1 (L). A splitting is defined by mapping the green arc to a transverse element of H 1 (Λ), and the unique associated cone is the Z ≥0 span of this vector. Example 5.19. Let F be as in the Example of Section 5.5. Given that Γ is simple, per Remark 5.14 and the fact that the internal face relation is a + b ∼ 0, we conclude that there are two choices of geometric cones: {a, b} and {a, c}. 5.7. Mutations of Foams and Cones. We now show that for a large class of mutations of the boundary graph, the foam filling can be mutated, along with a phase, framing and cone. Definition 5.20. A mutation of a deformed foam at an edge e ∈ Γ is allowable if e is not the boundary of a single tangle strand. The reason for this definition is to exclude the case where the class [e] ∈ H 1 (S Γ ) is in the kernel of τ : H 1 (S Γ ) → H 1 (L), rendering the action on the wavefunction zero. At the level of tangles, the condition ensures that the new tangle has no circle component. Proposition 5.21. Let Γ be a cubic planar graph bounding a deformed foam F . Let Γ e be the graph defined by performing an allowable mutation at the edge e ∈ E(Γ). Then there is either one or two canonically defined deformed foams F e,+ and F e,− with boundary Γ e , corresponding to positive and negative mutations, respectively. Proof. The proposition follows immediately from the proof of Proposition 5.13: the allowed mutations correspond to attaching a Harvey-Lawson foam along an edge, with the allowable condition corresponding to the hypothesis that the tangle of the deformed foam have no circle components. Nevertheless, for the convenience of the reader, we provide a separate description in the language of triangulations -though they are not as general as foams (the foams of necklace-type graphs are degenerate tetrahedronizations), they are often easier to visualize. We first mutate the ideal foam F, then worry about its deformation F . On the surface ∂B = S 2 ⊃ Γ, the geometry near the dual edge e ∨ of e is a quadrilateral as pictured here: Suppose is formed from by adding a tetrahedron as in the proof of Proposition 5.13, and so F e is formed from F by attaching a Harvey-Lawson foam F HL . We define F e by extending F together with a choice of one of the three possible smoothings of F HL One of these three pairs the two tangles with endpoints at the centers of triangles ABC and ABD (pictured in red and blue) with one another, creating a new short tangle component. This is the disallowed smoothing. The other two rotate pair these with the centers of the two new triangles ACD and BCD. The matching corresponding to the deformed foam of the positive mutation F e,+ is shown above. F e,− is defined similarly. Now suppose otherwise that is formed from by deleting a tetrahedron T . Then was the result of a mutation µ e,± of and only the inverse mutation µ e,∓ is possible. Since the case where e bounds a single tangle component is not allowed, the tangle components after deleting the tetrahedron are clear: they are truncations of the original tangle strands. Proposition 5.21 will allow us to transport foams across mutations, along with phases, framings and cones. This will allow us to connect open Gromov-Witten conjectures for Lagrangian fillings related by allowed mutations which have corresponding cones, phases and framings. Proposition 5.22. Let F and F be deformed foams corresponding to an allowed mutation Γ → Γ of their boundaries. Then there is a canonical isomorphism ϕ : Z E Γ ∪A F / ∼ F ∼ = Z E Γ ∪A F / ∼ F Proof. We can assume that Γ is obtained by attaching a tetrahedron, as removal will give rise to the inverse isomorphism. A local study near the attachment will suffice to establish ϕ. We label the relevant edges and vertices as in the figure below, with Γ indicated by dashed lines. 2 = 6 sheets of the deformed Harvey-Lawson foam, after gluing to F and deforming. They correspond to unordered pairs from among the vertices {v, w, v, w}. Write f v,w for the face determined by v and w, and likewise for the others. Let γ be the arc of the Harvey-Lawson deformed foam, and write α = σ(f v,w , γ) • γ = ±γ for the signed contribution to the relation from f v,w , as defined in Definition 5.12. Now suppose the face relations on F relate give e + s e ∼ 0, a + s a ∼ 0, and so on. Let us list the unordered pairs along with the relations from the corresponding glued face. f v,w : α + s e ⇒ α = e f v, w : e + α ⇒ e = −α = −e f v, v : a + s a ⇒ a = a f w, w : c + s c ⇒ c = c f v, w : b − α + s b ⇒ b = b + e f v,w : d − α + s d ⇒ d = d + e This gives the positive mutation. The other allowed matching v ↔ w gives the negative mutation, as follows from the interchange v ↔ w. Corollary 5.23. Suppose Γ is obtained by an allowed mutation of Γ. Let ν : Z E Γ → Z E Γ be the corresponding isomorphism of edge lattices, respecting the antisymmetric pairing. Let ϕ : Z E Γ ∪A F / ∼ F ∼ = Z E Γ ∪A F / ∼ F be the isomorphism provided by Proposition 5.22 above. Then under the isomorphisms of Equation  5 .2.2 and Proposition 5.13, the maps ν and ϕ intertwine τ : H 1 (S Γ ) → H 1 (L) with τ : H 1 (S Γ ) → H 1 ( L). Proof. It only remains to note that ν respects the antisymmetric pairing of edges. We immediately obtain the following. Corollary 5.24. The maps ν and ϕ map phases, framings and cones to phases, framings and cones. 6. The wavefunction 6.1. Construction of the wavefunction. Suppose that F is a deformed ideal foam obtained from the standard necklace foam by a sequence of admissible mutations, and f is a framing for F . As explained in Section 5, the pair (F , f ) gives rise to a framed seed i = i(F , f ). It is convenient to visualize the framed seed as a labelling of the edges of the cubic graph Γ by monomials in the standard quantum torus generated over Z[q ± ] by {U ± i , V ± i } i=1,...g . In this section, we will show that there is a canonical wavefunction Ψ i ∈ K associated to such a framed seed, thereby providing a prediction for the generating function of all-genus open Gromov-Witten invariants of the corresponding Lagrangian L F ⊂ C 3 . We begin with the definition of Ψ in the case of the standard necklace framed seed i neck . The corresponding foam gives rise to an exact Lagrangian filling of the Chekanov surface, so that by Stokes\' theorem all its open Gromov-Witten invariants will be zero. We therefore take the wavefunction for the standard necklace to be Ψ neck = 1. Let us note that the necklace wavefunction depends only on the underlying deformed foam, and is completely independent of the choice of framing f . Now let G ad be the sub-category of the framed seed groupoid G whose morphisms are given by the admissible ones, and let G ad (i neck ) be the connected component of G ad containing the framed seed i neck . Given an object i of G ad (i neck ), our prescription to construct Ψ i is as follows: choose an arbitrary path a : i neck → i in G ad . As explained in Section 3.4, the morphism a gives rise to an automorphism Φ a of K, which we apply to Ψ neck to produce a candidate for Ψ i : Ψ i := Φ a • Ψ neck . (6.1.1) What must be checked in order for this definition to make sense is that the wavefunction Ψ i depends only on the endpoint of the path a in the framed seeds groupoid. This path-independence is the content of the following Theorem. Theorem 6.1. The map Ψ : Ob (G ad (i neck )) −→ O, i −→ Ψ i is well-defined, i.e is independent of the choice of path a : i neck → i in (6.1.1). Moreover, if there exists such a path consisting entirely of primitive mutations, the wavefunction Ψ i satisfies the Ooguri-Vafa integrality constraint (1.3.1). Proof. The key observation is the following immediate consequence of Theorem 4.3: if Ψ i satisfies the face relations in framed seed i and i = a(i) where a is an admissible mutation or framing shift, then Φ a • Ψ i satisfies the face relations for i . Now suppose we have two sequences of admissible mutations and framing shifts a 1 and a 2 as in the statement of the theorem. Then it suffices to show that (6.1.2) Φ −1 a 1 Φ a 2 • Ψ neck = Ψ neck . To this end, consider the framed seed i = a −1 1 a 2 (i neck ). Its underlying cubic graph Γ is the image of the original necklace graph Γ neck under an element of the mapping class group of the (g +3)-times punctured sphere, and moreover the labelling of the edges of Γ by monomials in the U i , V i induced by its phase and framing are identical to that in the standard necklace framed seed. In particular, the face relations for i and i neck are identical, and from the binomial face relations corresponding to the beads we deduce that (1 − V i ) • Φ −1 a 1 Φ a 2 • Ψ neck = 0, i = 1, . . . , g. It follows that Φ −1 a 1 Φ a 2 •Ψ neck = Ψ neck = 1, which completes the proof that the map Ψ is well-defined. The Ooguri-Vafa integrality follows from Proposition 6.14, which is established in Section 6.4. 6.2. Examples of wavefunctions. We now proceed to compute the wavefunction defined in the previous section in some fundamental examples. Example 6.2. The calculation in Example 4.4 shows that the wavefunction Ψ i 1 associated to the framed seed i 1 for the canoe graph shown in Figure  4 .3.2 is given by Ψ i 1 = (X; q 2 ) ∞ . It satisfies the q-difference equation (1 + U V − V )Ψ i 1 = 0, which is a scalar multiple of the face relation R in (4.3.4). As an exercise, let us compute the effect on the wavefunction of applying the framing shift operator T −1 , which we recall acts on A 2g by U → q −1 U V −1 . The resulting framed seed is illustrated in Figure  6 .2.1. Lemma 6.3. We have qV U −1 3 qV U −1 6 −q −1 V −1 1 −q −1 U 2 −q −1 U 4 −q −1 V −1 5 Figure 6.2.1. The framed seed i 2 = (σ −1 • T −1 )(i 1 ) for the canoe graph. −q −1 U 2 −qU −1 4 −q −1 V −1 1 6 −q −1 V −q −1 V 3 5 −q −1 V −1 Ψ i 2 = (σ −1 • T −1 ) • (X; q 2 ) ∞ (6.2.1) = (X; q 2 ) −1 ∞ . Proof. Since (σ −1 • T −1 )(1 + U V − V ) = (1 − U − V )(σ −1 • T −1 ), the Lemma follows by observing that both sides satisfy the q-difference equation (1 − U − V )Ψ = 0, which is easily seen to have a unique formal power series solution of the form Ψ ∈ 1 + m ∈ O. Now observe that applying to the framed seed i 2 the positive mutation at edge 4 returns us to the framed seed shown in Figure  6 .2.2, which coincides with the standard necklace i 0 up to a permutation of the numbering of its edges. Hence we have a loop in the framed seed groupoid (6.2.2) i 0 i 1 i 2 µ + 3 σ −1 •T −1 µ + and we indeed see that Ψ µ + 4 (i 2 ) = Φ(−q −1 U ) • (X; q 2 ) −1 ∞ = (X; q 2 ) ∞ • (X; q 2 ) −1 ∞ = 1, in accordance with Theorem 6.1. More generally, we can consider the framed seed i canoe obtained from the standard genus g necklace framed seed i neck by performing positive mutations at all g beads labelled −q −1 U j , j = 1, . . . g under the framing isomorphism. The corresponding wavefunction is then (6.2.3) Ψ icanoe = g i=1 (X i ; q 2 ) ∞ , which is annihilated by the left ideal in D 2g generated by (6.2.4) R i = 1 + U i V i − V i , i = 1, . . . , g. Let us write Ψ i (1) canoe for the wavefunction obtained by applying the operator σ (−1,...,−1) • T −Ig to Ψ icanoe , where I g is the g × g identity matrix. Then we again have (6.2.5) Ψ i (1) canoe = g i=1 (X i ; q 2 ) −1 ∞ Lemma 6.4. The explicit power series of the wavefunction (6.2.5) is Ψ i (1) canoe = v∈Z g ≥0 1 (q 2 ) v X v , where (q 2 ) v = g i=1 v i k=1 (1 − q 2k ). Proof. Set Ψ := v∈Z g ≥0 C v (q) (q 2 ) v X v , where C 0 (q) = 1 Let e i ∈ Z g be the i th unit vector. At the level of the coefficients of X v , the equation (1−U i −V i )Ψ = 0 is equivalent to the recurrence C v (q) (q 2 ) v − C v−e i (q) (q 2 ) v−e i − C v (q) (q 2 ) v q 2v i = 0 Note that (q 2 ) v = (q 2 ) v−e i (1 − q 2v i ). Therefore we have C v (q) = C v−e i (q) = . . . = C 0 = 1. More generally, given a g × g integer symmetric matrix A, let us consider the framed seed i  Ψ i (A) canoe = v∈Z g ≥0 1 (q 2 ) v q v t v−v t Av X v , where (q 2 ) v = g i=1 v i k=1 (1 − q 2k ) (6.2.6) Example 6.5 (Non-existence of algebraic wavefunctions). Algebraic wavefunctions may not exist for framed seeds which cannot be obtained from the standard necklace by a sequence of admissible mutations. A simple counterexample is given by the framed seed obtained from the standard g = 1 necklace by performing positive mutations at both of its strands to produce another necklace graph. The arguments of the corresponding quantum dilogarithms are −q −1 U and −qU −1 , only the first of which corresponds to an admissible mutation. The face relation associated to the bead of the resulting framed seed imposes the difference equation (1 + V U )Ψ = 0, and it is easy to see this admits no solutions in the ring Q((q))((X)) (or for that matter in the opposite completion Q((q))((X −1 )). We conclude this section with an example from  [AENV]  concerning the unknot conormal after the conifold transition. Example 6.6 (Partition function for unknot conormal  [AENV] ). Set Ψ(X) = (X; q 2 ) −1 ∞ (QX; q 2 ) ∞ , where the closed-string parameter Q is a formal variable commuting with all the other variables. Then Ψ(X) is annihilated by L = (1 − U ) − (1 − QU )V = 1 − U − V + QU V Lemma 6.7. We have Ψ(X) := k≥0 (Q; q 2 ) k (q 2 ; q 2 ) k X k . Proof. Set Ψ(X) := k≥0 C k (Q, q 2 )X k , where C 0 = 1 (6.2.7) By computing the coefficients of L • Ψ, we get C k − C k−1 − q 2k C k + Qq 2(k−1) C k−1 = (1 − q 2k )C k − (1 − Qq 2(k−1) )C k−1 = 0 Therefore C k = k i=1 (1 − Qq 2(i−1) ) k i=1 (1 − q 2i ) = (Q; q 2 ) k (q 2 ; q 2 ) k . Note that under the specialization Q = 1 of the closed string parameter we recover the necklace wavefunction Ψ neck = 1, while the specialization Q = 0 delivers the wavefunction Ψ i 2 in (6.2.1) associated to the framed seed (6.2.1) for the canoe graph. Hence the closed-string parameter Q describes an interpolation between these two framed seeds. 6.3. Open Gromov-Witten Conjectures. We can now propose an interpretation of the wavefunction of a geometric seed: it is the generating function of open Gromov-Witten invariants of the Lagrangian filling defined by the deformed foam. To be more precise, we recall the geometric framework. Let i ∈ G ad , meaning there is a path a : i neck → i in the admissible framed seed groupoid. By Theorem 6.1, there is a well-defined wavefunction Ψ i = Φ a • Ψ i neck = Φ a • 1. The framing of i has geometric content. Recall from Example 5.17 that i neck is canonical. By Proposition 5.22 and especially Corollary 5.24, we learn i = a • i neck is a geometric seed, i.e. has a geometric phase, as well as a framing and cone. That is, there is a corresponding cubic graph Γ, deformed foam filling F , and Lagrangian L, along with phase K = Ker(τ : H 1 (S Γ ) → H 1 (L)) and transverse isotropic framing F ⊂ H 1 (S Γ ), as well as a cone C ⊂ F. We choose a basis e i , i = 1, ..., g for C. The sequence 0 → K → H 1 (S Γ ) → π(F ) → 0 and basis e i then defines a framing for i in the sense of Section 3.1. The geometric seed identifies the quantum torus T q i with the quantization of the symplectic lattice H 1 (S Γ ) endowed with its intersection form. In particular, a monomial X d = g i=1 X d Such open Gromov-Witten problems depend on additional data known as a framing. While there is not yet a rigorous definition of these open Gromov-Witten invariants, it is anticipated that it will involve framings as constructed here, generalizing the well-studied cases of Aganagic-Vafa branes  [AKV, KL, FL] .  6 We then conjecture that the wavefunction Ψ i ∈ C[q, q −1 ][[{X i }] ] is the all-genus generating function of open Gromov-Witten invariants and obeys Ooguri-Vafa integrality, which expresses the invariants in terms of the quantum dilogarithm Φ(z) = n≥0 (1 + q 2n+1 z) −1 . Conjecture 6.8. Let i ∈ G ad be a framed seed with wavefunction Ψ i . Write A for the framing and L for the Lagrangian of the deformed foam. Then Ψ i = d∈Z ≥0 \\{0} s∈Z Φ(X d (−q) s ) n (A) d,s , with n (A) d,s ∈ Z the Ooguri-Vafa invariants. Remark 6.9. The Ooguri-Vafa invariants are related to open Gromov-Witten invariants as follows. Write q 2 = e λ and expand Ψ i as a power series in λ (and the X i ). Then the coefficient of X d λ h is the genus-h open Gromov-Witten invariant of L in framing A, in class d ∈ H 1 (L) ∼ = H 2 (C 3 , L). See  [Za, Sections 2 and 4]  for further discussion of these variables. Remark 6.10. Recall Φ(z) ∼ e Li 2 (z)/λ as λ → 0. Conjecture 6.8 therefore reduces to the conjecture of  [TZ]  for disk invariants, described in the Introduction in Section 1.3. More specifically, writing Ψ i ∼ e W i /λ , in the semiclassical limit, meaning W is a local potential for the Lagrangian subspace M Γ ⊂ P Γ . Remark 6.11. In the next section, we provide evidence for the conjecture by arguing that the wavefunctions Ψ i obey integrality. The Harvey-Lawson brane in C 3 with its various framings, as studied in [AKV, Section 6.1], gives further evidence. This example enjoys a U (1) symmetry, permitting localization techniques for open Gromov-Witten calculations  [KL] , while the Lagrangians for for cubic graphs Γ generally do not. Further tests of the conjecture must therefore await rigorous defitions of open Gromov-Witten invariants and the development of new techniques. 6.4. Integrality of the wavefunction. In this section we will complete the proof of Theorem 6.1 by showing that the wavefunctions Ψ i satisfy Ooguri-Vafa integrality. The wavefunction Ψ constructed in the previous section is an element of the commutative local ring O Q := Q ⊗ Z O of formal power series in X 1 , . . . , X g with coefficients in the field Q((q)). Let m be the unique maximal ideal in the ring O Q . By considering the quotients O Q /m k , it is easy to show that every F ∈ 1 + m admits a unique factorization (6.4.1) F = v∈Z g ≥0 −{0} k∈Z (−q) k X v ; q 2 cv,k ∞ , c v,k ∈ Q. The coefficients c v,k for each v ∈ Z g ≥0 \\ {0} can be packaged in a Laurent series P F,v (t) := k∈Z c v,k t k ∈ Q((t)). Following [KS, §6.1], a series F ∈ 1 + m is called admissible if the P F,v (t) are Laurent polynomials with integral coefficients for all v. cluster variables associated to the seed i. For each i, the generators of these quantum tori act by unbounded, self-adjoint operators in the Hilbert space H i . The latter space comes equipped with the additional data of a dense subspace S i , the Fock-Goncharov Schwartz space, defined to be the maximal joint domain of the algebras L q X , L q ∨ X . The unitary isomorphism K i→i corresponding to an arrow a : i → i in the cluster modular groupoid preserves the corresponding Schwartz spaces, where it intertwines the action of G X on L q X by cluster transformations. When the skew form on Λ has a nontrivial kernel, the Fock-Goncharov unitary representations of the quantum cluster variety are labelled by central characters λ ∈ Hom(ker( ), R), and thus can be thought of as a kind of principal series. The reality condition is required to ensure that all elements of the underlying Heisenberg algebra act by self-adjoint operators. This self-adjointness is crucial for the entire construction: indeed, it guarantees that for each logarithmic cluster variable x k its noncompact quantum dilogarithm ϕ(x k ) defines a unitary automorphism of H i , which forms the key ingredient in defining the intertwiner K i→i . In the context of moduli spaces of framed local systems on surfaces with punctures, recall that the central characters parametrize the eigenvalues of the local system\'s monodromy around the punctures. As we have seen in Section 4.2, however, the quantization (4.3.1) of the defining constraints for P g , which impose that the monodromy around each puncture be unipotent, forces a sum of logarithmic cluster variables to act by a pure imaginary scalar, a constraint which cannot be satisfied if each such variable acts by a self-adjoint operator. Thus we cannot appeal to the standard theory of principal series in order quantize the chromatic Lagrangian -a new kind of representation of the quantum cluster variety is required. Although we do not currently know how such representations should be defined, let us sketch out some features we would desire of them in order to define an analytic wavefunction. To a framed seed i(F ) with underlying deformed foam F , we would like to associate a space of meromorphic functions V i in g variables z 1 , . . . , z g , defined by appropriate conditions on their asymptotic behavior along with the possible locations of their poles. The framed seed determines a natural action of L q X by q-difference operators on the space of all meromorphic functions on C g , and the subspace V i should be preserved under this action. To each admissible mutation or framing shift a : i 1 → i 2 , there should correspond an isomorphism between the spaces V i 1 , V i 2 . These isomorphisms should again intertwine the action of L q X , and their composites corresponding to trivial cluster transformations should act by scalar multiples of the identity. Given a representation of the quantum cluster variety P g in this sense, one could then attempt to define the wavefunction associated to a framed seed obtained from the standard necklace by a sequence of admissible mutations as in (6.1.1). To verify that this prescription is indeed welldefined would amount to showing that the action of the mapping class group of the punctured sphere (which is generated by the half Dehn twists around pairs of punctures) fixes the necklace wavefunction Ψ neck = 1. Let us note that there is another regime for which is nicely compatible with the analytic properties of the noncompact quantum dilogarithm -namely, when | | = 1. In this regime, Faddeev  [F]  has constructed discrete series-type representations of the modular double of U q (sl 2 ) whose central characters also correspond to a sum of logarithmic cluster variables acting by a pure imaginary scalar. Thus the regime | | = 1 may in fact be the most suitable one in which to try to carry out the construction of such representations of quantum cluster varieties associated to punctured surfaces. In the following subsections, we present some explicit calculations in g = 1, 2 which indicate how one might try to define the action of admissible mutations and framing shifts in the non-unitary case, and provide examples of candidate analytic wavefunctions. 7.1. Analytic wavefunctions for g = 1. We begin with the standard necklace framed seed i 0 for g = 1 as shown in Figure  7 .1.1. In the analytic setting, it is natural to regard a framed seed as associating to each edge a Heisenberg algebra element, which can then be exponentiated to yield elements of either of the two modular dual quantum tori. u − c 3 −u + c 6 −v − c 1 2 v − c v − c 4 5 −v − c Figure 7.1.1. The standard necklace framed seed i 0 for g = 1 −u + c 3 −u + c 6 −v − c 1 u + v − 2c 2 u + v − 2c 4 −v − c 5 Figure 7.1.2. The framed seed i 1 = µ + 3 (i 0 ) for the canoe graph. Hence in Figure  7 .1.1, we have decorated the edges of the cubic graph by the Heisenberg algebra elements corresponding to the logarithmic cluster variables, and we have set (see Appendix A) c := i 2 + −1 ∈ iR. The pair of modular dual quantum torus elements corresponding to edge e 3 , for example, are given by X 3 → e 2π (u−c ) = −q −1 e 2π u , X ∨ 3 → e 2π −1 (u−c ) = −(q ∨ ) −1 e 2π −1 u . Now consider the following loop in the framed seed groupoid. First, observe that the positive mutation µ + 3 at edge 3 yields the canoe framed seed i 1 shown in Figure  7 .1.2. Performing the change of framing σ conjugating all Heisenberg algebra elements by e πi(v−c ) 2 , thereby effecting the shift σ : u → u − v + c , we arrive at the framed seed i 2 = σ(i 1 ) shown in Figure  7    .1.3. v − u 3 v − u 6 −v − c 1 u − c 2 u − c 4 −v − c 5 − c 2 −u + c 4 −v − c 1 6 v − c v − c 3 5 −v − c Figure 7.1.4. The framed seed µ + 4 (i 2 ) i 0 . Finally, performing a positive mutation µ + 4 at edge 4 in i 2 results in the framed seed shown in Figure  7 .1.4, which represents the same framed seed as the initial one i 0 . We therefore have a loop in the framed seeds groupoid (7.1.1) i 0 i 1 i 2 µ + 3 σ µ + 4 We once again take the wavefunction for the standard necklace framed seed i 0 to be ψ i 0 = 1, but now regarded as an entire function on C rather than as a formal power series. We now explain how the mutations and framing shifts in (7.1.1) give rise to operators on spaces of meromorphic functions with appropriate analytic properties, and verify that the composite of these operators indeed preserves ψ i 0 up to a phase. By analogy with the Fock-Goncharov construction in the unitary case, we take the positive mutation µ + 3 at edge 3 carrying Heisenberg element by u − c to correspond to the operator of multiplication by the meromorphic function ϕ(z −c ), which has simple poles at {in +im −1 } n,m∈Z ≥1 . We thus obtain ψ i 1 = ϕ(z − c ) • ψ i 0 = ϕ(z − c ) which now satisfies the dual pair of face relations e −2π z ψ i 1 (z) + (1 − e −2π z )ψ i 1 (z + i ) = 0 e −2π −1 z ψ i 1 (z) + (1 − e −2π −1 z )ψ i 1 (z + i −1 ) = 0. Let us regard the function ψ i 1 as an element of the space V i 1 consisting of functions f (z) analytic outside of the cone {in + im −1 } n,m∈R ≥1 , and having prescribed asymptotic behavior f (z) z→∞ ∼ A − | arg(z)| > π 2 + arg( ) A + e πi(z−c ) 2 | arg(z)| < π 2 − arg( ) (7.1.2) for some A ± ∈ C. Let us now consider the effect of performing the change of framing σ. As in the case of mutation, we again define its action on our wavefunction by analytic continuation of the integral transform representing the action of e πiv 2 in the unitary case. Indeed, consider the integral (7.1.3) f −→ e −πi(z−t−c ) 2 f (t + 2c )dt, where the contour of integration stays within the domain of analyticity of f and escapes to infinity in the sectors | arg(t)| > π 2 +arg( ) and | arg(t)| < π 2 −arg( ). It follows from the asymptotics (7.1.2) that the integral converges absolutely for | arg(z) − π 2 | < π − arg( ), so that f (z) defines an analytic function on the complement of the cone {−in − im −1 } n,m∈R ≥0 . Applying (7.1.3) to ψ canoe and using the inversion and Fourier transformation properties (A.1.1) and (A.2.1) of the noncompact quantum dilogarithm, we obtain ψ i 2 = e −πi(z−t−c ) 2 ψ i 1 (t + 2c )dt = e −πiz 2 +2πic z e 2πizt e −πi(t+c ) 2 ϕ(t + c )dt = ζ inv e −πiz 2 +2πic z e 2πizt ϕ(−t − c ) dt = ζ inv ζe −πiz 2 +2πic z ϕ(−z + c ) = e πic 2 ζ 2 inv ζ • ϕ(z − c ) −1 . Finally, the positive mutation µ + 4 at edge 4 of i 2 which carries Heisenberg element u − c acts by the operator of multiplication by the meromorphic function ϕ(z − c ), and hence under our proposed action for framing shifts and admissible mutations the loop (7.1.1) does indeed act trivially on the analytic wavefunction ψ i 0 , up to a constant phase. Let us conclude our discussion of the analytic picture for g = 1 case with an example of a genuinely non-algebraic wavefunction. Recall the framed seed from Example 6.5 obtained from that in Figure  7 .1.1 by performing positive mutations at the edges labelled 3 and 6, for which we showed no algebraic wavefunction exists. On the other hand, following the prescription above, we obtain the corresponding analytic wavefunction associated to the this framed seed: Ψ = ϕ(z − c )ϕ(−z + c ) = ζ inv e πi(z−c ) 2 , where we again used the inversion formula (A.1.1) for the noncompact quantum dilogarithm. 7.2. Analytic wavefunctions for g = 2. In the genus 2 case, the combinatorics of ideal foams and framed seeds becomes richer. To illustrate this, we will describe a loop in the framed seeds groupoid that reflects a 3-2 Pachner move for deformed foams, and verify that this loop acts trivially on our proposed analytic wavefunction. Again we begin with the standard necklace framed seed i neck for g = 2, for which ψ neck = 1. Performing positive mutations at the (commuting) edges labelled u 1 − c , u 2 − c , we obtain the framed seed for the canoe graph shown i 2 in Figure  7 .2.1, whose underlying deformed foam consists of two tetrahedra. The corresponding wavefunction is ψ i 2 = ϕ(z 1 − c )ϕ(z 2 − c ). On the other hand, consider the framed seed i 3 obtained from i neck by instead performing negative mutations at the edges labelled u 1 − c , u 2 − c , followed by a positive mutation at the edge labelled −u 1 − u 2 + 3c . This framed seed is illustrated in Figure  7 .2.2. The corresponding deformed foam now consists of three deformed Harvey-Lawson tetrahedra, and the wavefunction is ψ i 3 = ϕ(−z 1 − z 2 + 3c ) ϕ(−z 1 + c )ϕ(−z 2 + c ) . Introducing the following composite of framing shift and coordinate rescaling operators σ : u 1 −→ u 1 + v 2 − v 1 + 3c u 2 −→ u 2 − v 2 + v 1 + 3c and the change of coordinates τ : we observe that the framed seed (τ • σ) • i 2 coincides with i 3 up to a re-labelling of edges of the cubic graph. We will now confirm that the corresponding wavefunctions are indeed projectively equal. u j → −u j , v j → −v j , u 1 + v 1 − 2c −v 1 − c −u 1 + c −v 2 − c u 2 + v 2 − 2c −u 2 + c u 2 + v 2 − v 1 − 2c −u 1 − u 2 + 3c u 1 + v 1 − v 2 − 2c The action of the operator σ = e −6πic (v 1 +v 2 ) e πi(v 2 −v 1 ) 2 on ψ i 2 can be understood with the help of the following Lemma: Lemma 7.1. We have e −6πic (v 1 +v 2 ) e πi(v 2 −v 1 ) 2 • ϕ(z 1 − c )ϕ(z 1 − c ) ≡ ϕ(z 1 + z 2 + 3c ) ϕ(z 2 + c )ϕ(z 1 + c ) , where the symbol ≡ denotes projective equality modulo phase constants. Commuting the operator e −6πic (v 1 +v 2 ) e πi(v 2 −v 1 ) 2 past ϕ(z 1 − c )ϕ(z 1 − c ), we see that The action of the latter operators are once again understood by means of the Fourier self-duality (A.2.1), so that we have, e.g. Proof. σ • ϕ(z 1 − c )ϕ(z 2 − c ) = ϕ(u 1 + v 2 − v 1 + 2c )ϕ(u 2 − v 2 + v 1 + 2c ) • σ • 1 = ϕ(u 1 + v 2 − v 1 + 2c )ϕ(u 2 − v 2 + v 1 + 2c ) • 1. v 1 − c u 1 + v 2 − v 1 − 2c −u 1 − v 2 + c −u 2 − v 1 + c u 2 + v 1 − v 2 − 2c v 2 − c u 1 + u 2 − 3c −u 1 + c −u 2 + c ϕ(u 1 + v 2 − v 1 + 2c ) • f (z 1 , z 2 ) = ζ −1 e 2πit(u 1 +v 2 −v 1 +c ) ϕ(t − c ) f (z 1 , z 2 )dt = ζ −1 e πit 2 e 2πit(u 1 +c ) ϕ(t − c ) e 2πit(v 2 −v 1 ) • f (z 1 , z 2 )dt = ζ −1 e πit 2 e 2πit(u 1 +c ) ϕ(t − c ) • f (z 1 + t, z 2 − t)dt = e −πic 2 (ζ inv ζ) −1 e −2πit(u 1 +2c ) ϕ(t + c ) • f (z 1 − t, z 2 + t)dt Hence we see that up to multiplicative phase constants, σ • ψ i 2 ≡ e −2πit(z 1 +2c ) e −2πis(z 2 +t+2c ) ϕ(t + c )ϕ(s + c )dsdt ≡ e −2πit(z 1 +2c ) ϕ(t + c ) ϕ(t + z 2 + c ) dt, where the Fourier integral over s is again performed using (A.2.1). On the other hand, the resulting integral over t may be computed by means of the \'pentagon\' integral evaluation (A.2.4), with the Hence we conclude that (τ • σ) • ψ i 2 ≡ ψ i 3 . Example 7.2. An interesting example, explored at the semiclassical level in Section 5.4 of  [TZ] , is the genus 3 cubic graph obtained as the 1-skeleton of the cube shown in Figure  7 .2.3. In our setting, the phase and framing considered in  [TZ]  correspond to the object i cube of the framed seed groupoid in which the edges e i are labelled by Heisenberg algebra elements Let us now explain how to derive the analytic wavefunction associated to this framed seed, and verify its semiclassical limit reproduces the prediction for the holomorphic disk invariants given in  [TZ] . We begin with the standard g = 3 necklace framed seed i neck , and perform positive mutations at the strands labelled u 1 − c , u 2 − c , u 3 − c , as well as a negative mutation at the strand labelled 5c − u 1 − u 2 − u 3 . The resulting cubic graph is isomorphic to the 1-skeleton of the cube, and the corresponding framed seed i cube is given by i cube : e 1 → c − u 1 e 2 → u 2 + v 2 − v 1 − 2c e 3 → c − u 2 e 4 → u 1 + v 1 − v 2 − 2c e 5 → u 1 + u 2 + u 3 − 5c e 6 → c − v 3 e 7 → c − u 3 e 8 → v 3 − u 1 − u 2 + 3c e 9 → −v 1 − c e 10 → v 1 − u 2 − u 3 + 3c e 11 → u 3 + v 3 − v 2 − 2c e 12 → u 2 + v 2 − v 3 − 2c The wavefunction associated to i cube is given by ψ cube ≡ ϕ(z 1 − c )ϕ(z 2 − c )ϕ(z 3 − c ) ϕ(z 1 + z 2 + z 3 − 5c ) . To pass from the framed seed i cube to the desired one i cube , we first apply the coordinate rescaling and framing shift operators e −6πic (v 1 +v 2 ) e πi(v 1 −v 2 ) 2 . By Lemma 7.1, their effect on the wavefunction is given by e −6πic (v 1 +v 2 ) e πi(v 1 −v 2 ) 2 • ψ cube ≡ ϕ(z 1 + z 2 + 3c )ϕ(z 3 − c ) ϕ(z 1 + z 2 + z 3 + c )ϕ(z 1 + c )ϕ(z 2 + c ) . In the resulting framed seed, the edge numbered 2 in Figure  7 .2.3 now carries the label u 2 + c . We now perform two consecutive positive mutations at this edge. Note that in the setting of algebraic representations and wavefunctions, we can never perform two such consecutive mutations of the same sign at an edge, as it is impossible for both mutations to be admissible in the sense of Section 3.4. In the analytic setting, on the other hand, the action of these two mutations on the wavefunction yields ψ cube ≡ ϕ(−u 2 − c )ϕ(u 2 + c ) • ψ cube ≡ ϕ(z 1 + z 2 + 3c )ϕ(z 3 − c )ϕ(−z 2 − c ) ϕ(z 1 + z 2 + z 3 + c )ϕ(z 1 + c ) . After applying the symplectic lift τ of the change of basis (u 1 , u 2 , u 3 ) → (−u 1 − u 2 , u 2 , −u 3 ), the resulting framed seed can be identified with i cube , and the corresponding wavefunction is given by ψ cube ≡ τ • ψ cube ≡ ϕ(−z 1 + 3c )ϕ(−z 2 − c )ϕ(−z 3 − c ) ϕ(−z 1 − z 3 + c )ϕ(−z 1 − z 2 + c ) . (7.2.1) In this setting, the semiclassical limit is realized by sending → 0 while rescaling z i → (2π ) −1 z i . Using the asymptotics for ϕ given in the Appendix and setting Z i = e −z i as in  [TZ] , we find 2πi 2 log(ψ cube ) ∼ Li 2 (Z 1 ) + Li 2 (Z 2 ) + Li 2 (Z 3 ) − Li 2 (Z 1 Z 2 ) − Li 2 (Z 1 Z 3 ), which coincides with the expression for the superpotential W in Section 5.4 of  [TZ] . \n Framing Duality In this section we observe a curious relationship we call framing duality. For the class of Legendrian surfaces which generalize the Clifford torus to arbitrary genus, the wavefunctions for different framings correspond to Donaldson-Thomas/Hall-Algebra generating functions of different quivers, as computed in  [KS] . The framings are defined by g × g symmetric integer matrices, A. When all entries are non-negative, the matrix determines a quiver Q A with adjacency matrix A. The quiver invariants are proven to be integers in  [E] , and are thus equal to the Ooguri-Vafa integers for the corresponding brane. They are also conjectured in  [HRV]  to count the dimensions of isotypic components of the middle cohomology of twisted character varieties. 8.1. Wavefunction for Clifford Surfaces. We define the Chekanov surface of genus g to be the Legendrian defined by the necklace graph Γ g neck , and the Clifford surface to be the defined by the canoe graph Γ g canoe -see Figure  1 .5.1. We now define the standard necklace framed seed i neck g,0 , generalizing the g = 1 case of Example 4.4 (see also Figure  4 .3.1). To fix notation and the cyclic structure, we embed Γ g neck in the plane R 2 with its standard orientation. Let I = {0, 1, ..., 2g + 1} and define the vertex set be I × {0}. Define the strand edges s 1 , ..., s g by s k = [2k − 1, 2k] × {0} and set s g+1 to be a big loop in the upper half plane connecting (0, 0) and (2g + 1, 0).  7  The two edges of the kth bead, k = 1, ..., g + 1, are taken to lie on a circle of radius 1/2 centered at (2k − 3/2, 0), with the upper hemisphere called Weyl group of type A v 1 −1 × . . . × A vr−1 that is generated by the reflections at the extra vertices. Let Q ṽ be the smooth generic complex quiver variety associated to ( Γ, ṽ). The Weyl group W v acts on H * c (Q ṽ, C) and hence gives a natural decomposition of the latter into isotypical components. According to  [HLRV, Cor 1.5] , after a slight renormalization, we have DT Γ,v (t 1 2 ) = i dim H 2i c (Q ṽ, C) Wv t i−d ṽ . • • • • • • • • • Figure 8.4.1. A new quiver associated to v = (3, 2, 1) 8.5. Classical Limit. We define the quantity W (A) from the classical limit of the logarithm of the DT series. We then relate it to the superpotential à la Aganagic-Vafa, through framing duality. Comparing (8.4.1) with (6.4.2), we have W (A) (X) := lim DT A (t 1/2 ; X 1 , ..., tX i , ..., X n ) DT A (t 1/2 ; X 1 , ..., X i , ..., X n ) . By  [KS, Theorem 5.3] , Y 1 , ..., Y n are solutions to the system of equations X i (−Y i ) 1−a ii j =i Y −a ij j + Y i = 1, i = 1, ..., n. Recalling that framing duality equates q with −t 1 2 , we should compare this equation with the q → −1 limit of the wavefunction of the genus-g canoe. Upon setting X i = U i and Y i = V i , we recognize this as the q → −1 limit of the genus-g canoe wavefunction of Equation (6.2.4), after changing frame using Equation (3.3.1) with change-of-frame matrix Ω = 1 − A. The classical limit has a geometric interpretation in terms of moduli spaces as well, which we now discuss. The canoe graph Γ canoe g is an iterated g-fold blow-up of the Θ-graph with two nodes and three edges: Γ canoe g is obtained from blowing up Γ canoe g−1 at either of the two vertices at the ends of the canoe. Recall from  [TZ, Section 5.2 ] that if Γ is the blow-up of Γ at a vertex, then the moduli of objects of the corresponding sheaf categories are related by M Γ = H × M Γ , where H is the pair of pants P 1 \\ {0, 1, ∞}. So since M Θ is a point, M Γ canoe g ∼ = H g , i.e. g copies of the tetrahedron moduli space. After choosing a framing, we can define compatible coordinates on the torus in which M Γ canoe g lives, then lifting from (C × ) 2g to its half-universal cover T * (C × ) g , we can write M Γ as the graph of the differential of a superpotential. ∈ Z, and for fixed degree d only finitely many of these integers n function of all-genus open Gromov-Witten invariants. (See Conjecture 6.8 for details.) \n vanishes by the Steinberg relations. Combined with the other relation (ab) ∧ c = a ∧ c + b ∧ c, it follows that X ∧ Y restricts to zero. The general case is proven similarly. \n Figure 1.5.1. Mutating the necklace graph Γ g neck (left) along the g short strands results in the canoe graph Γ g canoe (right). Here g = 5. The Legendrian surfaces generalize the Chekanov and Clifford tori, respectively, which arise when g = 1. \n Figure 2 2 Figure 2.5.1. A framed local system on a sphere with 4 punctures. \n Figure 4 4 Figure 4.1.1. The front projection of S Γ near a vertex. \n which is an algebraic torus equipped with an algebraic symplectic form coming from the intersection pairing on H 1 (S Γ ) [TZ,   §4.6]. More precisely, let B be the set of branch points of S Γ , corresponding to the vertices of Γ. The period domain P Γ is the moduli space that parametrizes flat line bundles over S Γ \\B such that the monodromies surrounding the branch points are −1. Note that H 1 (S Γ , C * ) can be identified with the moduli space of flat line bundles over S Γ . It acts on P Γ by taking the tensor product of corresponding line bundles, and this action equips P Γ with the structure of an H 1 (S Γ , C * ) torsor. (3) A moduli space M Γ of microlocal-rank-one constructible sheaves on R 3 , whose singular support lies in S Γ [TZ, §4.3]. More concretely, M Γ is the space of PGL 2 -equivalence classes of P 1 -colorings of the faces of Γ. (4) A Lagrangian microlocal monodromy map M Γ → P Γ [TZ, §4.7]. It can be described as follows. Every edge e of Γ connects branch points and therefore defines an element of H 1 (S Γ ). It gives rise to a character x e : P Γ → C * by the canonical pairing between H 1 and H 1 . The sum of edges surrounding a face f is a trivial cycle in H 1 , so e∈∂f x e = 1. The map M Γ → P Γ is defined by setting x e to be the cross ratio (4.1.1) x e = − a − b b − c • c − d d − a where a, b, c, d ∈ P 1 = C ∪ {∞} are the colors of faces surrounding an edge e in the following pattern: One easily verifies the relations e∈∂f x e = 1. We exhibit defining equations for M Γ . The characters x e generate the coordinate ring of P Γ , obeying the relation (4.1.2) e x e = (−1) g+1 and further the equation (4.1.3) x e 1 x e 2 • • • x en = 1. \n Figure 5 5 Figure 5.1.1. The necklace graph Γ neck g , pictured in blue, and its foam filling. At left is a local model near a bead, with tangle strand in red. In the middle is the foam filling for g = 1. The Ubbi toy at right is something close to a foam for Γ neck 4 . \n Figure 5 5 Figure 5.1.2. Left: the Harvey-Lawson foam, with its four edges but just two of the 4 2 = 6 faces drawn. Right: the deformed foam, with arc in green, and the two deformed faces drawn. \n Figure 5 5 Figure 5.1.3. At left is the foam of the Harvey-Lawson Lagrangian, with all 6 = 4 2 \n Figure5.1.4. The neighborhood of an arc (the green line segment) and its lift to the Lagrangian (green oval). Four sheets, forming two surfaces (gray) meet at the arc. The cross-sectional planes are shown, along with the sign of the primitive function f on L. The red dots are where the sheets s = 0, π meet the arc, so the sign of f changes as they are crossed. The purple dot is the cross section of the oriented loop γ a -see Definition 5.9. \n a) • a ∼ F 0, for each face F of F . 5.4. Face relations for foams. On general grounds, L with ∂L = S Γ and b 1 (L) = g = 1 2 b 1 (S Γ ) defines a phase as the kernel of the surjection τ : H 1 (S Γ ) \n Figure 5 5 Figure 5.4.1. Attaching a Harvey-Lawson foam to F in a neighborhood f of a vertex, shaded in brown. (Dually, f is a face of the dual triangulation to Γ F .)F ∆ respectively, so that we have L = L ∪ π −1 ( f ) HL , where f is the neighborhood of the vertex v where we attached F ∆ , i.e. the face of along which the dual tetrahedron ∆ was glued. This gluing is illustrated in Figure5.4.2. \n Figure 5 5 Figure 5.4.3. The cubic graph (shown solid in blue) produced by gluing a Harvey-Lawson foam an edge is related to the orignal (blue, dotted) by a diagonal exchange. The Harvey-Lawson dual tetrahedron is shown in red. \n Figure 5 5 Figure 5.4.4. Shaded in brown is the subset π −1 (σ) of ∂HL = T 2 . Edges of the cubic graph are shown in blue, and those of the dual triangulation in black.Since the opposite pairs of blue boundary edges are identified, the space π −1 (σ) is homeomorphic to a cylinder. \n : if the two faces above are part of a tetrahedron T , then = \\ T. Otherwise, let T be the tetrahedron with two faces as pictured above and the other two ACD and BCD, and set = ∪ T, In both cases, the geometry of the quadrilateral ABCD at the boundary of is so constructed F e . It remains to describe how to deform F e to F e,± . \n Figure 6 6 Figure 6.2.2. The framed seed µ + 4 (i 2 ) i 0 . \n σ (−1,...,−1) • T −A to i canoe . Then by (3.3.2) we have \n i i in the ring of power series C[[{X i }]] has exponent d lying in Z g ≥0 ∼ = C ⊂ H 1 (L). Each such d determines an open Gromov-Witten problem of counting holomorphic maps from Riemann surfaces with one boundary component mapping to the pair (C 3 , L), such that the image of the boundary lies in homology class d. \n Figure 7 7 Figure 7.1.3. The framed seed i 2 = σ(i 1 ) for the canoe graph. \n u \n Figure 7 7 Figure 7.2.1. The framed seed i 2 \n Figure 7 7 Figure 7.2.2. The framed seed i 3 \n Figure 7.2.3. The cubic graph of genus 3 given by the 1-skeleton of the cube. \n i cube : e 1 → u 2 − v 2 − 3c e 2 → u 2 + c e 3 → v 2 − c e 4 → c − u 1 − u 2 e 5 → c − u 1 − u 2 e 6 → v 3 − c e 7 → u 3 + c e 8 → u 1 − v 3 − 3c e 9 → v 1 − c e 10 → u 3 + v 2 − v 1 + 3c e 11 → v 1 − v 2 − u 2 − u 3 − 3c e 12 → u 2 + v 3 − v 1 + c \n\t\t\t Within this paper, we only take into account the birational structure of X . \n\t\t\t In this section, homology will be taken with Z coefficients unless otherwise stated. \n\t\t\t In[I]  a similar definition of framing is made, but without the isotropic condition. \n\t\t\t We thank Jake Solomon and Sara Tukachinski for explaining this perspective. See also[I]  for a more general definition of framings. \n\t\t\t As we are working on the Riemann sphere S 2 , there is no difference between placing sg+1 in the upper or lower half-plane, so the necklaces of Figures 1.5.1 and 4.3.1 are in fact consistent.', acknowledgement=None, annex='Recall the logarithm log : 1 + m −→ m, log(1 + f ) := k≥1 (−1) k f k . Lemma 6.12. For each admissible series F , we have Proof. By the third formula of (2.2.5), we have (6.4.2) lim q→−1 The rest is clear. The property of admissibility is clearly preserved under the action of the coordinate rescaling automorphisms σ δ introduced in (3.2.1). In  [KS] , Kontsevich and Soibelman proved that admissibility is also preserved under another, much less trivial family of automorphisms: the changes of framing. Theorem 6.13 ([  KS, Th.6 .1]). A power series F ∈ O is admissible if and only if T Ω •F is admissible for all integral symmetric matrices Ω. The integrality of the wavefunction defined in Theorem 6.1 now follows from the following Proposition. Proposition 6.14. Suppose that the mutation a is both admissible and primitive, and that F ∈ O is an admissible formal power series. Then the power series Φ a • F is also admissible. Proof. Since the exponent vector m in (3.4.1) is primitive we can choose a basis for Z g containing m as one of its elements. Hence (cf. Remark 3.2) we may reduce to proving the Lemma in the case that ι f (e k ) = (−q) r g j=1 U 1 V n j j . In this case, let Ω be any symmetric matrix whose first column is −n = (−n 1 , . . . , −n g ). By Theorem 6.13, it suffices to show T Ω • Φ a • F is admissible, so we compute admissible by Theorem 6.13, and so is Φ ((−q) r U 1 ) k by the last formula in (2.2.5). Since the product of two admissible series is clearly admissible, this implies that Φ a • F is also admissible, thereby proving the Lemma. \n Towards an analytic wavefunction In this section we will discuss the problem of promoting the algebraic construction of the wavefunction from Section 6 to an analytic one. Doing this in general would necessitate extending the theory of representations of quantum cluster varieties beyond the "principal series", a task we do not take on in the present work. Nonetheless, we will present several examples which we believe provide nontrivial evidence for the existence of a well-defined analytic wavefunction associated to a smoothed ideal foam. Let us first recall some of the elements of the theory of unitary representations of quantum cluster varieties as developed in  [FG2] . A representation of a quantum cluster variety is, by definition, a functor G X → Hilb from the cluster modular groupoid to the category of Hilbert spaces with morphisms given by unitary isomorphisms. The representations constructed by Fock and Goncharov depend on a quantization parameter ∈ R. To each object i of G X is associated a pair of quantum tori T q and T q ∨ , generated respectively by {e 2π x k } and {e 2π −1 x k }, where {x k } are the logarithmic edge a i and lower hemisphere b i , in the upper and lower half-planes, respectively. We parametrize the edge variables X e i with the quantum torus D 2g as follows: It is straightforward to check that this assignment satisfies Equations (4.3.1) and (4.3.2). The quantized chromatic Lagrangian is the ideal defined by the face relations of Equation (4.3.3). These impose V i = 1, and nothing further, giving rise to the wavefunction The canoe Γ g canoe is obtained by performing g positive mutations at strands 1, ..., g, similar to Examples 4.4 and 6.2. These mutations are all admissible and mutually commuting. The edge variable on the kth strand is −q −1 U k , so the mutation is effected by conjugation by Φ What results, then is the wavefunction where on the right we have used the power-series expression of the infinite Pochhammer symbol from Lemma 6.4. Now let A be an n × n, symmetric matrix with non-negative entries. According to Equation 3.3.2, a frame-changing transformation by A takes us to the seed i canoe g,A \n and (8.1.1) We define the Ooguri-Vafa invariants n v,k by passing to an infinite product expansion and setting Framing duality is the observation that, using the computation of Kontsevich-Soibelman  [KS] , we can identify Ψ i canoe g,A of Equation (8.1.1) as the DT series of the quiver with adjacency matrix A. This statement will be made precise after defining these terms in the next section. 8.2. DT series for symmetric quivers. Let A = (a ij ) be an n × n symmetric matrix with nonnegative integral entries and let Q A be its corresponding symmetric quiver. The generating function for the COHA H of Q A , also called the DT series, is These were computed in  [KS, §5.6 ] for symmetric quivers Q A , giving the result (8.2.2) DT A (t 1/2 , X) =: where above we have defined the coefficient functions 2 ), and once again (t We now prove a lemma to be used in the next section. First define σ(v) = n i=1 v i . Then we have: Proof. To see this, note (y v) . As a result, we have The second equation follows, and then the first. 8.3. Framing Duality. We now come to the main point of this section: to compare wavefunctions for canoe graphs with DT series of symmetric quivers. Recall Equation  8 .1.1 for the genus-g canoe graph in framing A from Section 8.1 above. Comparing the form of its wavefunction given in Equation (6.2.6) with that of Equation (8.2.2), we have the following. Proposition 8.2 (Framing Duality). For any integral g × g symmetric matrix A with non-negative entries, the wavefunction Ψ i canoe g,A associated to the framed seed i canoe g,A of the genus-g canoe graph coincides with the DT series for the symmetric quiver with adjacency matrix A under the identification q = −t Proof. Comparing Equations (8.2.2) and (8.1.1), the proposition follows from Lemma 8.1. 8.4. Integer Invariants. We define the quiver invariants \n ∞ We can rewrite this in another form using the plethystic exponential Exp and its inverse Log. Recall that for a power series vanishing at the origin, v,k are non-negative integers and are nonzero only for finitely many k ∈ Z. In terms of integer invariants, framing duality says v,k are well-defined for non-positive A. Remark 8.3. The paper  [HLRV]  gives a cohomological interpretation of DT-invariants of quivers. Let Γ be a quiver with r vertices and let v = (v 1 , ..., v r ) ∈ Z r ≥0 be a dimension vector. Associate to (Γ, v) a new quiver Γ by attaching a leg of length v i − 1 at the vertex i. We extend the dimension vector v to ṽ by placing decreasing dimensions v i − 1, v i − 2, ..., 1 at the extra leg. Let W v be the In zero framing -i.e., the one defined by mutation from the necklace as in Example 6.2 -we have (8.5.1) Li 2 (X i ), where X i = e u i -and we will also need the conjugate logarithmic coordinates v i on the cotangent fibers. A framing shift by a g × g symmetric integral matrix A, as in Equation (3.3.1) and Remark 3.3, defines new coordinates v i = v i , u i = u i +A i,j v j . Then the lift of M Γ canoe g is cut out from P Γ canoe g in these coordinates as the graph of the associated superpotential W (A) Γ canoe g , which is the classical limit of the frame-shifted wavefunction. We write (8.5.2) where X d = g i=1 X i d i and X i = e u i , and n d,k . For g = 1 and A = (p), these integers appear (with a slightly different sign convention) in [AKV, Section 6.1]. 8.6. Kac polynomial of a quiver. We recall the Kac polynomial of a quiver. Let B be an n × n integal matrix with non-negative entries, and let Q B be the quiver with n nodes labeled 1, ..., n, and B i,j arrows between node i and node j. The Kac polynomial of Q B is defined as follows. Let d = 0 ∈ (Z ≥0 ) n be a dimension vector. Then Remark 8.4. The Kac polynomials are DT-invariants for quivers with potential. Let Q be a quiver with arrows a ij at vertex i. Let Q be the double quiver by adding arrows a * ij of opposite direction and a new loop c i for each vertex. Take the quiver potential The Kac polynomials for Q are the DT-invariants for the quiver-with-potential ( Q, W ). In  [RV] , a refinement of the Kac polynomial was introduced, in which the label is not simply a counting number d but a partition λ. Then A d (q) = |λ|=d A λ (q). We will be interested in the special case λ = 1 d = (1, 1, ..., 1). Proposition 8.5. Let h ≥ 1 and let B = (2−2h) be the one-by-one matrix with single entry 2−2h, considered as a framing of the genus-one canoe. Let Q be the quiver with one node and h arrows. d . Proof. Consider first the case when h = 1, so B = 0. Then the quiver has no arrows and there is a unique irreducible representation for each d, thus A d (q) = 1 and it is shown in  [RV]  that this corresponds to the partition (d). When d = 1 this equals 1 d , but not otherwise -so we require n (0) d = δ d,1 , which agrees with Equation (8.5.1) when g = 1. More generally, we refer to Equation (4.3.1) and Proposition 4.2.1 of  [RV] , where the notations V, x, z, and N are here W, X, e v , and g, respectively. In our notation, Equation (4.3.1) says dW d . Comparison with Equation (4.3.1) of  [RV]  gives n \n (B) d = A 1 d (1), as claimed. We note that in  [RV]  the function W (X) is called Schläfli\'s differential by analogy with the volume of hyperbolic polyhedra, which is part of a dual superpotential computation in  [DGGo] . Consider first the quiver with one node and two arrows. The polynomials A 1 d (q) are listed in  [H, Appendix II]  for d = 1, 2, 3, 4, giving A 1 d (1) = 1, 1, 3, 10, respectively. On the other hand, we may compare these integers with the disk invariants for framing p = 2 − 2 • 2 = −2 obtained in the formulas of  [AKV, Section 6 .1] after their Equation  6 .4, where they find n (2) d = 1, 1, 3, 10 for these same values of d. For another class of examples, consider the quiver with one node and g > 1 arrows. The polynomials A 1 2 (q) were computed in  [H, Section 3] , giving A 1 2 (1) = g − 1. This agrees with n (2−2g) 2 as computed in  [AKV] . For the reader\'s convenience, we record the following table of the integers A 1 d (1) for the quiver with a single vertex and h loops, which by Proposition 8.5 coincide with n (2−2h) d : Remark 8.7. It is a conjecture of Hausel and Rodriguez Villegas  [HRV, Remark 4.4.6 ] that for the one-node quiver with h arrows, we have that A (h) d (1) is the dimension of the middle cohomology of the twisted GL d -character variety M h of a genus-h surface. Given that A d (1) = |λ|=d A λ (1), it would be interesting to find a relationship between other refined Kac polynomials and invariants of topological strings  [LMV] . Curiously, such results for various genera h would correspond to different framings of the same genus-one Legendrian surface. \n Appendix A. Non-compact quantum dilogarithms In this appendix, we recall some important properties of the non-compact quantum dilogarithm that we use in the paper. For further background and details regarding this function, we refer the reader to  [FKV, Ka, V] . We assume that ∈ C is such that + −1 ∈ R, and lies in the first quadrant ( ) > 0, ( ) ≥ 0. Let us also write A.1. The non-compact quantum dilogarithm. Definition A.1. Let C be the contour going along the real line from −∞ to +∞, surpassing the origin in a small semi-circle from above. The non-compact quantum dilogarithm function ϕ (z) is defined in the strip | (z)| < c by the following formula  [Ka] : The non-compact quantum dilogarithm can be analytically continued to the entire complex plane as a meromorphic function with an essential singularity at infinity. The resulting function ϕ (z) enjoys the following properties  [Ka] : Relation with the compact quantum dilogarithm: For ( 2 ) > 0, setting q = e −πi −2 we have ϕ (z) = (e 2πb(z+c ) ; q 2 ) ∞ (e 2πb −1 (z−c ) ; q 2 ) ∞ . Poles and zeros: Behavior around poles and zeros: Asymptotic behavior: | arg(z)| > π 2 + arg( ); while we have the following asymptotic behaviour as → 0: Inversion formula: Functional equations: In what follows we will drop the subscript from the notation for the quantum dilogarithm, and simply write ϕ(z). A.2. Integral identites for ϕ(z). The quantum dilogarithm function ϕ(z) satisfies many important integral identities. Before describing some of them, let us fix a useful convention. Remark A.2. We will often consider contour integrals of the form where f (t) is some entire function. Unless otherwise specified, the contour C in such an integral is always chosen to be passing below the poles of ϕ(t − a j ) for all j, above the poles of ϕ(t − b k ) −1 for all k, and escaping to infinity in such a way that the integrand is rapidly decaying. The Fourier transform of the quantum dilogarithm can be calculated explicitly by the following integrals: Each of these integral evaluations is equivalent to the non-commutative pentagon identity for ϕfor further details, see  [FKV] .')
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Prafulla Dhariwal', given_name='Prafulla', middle_name=None, surname='Dhariwal', email=None, orcid=None, affiliation=GrobidAffiliation(institution=None, department='Equal contribution 1 OpenAI', laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement='San Francisco', country=None))), GrobidAuthor(full_name='Heewoo Jun', given_name='Heewoo', middle_name=None, surname='Jun', email=None, orcid=None, affiliation=GrobidAffiliation(institution=None, department='Equal contribution 1 OpenAI', laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement='San Francisco', country=None))), GrobidAuthor(full_name='Christine Payne', given_name='Christine', middle_name=None, surname='Payne', email=None, orcid=None, affiliation=GrobidAffiliation(institution=None, department='Equal contribution 1 OpenAI', laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement='San Francisco', country=None))), GrobidAuthor(full_name='Jong Wook Kim', given_name='Jong', middle_name='Wook', surname='Kim', email=None, orcid=None, affiliation=GrobidAffiliation(institution=None, department='Equal contribution 1 OpenAI', laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement='San Francisco', country=None))), GrobidAuthor(full_name='Alec Radford', given_name='Alec', middle_name=None, surname='Radford', email=None, orcid=None, affiliation=GrobidAffiliation(institution=None, department='Equal contribution 1 OpenAI', laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement='San Francisco', country=None))), GrobidAuthor(full_name='Ilya Sutskever', given_name='Ilya', middle_name=None, surname='Sutskever', email=None, orcid=None, affiliation=GrobidAffiliation(institution=None, department='Equal contribution 1 OpenAI', laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement='San Francisco', country=None))), GrobidAuthor(full_name='Lookup Codebook Codebook', given_name='Lookup', middle_name='Codebook', surname='Codebook', email=None, orcid=None, affiliation=GrobidAffiliation(institution=None, department='Equal contribution 1 OpenAI', laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement='San Francisco', country=None))), GrobidAuthor(full_name='Lookup', given_name=None, middle_name=None, surname='Lookup', email=None, orcid=None, affiliation=GrobidAffiliation(institution=None, department='Equal contribution 1 OpenAI', laboratory=None, address=GrobidAddress(addr_line=None, post_code=None, settlement='San Francisco', country=None)))], index=None, id=None, unstructured=None, date=None, title='Jukebox: A Generative Model for Music', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), pdf_md5='783CDCCC38136015C54128EABD7D612F', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='K Akuzawa', given_name='K', middle_name=None, surname='Akuzawa', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Iwasawa', given_name='Y', middle_name=None, surname='Iwasawa', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Matsuo', given_name='Y', middle_name=None, surname='Matsuo', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date='2018', title='Expressive speech synthesis via modeling expressions with variational autoencoder', book_title='INTERSPEECH', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Anders', given_name='T', middle_name=None, surname='Anders', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E R ; Miranda', given_name='E', middle_name='R ;', surname='Miranda', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Li', given_name='X', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Miller', given_name='J', middle_name=None, surname='Miller', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Ng', given_name='A', middle_name=None, surname='Ng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Raiman', given_name='J', middle_name=None, surname='Raiman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Sengupta', given_name='S', middle_name=None, surname='Sengupta', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Shoeybi', given_name='M', middle_name=None, surname='Shoeybi', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='2011', title='Constraint programming systems for modeling music theories and composition', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='43', issue=None, pages='195-204', first_page='195', last_page='204', note='Deep Voice: Real-time neural text-to-speech', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Ö Arık', given_name='S', middle_name='Ö', surname='Arık', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Chen', given_name='J', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Peng', given_name='K', middle_name=None, surname='Peng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Ping', given_name='W', middle_name=None, surname='Ping', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Zhou', given_name='Y', middle_name=None, surname='Zhou', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date=None, title='Neural voice cloning with a few samples', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2018', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Ö Arık', given_name='S', middle_name='Ö', surname='Arık', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Jun', given_name='H', middle_name=None, surname='Jun', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Diamos', given_name='G', middle_name=None, surname='Diamos', email=None, orcid=None, affiliation=None)], index=3, id='b3', unstructured=None, date='2018', title='Fast spectrogram inversion using multi-head convolutional neural networks', book_title=None, series_title=None, editors=None, journal='IEEE Signal Processing Letters', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='26', issue='1', pages='94-98', first_page='94', last_page='98', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J L Ba', given_name='J', middle_name='L', surname='Ba', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J R Kiros', given_name='J', middle_name='R', surname='Kiros', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G E Hinton', given_name='G', middle_name='E', surname='Hinton', email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='2016', title='Layer normalization', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1607.06450', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C Berner', given_name='C', middle_name=None, surname='Berner', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Brockman', given_name='G', middle_name=None, surname='Brockman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Chan', given_name='B', middle_name=None, surname='Chan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Cheung', given_name='V', middle_name=None, surname='Cheung', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Dębiak', given_name='P', middle_name=None, surname='Dębiak', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Dennison', given_name='C', middle_name=None, surname='Dennison', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Farhi', given_name='D', middle_name=None, surname='Farhi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Q Fischer', given_name='Q', middle_name=None, surname='Fischer', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Hashme', given_name='S', middle_name=None, surname='Hashme', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Hesse', given_name='C', middle_name=None, surname='Hesse', email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date='2019', title='Dota 2 with large scale deep reinforcement learning', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1912.06680', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Beyls', given_name='P', middle_name=None, surname='Beyls', email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date='1989', title='The musical universe of cellular automata', book_title='International Computer Music Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='34-41', first_page='34', last_page='41', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Blaauw', given_name='M', middle_name=None, surname='Blaauw', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Bonada', given_name='J', middle_name=None, surname='Bonada', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date='2017', title='A neural parametric singing synthesizer', book_title='INTERSPEECH', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Bonada', given_name='J', middle_name=None, surname='Bonada', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Serra', given_name='X', middle_name=None, surname='Serra', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date='2007', title='Synthesis of the singing voice by performance sampling and spectral models', book_title=None, series_title=None, editors=None, journal='IEEE signal processing magazine', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='24', issue='2', pages='67-79', first_page='67', last_page='79', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Brock', given_name='A', middle_name=None, surname='Brock', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Donahue', given_name='J', middle_name=None, surname='Donahue', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Simonyan', given_name='K', middle_name=None, surname='Simonyan', email=None, orcid=None, affiliation=None)], index=9, id='b9', unstructured=None, date='2019', title='Large scale GAN training for high fidelity natural image synthesis', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G Brunner', given_name='G', middle_name=None, surname='Brunner', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Konrad', given_name='A', middle_name=None, surname='Konrad', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wang', given_name='Y', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Wattenhofer', given_name='R', middle_name=None, surname='Wattenhofer', email=None, orcid=None, affiliation=None)], index=10, id='b10', unstructured=None, date='2018', title='MIDI-VAE: modeling dynamics and instrumentation of music with applications to style transfer', book_title='International Society for Music Information Retrieval Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='747-754', first_page='747', last_page='754', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Child', given_name='R', middle_name=None, surname='Child', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Gray', given_name='S', middle_name=None, surname='Gray', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Radford', given_name='A', middle_name=None, surname='Radford', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Sutskever', given_name='I', middle_name=None, surname='Sutskever', email=None, orcid=None, affiliation=None)], index=11, id='b11', unstructured=None, date='2019', title='Generating long sequences with sparse transformers', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1904.10509', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J De Fauw', given_name='J', middle_name=None, surname='De Fauw', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Dieleman', given_name='S', middle_name=None, surname='Dieleman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Simonyan', given_name='K', middle_name=None, surname='Simonyan', email=None, orcid=None, affiliation=None)], index=12, id='b12', unstructured=None, date='2019', title='Hierarchical autoregressive image models with auxiliary decoders', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1903.04933', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Dehak', given_name='N', middle_name=None, surname='Dehak', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P J Kenny', given_name='P', middle_name='J', surname='Kenny', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Dehak', given_name='R', middle_name=None, surname='Dehak', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Dumouchel', given_name='P', middle_name=None, surname='Dumouchel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Ouellet', given_name='P', middle_name=None, surname='Ouellet', email=None, orcid=None, affiliation=None)], index=13, id='b13', unstructured=None, date='2011', title='Front-end factor analysis for speaker verification', book_title=None, series_title=None, editors=None, journal='IEEE Transactions on Audio, Speech, and Language Processing', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='19', issue='4', pages='788-798', first_page='788', last_page='798', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Dieleman', given_name='S', middle_name=None, surname='Dieleman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Van Den Oord', given_name='A', middle_name=None, surname='Van Den Oord', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Simonyan', given_name='K', middle_name=None, surname='Simonyan', email=None, orcid=None, affiliation=None)], index=14, id='b14', unstructured=None, date='2018', title='The challenge of realistic music generation: modelling raw audio at scale', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='7989-7999', first_page='7989', last_page='7999', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L Dinh', given_name='L', middle_name=None, surname='Dinh', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Krueger', given_name='D', middle_name=None, surname='Krueger', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Nice', given_name=None, middle_name=None, surname='Nice', email=None, orcid=None, affiliation=None)], index=15, id='b15', unstructured=None, date='2015', title='Non-linear independent components estimation', book_title='International Conference in Learning Representations, Workshop', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L Dinh', given_name='L', middle_name=None, surname='Dinh', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Sohl-Dickstein', given_name='J', middle_name=None, surname='Sohl-Dickstein', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Bengio', given_name='S', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None)], index=16, id='b16', unstructured=None, date='2017', title='Density estimation using Real NVP', book_title='International Conference in Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C Donahue', given_name='C', middle_name=None, surname='Donahue', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H H Mao', given_name='H', middle_name='H', surname='Mao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y E Li', given_name='Y', middle_name='E', surname='Li', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G W Cottrell', given_name='G', middle_name='W', surname='Cottrell', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J J Mcauley', given_name='J', middle_name='J', surname='Mcauley', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Lakhnes', given_name=None, middle_name=None, surname='Lakhnes', email=None, orcid=None, affiliation=None)], index=17, id='b17', unstructured=None, date='2019', title='Improving multi-instrumental music generation with cross-domain pre-training', book_title='ternational Society for Music Information Retrieval Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='685-692', first_page='685', last_page='692', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H.-W Dong', given_name='H.-W', middle_name=None, surname='Dong', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W.-Y Hsiao', given_name='W.-Y', middle_name=None, surname='Hsiao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L.-C Yang', given_name='L.-C', middle_name=None, surname='Yang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y.-H Yang', given_name='Y.-H', middle_name=None, surname='Yang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Musegan', given_name=None, middle_name=None, surname='Musegan', email=None, orcid=None, affiliation=None)], index=18, id='b18', unstructured=None, date='2018', title='Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment', book_title='Thirty-Second AAAI Conference on Artificial Intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Engel', given_name='J', middle_name=None, surname='Engel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Resnick', given_name='C', middle_name=None, surname='Resnick', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Roberts', given_name='A', middle_name=None, surname='Roberts', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Dieleman', given_name='S', middle_name=None, surname='Dieleman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Norouzi', given_name='M', middle_name=None, surname='Norouzi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Eck', given_name='D', middle_name=None, surname='Eck', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Simonyan', given_name='K', middle_name=None, surname='Simonyan', email=None, orcid=None, affiliation=None)], index=19, id='b19', unstructured=None, date='2017', title='Neural audio synthesis of musical notes with wavenet autoencoders', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1068-1077', first_page='1068', last_page='1077', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Engel', given_name='J', middle_name=None, surname='Engel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K K Agrawal', given_name='K', middle_name='K', surname='Agrawal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Chen', given_name='S', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Gulrajani', given_name='I', middle_name=None, surname='Gulrajani', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Donahue', given_name='C', middle_name=None, surname='Donahue', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Roberts', given_name='A', middle_name=None, surname='Roberts', email=None, orcid=None, affiliation=None)], index=20, id='b20', unstructured=None, date='2019', title='GANSynth: Adversarial neural audio synthesis', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Gibiansky', given_name='A', middle_name=None, surname='Gibiansky', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Ö Arık', given_name='S', middle_name='Ö', surname='Arık', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Diamos', given_name='G', middle_name=None, surname='Diamos', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Miller', given_name='J', middle_name=None, surname='Miller', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Peng', given_name='K', middle_name=None, surname='Peng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Ping', given_name='W', middle_name=None, surname='Ping', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Raiman', given_name='J', middle_name=None, surname='Raiman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Zhou', given_name='Y', middle_name=None, surname='Zhou', email=None, orcid=None, affiliation=None)], index=21, id='b21', unstructured=None, date='2017', title='Deep Voice 2: Multispeaker neural text-to-speech', book_title='Advances in neural information processing systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2962-2970', first_page='2962', last_page='2970', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='I Goodfellow', given_name='I', middle_name=None, surname='Goodfellow', email=None, orcid=None, affiliation=None)], index=22, id='b22', unstructured=None, date='2016', title='NIPS 2016 tutorial: Generative adversarial networks', book_title='Neural Information Processing Systems, Tutorial', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='I Goodfellow', given_name='I', middle_name=None, surname='Goodfellow', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Pouget-Abadie', given_name='J', middle_name=None, surname='Pouget-Abadie', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Mirza', given_name='M', middle_name=None, surname='Mirza', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Xu', given_name='B', middle_name=None, surname='Xu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Warde-Farley', given_name='D', middle_name=None, surname='Warde-Farley', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Ozair', given_name='S', middle_name=None, surname='Ozair', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Courville', given_name='A', middle_name=None, surname='Courville', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None)], index=23, id='b23', unstructured=None, date='2014', title='Generative adversarial nets', book_title='Advances in neural information processing systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2672-2680', first_page='2672', last_page='2680', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C Gupta', given_name='C', middle_name=None, surname='Gupta', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Yılmaz', given_name='E', middle_name=None, surname='Yılmaz', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Li', given_name='H', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None)], index=24, id='b24', unstructured=None, date='2020', title='Automatic lyrics transcription in polyphonic music: Does background music help?', book_title='International Conference on Acoustics, Speech, and Signal Processing', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G Hadjeres', given_name='G', middle_name=None, surname='Hadjeres', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Pachet', given_name='F', middle_name=None, surname='Pachet', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Nielsen', given_name='F', middle_name=None, surname='Nielsen', email=None, orcid=None, affiliation=None)], index=25, id='b25', unstructured=None, date='2017', title='Deepbach: a steerable model for bach chorales generation', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='JMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1362-1371', first_page='1362', last_page='1371', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Ho', given_name='J', middle_name=None, surname='Ho', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Kalchbrenner', given_name='N', middle_name=None, surname='Kalchbrenner', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Weissenborn', given_name='D', middle_name=None, surname='Weissenborn', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Salimans', given_name='T', middle_name=None, surname='Salimans', email=None, orcid=None, affiliation=None)], index=26, id='b26', unstructured=None, date='2019', title='Axial attention in multidimensional transformers', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1912.12180', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W.-N Hsu', given_name='W.-N', middle_name=None, surname='Hsu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Zhang', given_name='Y', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R J Weiss', given_name='R', middle_name='J', surname='Weiss', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Zen', given_name='H', middle_name=None, surname='Zen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wu', given_name='Y', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wang', given_name='Y', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Cao', given_name='Y', middle_name=None, surname='Cao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Jia', given_name='Y', middle_name=None, surname='Jia', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Chen', given_name='Z', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Shen', given_name='J', middle_name=None, surname='Shen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Nguyen', given_name='P', middle_name=None, surname='Nguyen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Pang', given_name='R', middle_name=None, surname='Pang', email=None, orcid=None, affiliation=None)], index=27, id='b27', unstructured=None, date='2019', title='Hierarchical generative modeling for controllable speech synthesis', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C A Huang', given_name='C', middle_name='A', surname='Huang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Cooijmans', given_name='T', middle_name=None, surname='Cooijmans', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Roberts', given_name='A', middle_name=None, surname='Roberts', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A C Courville', given_name='A', middle_name='C', surname='Courville', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Eck', given_name='D', middle_name=None, surname='Eck', email=None, orcid=None, affiliation=None)], index=28, id='b28', unstructured=None, date='2017', title='Counterpoint by convolution', book_title='International Society for Music Information Retrieval Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='211-218', first_page='211', last_page='218', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C.-Z A Huang', given_name='C.-Z', middle_name='A', surname='Huang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Vaswani', given_name='A', middle_name=None, surname='Vaswani', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Uszkoreit', given_name='J', middle_name=None, surname='Uszkoreit', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Shazeer', given_name='N', middle_name=None, surname='Shazeer', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Simon', given_name='I', middle_name=None, surname='Simon', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Hawthorne', given_name='C', middle_name=None, surname='Hawthorne', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A M Dai', given_name='A', middle_name='M', surname='Dai', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M D Hoffman', given_name='M', middle_name='D', surname='Hoffman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Dinculescu', given_name='M', middle_name=None, surname='Dinculescu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Eck', given_name='D', middle_name=None, surname='Eck', email=None, orcid=None, affiliation=None)], index=29, id='b29', unstructured=None, date='2019', title='Music Transformer: Generating music with long-term structure', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Y Huang', given_name='Y', middle_name=None, surname='Huang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Cheng', given_name='Y', middle_name=None, surname='Cheng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Bapna', given_name='A', middle_name=None, surname='Bapna', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Firat', given_name='O', middle_name=None, surname='Firat', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Chen', given_name='D', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Chen', given_name='M', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Lee', given_name='H', middle_name=None, surname='Lee', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Ngiam', given_name='J', middle_name=None, surname='Ngiam', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Q V Le', given_name='Q', middle_name='V', surname='Le', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wu', given_name='Y', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None)], index=30, id='b30', unstructured=None, date='2019', title='Gpipe: Efficient training of giant neural networks using pipeline parallelism', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='103-112', first_page='103', last_page='112', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A J Hunt', given_name='A', middle_name='J', surname='Hunt', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A W Black', given_name='A', middle_name='W', surname='Black', email=None, orcid=None, affiliation=None)], index=31, id='b31', unstructured=None, date='1996', title='Unit selection in a concatenative speech synthesis system using a large speech database', book_title='IEEE International Conference on Acoustics, Speech, and Signal Processing Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='373-376', first_page='373', last_page='376', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Jehan', given_name='T', middle_name=None, surname='Jehan', email=None, orcid=None, affiliation=None)], index=32, id='b32', unstructured=None, date='2005', title='Creating music by listening', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution='Massachusetts Institute of Technology, School of Architecture and Planning, Program in Media Arts and Sciences', issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Y Jia', given_name='Y', middle_name=None, surname='Jia', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Zhang', given_name='Y', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Weiss', given_name='R', middle_name=None, surname='Weiss', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Q Wang', given_name='Q', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Shen', given_name='J', middle_name=None, surname='Shen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Ren', given_name='F', middle_name=None, surname='Ren', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Chen', given_name='Z', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Nguyen', given_name='P', middle_name=None, surname='Nguyen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Pang', given_name='R', middle_name=None, surname='Pang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Lopez Moreno', given_name='I', middle_name=None, surname='Lopez Moreno', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wu', given_name='Y', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None)], index=33, id='b33', unstructured=None, date='2018', title='Transfer learning from speaker verification to multispeaker text-to-speech synthesis', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='4480-4490', first_page='4480', last_page='4490', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Kalchbrenner', given_name='N', middle_name=None, surname='Kalchbrenner', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Elsen', given_name='E', middle_name=None, surname='Elsen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Simonyan', given_name='K', middle_name=None, surname='Simonyan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Noury', given_name='S', middle_name=None, surname='Noury', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Casagrande', given_name='N', middle_name=None, surname='Casagrande', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Lockhart', given_name='E', middle_name=None, surname='Lockhart', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Stimberg', given_name='F', middle_name=None, surname='Stimberg', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Oord', given_name='A', middle_name=None, surname='Oord', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Dieleman', given_name='S', middle_name=None, surname='Dieleman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Kavukcuoglu', given_name='K', middle_name=None, surname='Kavukcuoglu', email=None, orcid=None, affiliation=None)], index=34, id='b34', unstructured=None, date='2018', title='Efficient neural audio synthesis', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2410-2419', first_page='2410', last_page='2419', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J W Kim', given_name='J', middle_name='W', surname='Kim', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Bittner', given_name='R', middle_name=None, surname='Bittner', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Kumar', given_name='A', middle_name=None, surname='Kumar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J P Bello', given_name='J', middle_name='P', surname='Bello', email=None, orcid=None, affiliation=None)], index=35, id='b35', unstructured=None, date='2019', title='Neural music synthesis for flexible timbre control', book_title='IEEE International Conference on Acoustics, Speech and Signal Processing', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='176-180', first_page='176', last_page='180', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D P Kingma', given_name='D', middle_name='P', surname='Kingma', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Dhariwal', given_name='P', middle_name=None, surname='Dhariwal', email=None, orcid=None, affiliation=None)], index=36, id='b36', unstructured=None, date='2018', title='Glow: Generative flow with invertible 1x1 convolutions', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='10215-10224', first_page='10215', last_page='10224', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D P Kingma', given_name='D', middle_name='P', surname='Kingma', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Welling', given_name='M', middle_name=None, surname='Welling', email=None, orcid=None, affiliation=None)], index=37, id='b37', unstructured=None, date='2014', title='Auto-encoding variational bayes', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D P Kingma', given_name='D', middle_name='P', surname='Kingma', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Salimans', given_name='T', middle_name=None, surname='Salimans', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Jozefowicz', given_name='R', middle_name=None, surname='Jozefowicz', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Chen', given_name='X', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Sutskever', given_name='I', middle_name=None, surname='Sutskever', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Welling', given_name='M', middle_name=None, surname='Welling', email=None, orcid=None, affiliation=None)], index=38, id='b38', unstructured=None, date='2016', title='Improved variational inference with inverse autoregressive flow', book_title='Advances in neural information processing systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='4743-4751', first_page='4743', last_page='4751', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D H Klatt', given_name='D', middle_name='H', surname='Klatt', email=None, orcid=None, affiliation=None)], index=39, id='b39', unstructured=None, date='1980', title='Software for a cascade/parallel formant synthesizer', book_title=None, series_title=None, editors=None, journal='Journal of the Acoustical Society of America', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='67', issue='3', pages='971-995', first_page='971', last_page='995', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Kumar', given_name='K', middle_name=None, surname='Kumar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Kumar', given_name='R', middle_name=None, surname='Kumar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T De Boissiere', given_name='T', middle_name=None, surname='De Boissiere', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Gestin', given_name='L', middle_name=None, surname='Gestin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Z Teoh', given_name='W', middle_name='Z', surname='Teoh', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Sotelo', given_name='J', middle_name=None, surname='Sotelo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A De Brébisson', given_name='A', middle_name=None, surname='De Brébisson', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A C Courville', given_name='A', middle_name='C', surname='Courville', email=None, orcid=None, affiliation=None)], index=40, id='b40', unstructured=None, date='2019', title='MelGAN: Generative adversarial networks for conditional waveform synthesis', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='14881-14892', first_page='14881', last_page='14892', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Mehri', given_name='S', middle_name=None, surname='Mehri', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Kumar', given_name='K', middle_name=None, surname='Kumar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Gulrajani', given_name='I', middle_name=None, surname='Gulrajani', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Kumar', given_name='R', middle_name=None, surname='Kumar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Jain', given_name='S', middle_name=None, surname='Jain', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Sotelo', given_name='J', middle_name=None, surname='Sotelo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Courville', given_name='A', middle_name=None, surname='Courville', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None)], index=41, id='b41', unstructured=None, date='2017', title='SampleRNN: An unconditional end-to-end neural audio generation model', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J A Moorer', given_name='J', middle_name='A', surname='Moorer', email=None, orcid=None, affiliation=None)], index=42, id='b42', unstructured=None, date='1972', title='Music and computer composition', book_title=None, series_title=None, editors=None, journal='Communications of the ACM', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='15', issue='2', pages='104-113', first_page='104', last_page='113', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Mor', given_name='N', middle_name=None, surname='Mor', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Wolf', given_name='L', middle_name=None, surname='Wolf', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Polyak', given_name='A', middle_name=None, surname='Polyak', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Taigman', given_name='Y', middle_name=None, surname='Taigman', email=None, orcid=None, affiliation=None)], index=43, id='b43', unstructured=None, date='2019', title='Autoencoderbased music translation', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A V D Oord', given_name='A', middle_name='V D', surname='Oord', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Dieleman', given_name='S', middle_name=None, surname='Dieleman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Zen', given_name='H', middle_name=None, surname='Zen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Simonyan', given_name='K', middle_name=None, surname='Simonyan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Vinyals', given_name='O', middle_name=None, surname='Vinyals', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Graves', given_name='A', middle_name=None, surname='Graves', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Kalchbrenner', given_name='N', middle_name=None, surname='Kalchbrenner', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Senior', given_name='A', middle_name=None, surname='Senior', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Kavukcuoglu', given_name='K', middle_name=None, surname='Kavukcuoglu', email=None, orcid=None, affiliation=None)], index=44, id='b44', unstructured=None, date='2016', title='WaveNet: A generative model for raw audio', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1609.03499', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A V D Oord', given_name='A', middle_name='V D', surname='Oord', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Vinyals', given_name='O', middle_name=None, surname='Vinyals', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Kavukcuoglu', given_name='K', middle_name=None, surname='Kavukcuoglu', email=None, orcid=None, affiliation=None)], index=45, id='b45', unstructured=None, date='2017', title='Neural discrete representation learning', book_title='Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A V D Oord', given_name='A', middle_name='V D', surname='Oord', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Li', given_name='Y', middle_name=None, surname='Li', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Babuschkin', given_name='I', middle_name=None, surname='Babuschkin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Simonyan', given_name='K', middle_name=None, surname='Simonyan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Vinyals', given_name='O', middle_name=None, surname='Vinyals', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Kavukcuoglu', given_name='K', middle_name=None, surname='Kavukcuoglu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Van Den Driessche', given_name='G', middle_name=None, surname='Van Den Driessche', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Lockhart', given_name='E', middle_name=None, surname='Lockhart', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Cobo', given_name='L', middle_name=None, surname='Cobo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Stimberg', given_name='F', middle_name=None, surname='Stimberg', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Casagrande', given_name='N', middle_name=None, surname='Casagrande', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Grewe', given_name='D', middle_name=None, surname='Grewe', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Noury', given_name='S', middle_name=None, surname='Noury', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Dieleman', given_name='S', middle_name=None, surname='Dieleman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Elsen', given_name='E', middle_name=None, surname='Elsen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Kalchbrenner', given_name='N', middle_name=None, surname='Kalchbrenner', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Zen', given_name='H', middle_name=None, surname='Zen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Graves', given_name='A', middle_name=None, surname='Graves', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H King', given_name='H', middle_name=None, surname='King', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Walters', given_name='T', middle_name=None, surname='Walters', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Belov', given_name='D', middle_name=None, surname='Belov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Hassabis', given_name='D', middle_name=None, surname='Hassabis', email=None, orcid=None, affiliation=None)], index=46, id='b46', unstructured=None, date='2018', title='Parallel WaveNet: Fast high-fidelity speech synthesis', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='3918-3926', first_page='3918', last_page='3926', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Oura', given_name='K', middle_name=None, surname='Oura', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Mase', given_name='A', middle_name=None, surname='Mase', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Yamada', given_name='T', middle_name=None, surname='Yamada', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Muto', given_name='S', middle_name=None, surname='Muto', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Nankaku', given_name='Y', middle_name=None, surname='Nankaku', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Tokuda', given_name='K', middle_name=None, surname='Tokuda', email=None, orcid=None, affiliation=None)], index=47, id='b47', unstructured=None, date='2010', title='Recent development of the HMM-based singing voice synthesis system -Sinsy', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='G Papamakarios', given_name='G', middle_name=None, surname='Papamakarios', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Pavlakou', given_name='T', middle_name=None, surname='Pavlakou', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Murray', given_name='I', middle_name=None, surname='Murray', email=None, orcid=None, affiliation=None)], index=48, id='b48', unstructured=None, date='2017', title='Masked autoregressive flow for density estimation', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2338-2347', first_page='2338', last_page='2347', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C Payne', given_name='C', middle_name=None, surname='Payne', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Musenet', given_name=None, middle_name=None, surname='Musenet', email=None, orcid=None, affiliation=None)], index=49, id='b49', unstructured=None, date='2019', title='OpenAI blog', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='https://openai.com/blog/musenet'), GrobidBiblio(authors=[GrobidAuthor(full_name='W Ping', given_name='W', middle_name=None, surname='Ping', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Peng', given_name='K', middle_name=None, surname='Peng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Gibiansky', given_name='A', middle_name=None, surname='Gibiansky', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S O Arik', given_name='S', middle_name='O', surname='Arik', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Kannan', given_name='A', middle_name=None, surname='Kannan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Narang', given_name='S', middle_name=None, surname='Narang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Raiman', given_name='J', middle_name=None, surname='Raiman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Miller', given_name='J', middle_name=None, surname='Miller', email=None, orcid=None, affiliation=None)], index=50, id='b50', unstructured=None, date='2018', title='Deep Voice 3: 2000-speaker neural text-to-speech', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W Ping', given_name='W', middle_name=None, surname='Ping', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Peng', given_name='K', middle_name=None, surname='Peng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Chen', given_name='J', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Clarinet', given_name=None, middle_name=None, surname='Clarinet', email=None, orcid=None, affiliation=None)], index=51, id='b51', unstructured=None, date='2019', title='Parallel wave generation in end-to-end text-to-speech', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Prenger', given_name='R', middle_name=None, surname='Prenger', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Valle', given_name='R', middle_name=None, surname='Valle', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Catanzaro', given_name='B', middle_name=None, surname='Catanzaro', email=None, orcid=None, affiliation=None)], index=52, id='b52', unstructured=None, date='2019', title='WaveGlow: A flow-based generative network for speech synthesis', book_title='IEEE International Conference on Acoustics, Speech and Signal Processing', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='3617-3621', first_page='3617', last_page='3621', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Pressing', given_name='J', middle_name=None, surname='Pressing', email=None, orcid=None, affiliation=None)], index=53, id='b53', unstructured=None, date='1988', title='Nonlinear maps as generators of musical design', book_title=None, series_title=None, editors=None, journal='Computer Music Journal', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='12', issue='2', pages='35-46', first_page='35', last_page='46', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Radford', given_name='A', middle_name=None, surname='Radford', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Wu', given_name='J', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Child', given_name='R', middle_name=None, surname='Child', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Luan', given_name='D', middle_name=None, surname='Luan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Amodei', given_name='D', middle_name=None, surname='Amodei', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Sutskever', given_name='I', middle_name=None, surname='Sutskever', email=None, orcid=None, affiliation=None)], index=54, id='b54', unstructured=None, date=None, title='Language models are unsupervised multitask learners', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Razavi', given_name='A', middle_name=None, surname='Razavi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Van Den Oord', given_name='A', middle_name=None, surname='Van Den Oord', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Vinyals', given_name='O', middle_name=None, surname='Vinyals', email=None, orcid=None, affiliation=None)], index=55, id='b55', unstructured=None, date='2019', title='Generating diverse high-fidelity images with vq-vae-2', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='14837-14847', first_page='14837', last_page='14847', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Rezende', given_name='D', middle_name=None, surname='Rezende', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Mohamed', given_name='S', middle_name=None, surname='Mohamed', email=None, orcid=None, affiliation=None)], index=56, id='b56', unstructured=None, date='2015', title='Variational inference with normalizing flows', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1530-1538', first_page='1530', last_page='1538', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D J Rezende', given_name='D', middle_name='J', surname='Rezende', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Mohamed', given_name='S', middle_name=None, surname='Mohamed', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Wierstra', given_name='D', middle_name=None, surname='Wierstra', email=None, orcid=None, affiliation=None)], index=57, id='b57', unstructured=None, date='2014', title='Stochastic backpropagation and approximate inference in deep generative models', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1278-1286', first_page='1278', last_page='1286', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Roberts', given_name='A', middle_name=None, surname='Roberts', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Engel', given_name='J', middle_name=None, surname='Engel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Raffel', given_name='C', middle_name=None, surname='Raffel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Hawthorne', given_name='C', middle_name=None, surname='Hawthorne', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Eck', given_name='D', middle_name=None, surname='Eck', email=None, orcid=None, affiliation=None)], index=58, id='b58', unstructured=None, date='2018', title='A hierarchical latent vector model for learning longterm structure in music', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='4364-4373', first_page='4364', last_page='4373', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Saino', given_name='K', middle_name=None, surname='Saino', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Zen', given_name='H', middle_name=None, surname='Zen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Nankaku', given_name='Y', middle_name=None, surname='Nankaku', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Lee', given_name='A', middle_name=None, surname='Lee', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Tokuda', given_name='K', middle_name=None, surname='Tokuda', email=None, orcid=None, affiliation=None)], index=59, id='b59', unstructured=None, date='2006', title='An HMM-based singing voice synthesis system', book_title='INTERSPEECH', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Salakhutdinov', given_name='R', middle_name=None, surname='Salakhutdinov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Hinton', given_name='G', middle_name=None, surname='Hinton', email=None, orcid=None, affiliation=None)], index=60, id='b60', unstructured=None, date='2009', title='Deep boltzmann machines', book_title='Artificial intelligence and statistics', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='448-455', first_page='448', last_page='455', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Shen', given_name='J', middle_name=None, surname='Shen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Pang', given_name='R', middle_name=None, surname='Pang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R J Weiss', given_name='R', middle_name='J', surname='Weiss', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Schuster', given_name='M', middle_name=None, surname='Schuster', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Jaitly', given_name='N', middle_name=None, surname='Jaitly', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Yang', given_name='Z', middle_name=None, surname='Yang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Chen', given_name='Z', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Zhang', given_name='Y', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wang', given_name='Y', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Skerrv-Ryan', given_name='R', middle_name=None, surname='Skerrv-Ryan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R A Saurous', given_name='R', middle_name='A', surname='Saurous', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Agiomvrgiannakis', given_name='Y', middle_name=None, surname='Agiomvrgiannakis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wu', given_name='Y', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None)], index=61, id='b61', unstructured=None, date='2018', title='Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions', book_title='IEEE International Conference on Acoustics, Speech and Signal Processing', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='4779-4783', first_page='4779', last_page='4783', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Sites', given_name='D', middle_name=None, surname='Sites', email=None, orcid=None, affiliation=None)], index=62, id='b62', unstructured=None, date='2013', title='Compact language detector 2', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url='https://github.com/CLD2Owners/cld2'), GrobidBiblio(authors=[GrobidAuthor(full_name='J Sotelo', given_name='J', middle_name=None, surname='Sotelo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Mehri', given_name='S', middle_name=None, surname='Mehri', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Kumar', given_name='K', middle_name=None, surname='Kumar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J F Santos', given_name='J', middle_name='F', surname='Santos', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Kastner', given_name='K', middle_name=None, surname='Kastner', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A C Courville', given_name='A', middle_name='C', surname='Courville', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None)], index=63, id='b63', unstructured=None, date='2017', title='Char2Wav: End-toend speech synthesis', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Sturmel', given_name='N', middle_name=None, surname='Sturmel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Daudet', given_name='L', middle_name=None, surname='Daudet', email=None, orcid=None, affiliation=None)], index=64, id='b64', unstructured=None, date='2011', title='Signal reconstruction from stft magnitude: A state of the art', book_title='International Conference on Digital Audio Effects', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='I Sutskever', given_name='I', middle_name=None, surname='Sutskever', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Vinyals', given_name='O', middle_name=None, surname='Vinyals', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Q V Le', given_name='Q', middle_name='V', surname='Le', email=None, orcid=None, affiliation=None)], index=65, id='b65', unstructured=None, date='2014', title='Sequence to sequence learning with neural networks', book_title='Advances in neural information processing systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='3104-3112', first_page='3104', last_page='3112', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Y Taigman', given_name='Y', middle_name=None, surname='Taigman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Wolf', given_name='L', middle_name=None, surname='Wolf', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Polyak', given_name='A', middle_name=None, surname='Polyak', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Nachmani', given_name='E', middle_name=None, surname='Nachmani', email=None, orcid=None, affiliation=None)], index=66, id='b66', unstructured=None, date='2018', title='VoiceLoop: Voice fitting and synthesis via a phonological loop', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='B Uria', given_name='B', middle_name=None, surname='Uria', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M.-A Côté', given_name='M.-A', middle_name=None, surname='Côté', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Gregor', given_name='K', middle_name=None, surname='Gregor', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Murray', given_name='I', middle_name=None, surname='Murray', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Larochelle', given_name='H', middle_name=None, surname='Larochelle', email=None, orcid=None, affiliation=None)], index=67, id='b67', unstructured=None, date='2016', title='Neural autoregressive distribution estimation', book_title=None, series_title=None, editors=None, journal='The Journal of Machine Learning Research', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='17', issue='1', pages='7184-7220', first_page='7184', last_page='7220', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Van Den Oord', given_name='A', middle_name=None, surname='Van Den Oord', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Kalchbrenner', given_name='N', middle_name=None, surname='Kalchbrenner', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Espeholt', given_name='L', middle_name=None, surname='Espeholt', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Vinyals', given_name='O', middle_name=None, surname='Vinyals', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Graves', given_name='A', middle_name=None, surname='Graves', email=None, orcid=None, affiliation=None)], index=68, id='b68', unstructured=None, date='2016', title='Conditional image generation with pixelcnn decoders', book_title='Advances in neural information processing systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='4790-4798', first_page='4790', last_page='4798', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Vasquez', given_name='S', middle_name=None, surname='Vasquez', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Lewis', given_name='M', middle_name=None, surname='Lewis', email=None, orcid=None, affiliation=None)], index=69, id='b69', unstructured=None, date='2019', title='MelNet: A generative model for audio in the frequency domain', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1906.01083', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Vaswani', given_name='A', middle_name=None, surname='Vaswani', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Shazeer', given_name='N', middle_name=None, surname='Shazeer', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Parmar', given_name='N', middle_name=None, surname='Parmar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Uszkoreit', given_name='J', middle_name=None, surname='Uszkoreit', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Jones', given_name='L', middle_name=None, surname='Jones', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A N Gomez', given_name='A', middle_name='N', surname='Gomez', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Ł Kaiser', given_name='Ł', middle_name=None, surname='Kaiser', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Polosukhin', given_name='I', middle_name=None, surname='Polosukhin', email=None, orcid=None, affiliation=None)], index=70, id='b70', unstructured=None, date='2017', title='Attention is all you need', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='5998-6008', first_page='5998', last_page='6008', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Y Wang', given_name='Y', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Stanton', given_name='D', middle_name=None, surname='Stanton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Zhang', given_name='Y', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Skerry-Ryan', given_name='R', middle_name=None, surname='Skerry-Ryan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Battenberg', given_name='E', middle_name=None, surname='Battenberg', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Shor', given_name='J', middle_name=None, surname='Shor', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Xiao', given_name='Y', middle_name=None, surname='Xiao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Ren', given_name='F', middle_name=None, surname='Ren', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Jia', given_name='Y', middle_name=None, surname='Jia', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R A Saurous', given_name='R', middle_name='A', surname='Saurous', email=None, orcid=None, affiliation=None)], index=71, id='b71', unstructured=None, date='2018', title='Style Tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Wu', given_name='J', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Hu', given_name='C', middle_name=None, surname='Hu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wang', given_name='Y', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Hu', given_name='X', middle_name=None, surname='Hu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Zhu', given_name='J', middle_name=None, surname='Zhu', email=None, orcid=None, affiliation=None)], index=72, id='b72', unstructured=None, date='2019', title='A hierarchical recurrent neural network for symbolic melody generation', book_title=None, series_title=None, editors=None, journal='IEEE Transactions on Cybernetics', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Xie', given_name='S', middle_name=None, surname='Xie', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Girshick', given_name='R', middle_name=None, surname='Girshick', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Dollár', given_name='P', middle_name=None, surname='Dollár', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Tu', given_name='Z', middle_name=None, surname='Tu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K He', given_name='K', middle_name=None, surname='He', email=None, orcid=None, affiliation=None)], index=73, id='b73', unstructured=None, date='2017', title='Aggregated residual transformations for deep neural networks', book_title='IEEE Conference on Computer Vision and Pattern Recognition', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1492-1500', first_page='1492', last_page='1500', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Yamamoto', given_name='R', middle_name=None, surname='Yamamoto', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Song', given_name='E', middle_name=None, surname='Song', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J.-M Kim', given_name='J.-M', middle_name=None, surname='Kim', email=None, orcid=None, affiliation=None)], index=74, id='b74', unstructured=None, date='2020', title='Parallel Wave-GAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram', book_title='International Conference on Acoustics, Speech, and Signal Processing', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L Yang', given_name='L', middle_name=None, surname='Yang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Chou', given_name='S', middle_name=None, surname='Chou', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Yang', given_name='Y', middle_name=None, surname='Yang', email=None, orcid=None, affiliation=None)], index=75, id='b75', unstructured=None, date='2017', title='Midinet: A convolutional generative adversarial network for symbolic-domain music generation', book_title='International Society for Music Information Retrieval Conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='324-331', first_page='324', last_page='331', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Zen', given_name='H', middle_name=None, surname='Zen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Tokuda', given_name='K', middle_name=None, surname='Tokuda', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A W Black', given_name='A', middle_name='W', surname='Black', email=None, orcid=None, affiliation=None)], index=76, id='b76', unstructured=None, date='2009', title='Review: Statistical parametric speech synthesis', book_title=None, series_title=None, editors=None, journal='Speech Communication', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='51', issue='11', pages='1039-1064', first_page='1039', last_page='1064', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Zhang', given_name='H', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y N Dauphin', given_name='Y', middle_name='N', surname='Dauphin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Ma', given_name='T', middle_name=None, surname='Ma', email=None, orcid=None, affiliation=None)], index=77, id='b77', unstructured=None, date='2019', title='Fixup initialization: Residual learning without normalization', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Zhang', given_name='H', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Goodfellow', given_name='I', middle_name=None, surname='Goodfellow', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Metaxas', given_name='D', middle_name=None, surname='Metaxas', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Odena', given_name='A', middle_name=None, surname='Odena', email=None, orcid=None, affiliation=None)], index=78, id='b78', unstructured=None, date='2019', title='Self-attention generative adversarial networks', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None)], abstract='We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code.', body='Introduction Music is an integral part of human culture, existing from the earliest periods of human civilization and evolving into a wide diversity of forms. It evokes a unique human spirit in its creation, and the question of whether computers can ever capture this creative process has fascinated computer scientists for decades. We have had algorithms generating piano sheet music  (Hiller Jr & Isaacson, 1957; Moorer, 1972; Hadjeres et al., 2017; Huang et al., 2017) , digital vocoders generating a singer\'s voice  (Bonada & Serra, 2007; Saino et al., 2006; Blaauw & Bonada, 2017)  and also synthesizers producing timbres for various musical instruments  (Engel et al., 2017; 2019) . Each captures a specific aspect of music generation: melody, composition, timbre, and the human voice singing. However, a single system to do it all remains elusive. The field of generative models has made tremendous progress in the last few years. One of the aims of generative modeling is to capture the salient aspects of the data and to generate new instances indistinguishable from the true data The hypothesis is that by learning to produce the data we can learn the best features of the data 1  . We are surrounded by highly complex distributions in the visual, audio, and text domain, and in recent years we have devel-oped advances in text generation  (Radford et al.) , speech generation  (Xie et al., 2017)  and image generation  (Brock et al., 2019; Razavi et al., 2019) . The rate of progress in this field has been rapid, where only a few years ago we had algorithms producing blurry faces  (Kingma & Welling, 2014; Goodfellow et al., 2014 ) but now we now can generate high-resolution faces indistinguishable from real ones  (Zhang et al., 2019b) . Generative models have been applied to the music generation task too. Earlier models generated music symbolically in the form of a pianoroll, which specifies the timing, pitch, velocity, and instrument of each note to be played.  (Yang et al., 2017; Dong et al., 2018; Huang et al., 2019a; Payne, 2019; Roberts et al., 2018; Wu et al., 2019) . The symbolic approach makes the modeling problem easier by working on the problem in the lower-dimensional space. However, it constrains the music that can be generated to being a specific sequence of notes and a fixed set of instruments to render with. In parallel, researchers have been pursuing the nonsymbolic approach, where they try to produce music directly as a piece of audio. This makes the problem more challenging, as the space of raw audio is extremely high dimensional with a high amount of information content to model. There has been some success, with models producing piano pieces either in the raw audio domain  (Oord et al., 2016; Mehri et al., 2017; Yamamoto et al., 2020)  or in the spectrogram domain  (Vasquez & Lewis, 2019) . The key bottleneck is that modeling the raw audio directly introduces extremely long-range dependencies, making it computationally challenging to learn the high-level semantics of music. A way to reduce the difficulty is to learn a lower-dimensional encoding of the audio with the goal of losing the less important information but retaining most of the musical information. This approach has demonstrated some success in generating short instrumental pieces restricted to a set of a few instruments  (Oord et al., 2017; Dieleman et al., 2018) . In this work, we show that we can use state-of-the-art deep generative models to produce a single system capable of generating diverse high-fidelity music in the raw audio domain, with long-range coherence spanning multiple minutes. Our approach uses a hierarchical VQ-VAE architecture  (Razavi et al., 2019)  to compress audio into a discrete space, with a loss function designed to retain the maximum amount of musical information, while doing so at increasing levels of compression. We use an autoregressive Sparse Transformer  (Child et al., 2019; Vaswani et al., 2017)  trained with maximum-likelihood estimation over this compressed space, and also train autoregressive upsamplers to recreate the lost information at each level of compression. We show that our models can produce songs from highly diverse genres of music like rock, hip-hop, and jazz. They can capture melody, rhythm, long-range composition, and timbres for a wide variety of instruments, as well as the styles and voices of singers to be produced with the music. We can also generate novel completions of existing songs. Our approach allows the option to influence the generation process: by swapping the top prior with a conditional prior, we can condition on lyrics to tell the singer what to sing, or on midi to control the composition. We release our model weights and training and sampling code at https://github.com/openai/jukebox. \n Background We consider music in the raw audio domain represented as a continuous waveform x ∈ [−1, 1] T , where the number of samples T is the product of the audio duration and the sampling rate typically ranging from 16 kHz to 48 kHz. For music, CD quality audio, 44.1 kHz samples stored in 16 bit precision, is typically enough to capture the range of frequencies perceptible to humans. As an example, a fourminute-long audio segment will have an input length of ∼10 million, where each position can have 16 bits of information. In comparison, a high-resolution RGB image with 1024 × 1024 pixels has an input length of ∼3 million, and each position has 24 bits of information. This makes learning a generative model for music extremely computationally demanding with increasingly longer durations; we have to capture a wide range of musical structures from timbre to global coherence while simultaneously modeling a large amount of diversity. \n VQ-VAE To make this task feasible, we use the VQ-VAE  (Oord et al., 2017; Dieleman et al., 2018; Razavi et al., 2019)  to compress raw audio to a lower-dimensional space. A one-dimensional VQ-VAE learns to encode an input sequence x = x t T t=1 using a sequence of discrete tokens z = z s ∈ [K] S s=1 , where K denotes the vocabulary size and we call the ratio T /S the hop length. It consists of an encoder E(x) which encodes x into a sequence of latent vectors h = h s S s=1 , a bottleneck that quantizes h s → e zs by mapping each h s to its nearest vector e zs from a codebook C = {e k } K k=1 , and a decoder D(e) that decodes the embedding vectors back to the input space. It is thus an auto-encoder with a discretization bottleneck. The VQ-VAE is trained using the following objective: L = L recons + L codebook + βL commit (1) L recons = 1 T t x t − D(e zt ) 2 \n 2 (2) L codebook = 1 S s sg [h s ] − e zs 2 2 (3) L commit = 1 S s h s − sg [e zs ] 2 2 (4) where sg denotes the stop-gradient operation, which passes zero gradient during backpropagation. The reconstruction loss L recons penalizes for the distance between the input x and the reconstructed output x = D(e z ), and L codebook penalizes the codebook for the distance between the encodings h and their nearest neighbors e z from the codebook. To stabilize the encoder, we also add L commit to prevent the encodings from fluctuating too much, where the weight β controls the amount of contribution of this loss. To speed up training, the codebook loss L codebook instead uses EMA updates over the codebook variables.  Razavi et al. (2019)  extends this to a hierarchical model where they train a single encoder and decoder but break up the latent sequence h into a multi-level representation [h (1) , • • • , h (L) ] with decreasing sequence lengths, each learning its own codebook C (l) . They use non-autoregressive encoder-decoders and jointly train all levels with a simple mean-squared loss. \n Music VQ-VAE Inspired by the results from the hierarchical VQ-VAE model  (Razavi et al., 2019)  for images, we consider applying the same technique to model raw audio using three different levels of abstraction, as illustrated in Figure  1 . At each level, we use residual networks consisting of WaveNet-style noncausal 1-D dilated convolutions, interleaved with downsampling and upsampling 1-D convolutions to match different hop lengths. A detailed description of the architecture is provided in Appendix B.1. We make a number of modifications to our VQ-VAE compared to the ones in  (Oord et al., 2017; Razavi et al., 2019) , as described in the following subsections. \n Random restarts for embeddings VQ-VAEs are known to suffer from codebook collapse, wherein all encodings get mapped to a single or few embedding vectors while the other embedding vectors in the codebook are not used, reducing the information capacity of the bottleneck. To prevent this, we use random restarts: when the mean usage of a codebook vector falls below a threshold, we randomly reset it to one of the encoder outputs from the current batch. This ensures all vectors in the Audio can be reconstructed using the codes at any one of the abstraction levels, where the least abstract bottom-level codes result in the highest-quality audio, as shown in Figure  4 . For the detailed structure of each component, see Figure  7 . codebook are being used and thus have a gradient to learn from, mitigating codebook collapse. \n Separated Autoencoders When using the hierarchical VQ-VAE from  (Razavi et al., 2019)  for raw audio, we observed that the bottlenecked top level is utilized very little and sometimes experiences a complete collapse, as the model decides to pass all information through the less bottlenecked lower levels. To maximize the amount of information stored at each level, we simply train separate autoencoders with varying hop lengths. Discrete codes from each level can be treated as independent encodings of the input at different levels of compression. \n Spectral Loss When using only the sample-level reconstruction loss, the model learns to reconstruct low frequencies only. To capture mid-to-high frequencies, we add a spectral loss which is defined as L spec = |STFT(x)| − |STFT( x)| 2 It encourages the model to match the spectral components without paying attention to phase which is more difficult to learn. This is similar to the use of power loss  (Oord et al., 2018)  and spectral convergence  (Arık et al., 2018b)  when training parallel decoders for raw audio. One difference between the latter approach and ours is that we are no longer optimizing the spectral signal-to-noise ratio; dividing by the magnitude of the signal results in numerical instability for mostly silent inputs. To prevent the model from overfitting to a particular choice of the STFT parameters, we use the sum of the spectral losses L spec calculated over multiple STFT parameters that trade-off time and frequency resolutions  (Yamamoto et al., 2020) . \n Music Priors and Upsamplers After training the VQ-VAE, we need to learn a prior p(z) over the compressed space to generate samples. We break up the prior model as p(z) = p(z top , z middle , z bottom ) (5) = p(z top )p(z middle |z top )p(z bottom |z middle , z top ) (6) and train separate models for the top-level prior p(z top ), and upsamplers p(z middle |z top ) and p(z bottom |z middle , z top ). Each of these is an autoregressive modeling problem in the discrete token space produced by the VQ-VAE. We use Transformers with sparse attention  (Vaswani et al., 2017; Child et al., 2019)  as they are currently the SOTA in autoregressive modeling. We propose a simplified version which we call the Scalable Transformer, that is easier to implement and scale (see Appendix A for details). For the upsamplers, we need to provide the autoregressive Transformers with conditioning information from the codes of the upper levels. To do so, we use a deep residual WaveNet  (Xie et al., 2017)  followed by an upsampling strided convolution and a layer norm  (Ba et al., 2016) , and add the output as extra positional information to the embeddings of the current level. We condition the lower levels only on the chunk of upper level codes that correspond to the same segment of raw audio. At each level, we use Transformers over the same context length of discrete codes, which correspond to increasing the raw audio length with larger hop lengths, and modeling longer temporal dependencies at the higher levels while keeping the same computational footprint for training each level. As our VQ-VAE is convolutional, we can use the same VQ-VAE to produce codes for arbitrary lengths of audio. 4.1. Artist, Genre, and Timing Conditioning Our generative model can be made more controllable by providing additional conditioning signals while training. For our first models, we provide artist and genre labels for the songs. This has two advantages: first, it reduces the entropy of the audio prediction, so the model is able to achieve better quality in any particular style. Second, at generation time, we are able to steer the model to generate in a style of our choosing. Additionally, we attach a timing signal for each segment at training time. This signal includes the total duration of the piece, the start time of that particular sample and how much fraction of the song that has elapsed. This allows the model to learn audio patterns that depend on the overall structure, such as spoken or instrumental introductions and applause at the end of a piece. \n Lyrics Conditioning While the conditioned models above are able to generate songs of diverse genres and artistic styles, singing voices generated by those models, while often sung in a compelling melody, are mostly composed of babbling, rarely producing recognizable English words. In order to be able to control the generative model with lyrics, we provide more context at training time by conditioning the model on the lyrics corresponding to each audio segment, allowing the model to produce singing simultaneosly with the music. Lyrics-to-singing (LTS) task: The conditioning signal only includes the text of the lyrics, without timing or vocalisation information. We thus have to model the temporal alignment of lyrics and singing, the artists voice and also the diversity of ways one can sing a phrase depending on the pitch, melody, rhythm and even genre of the song. The conditioning data isn\'t precise as the lyrics data often contains textual references to repeated sections like "chorus" or mismatching portions of lyrics with the corresponding music. There is also no separation between lead vocals, accompanying vocals and the background music in target audio. This makes the Lyrics-to-singing (LTS) task significantly more challenging than the corresponding Text-to-speech (TTS) task. Providing lyrics for chunks of audio: Our dataset includes song-level lyrics, but to make the task easier we train on shorter (24 sec) chunks of audio. To provide the lyrics cor-   responding to the audio during training, we began with a simple heuristics of aligning the characters of the lyrics to linearly span the duration of each song, and pass a fixed-side window of characters centered around the current segment during training. While this simple strategy of linear alignment worked surprisingly well, we found that it fails for certain genres such as hip-hop with fast lyrics. To address this, we use Spleeter  (Hennequin et al., 2019)  to extract vocals from each song and run NUS AutoLyricsAlign  (Gupta et al., 2020)  on the extracted vocals to obtain a word-level alignments of the lyrics, allowing us to more accurately provide the lyrics for a given chunk of audio. We choose a large enough window so that the actual lyrics have a high probability of being inside the window. Encoder-decoder model: We use an encoder-decoder style model to condition on the characters of the lyrics, with the encoder producing features from the lyrics which are attended to by the decoder which produces the top level music tokens. The lyrics encoder is a Transformer with an autoregressive modeling loss for lyrics, and its last level is used as features of the lyrics. In the music decoder, we interleave a few additional layers with encoder-decoder attention where the queries from the music tokens are only allowed to attend to keys and values from the lyrics tokens. These layers attend on the activation from the last layer of the lyrics encoder (see Figure  8c ). In Figure  3 , we see that the attention pattern learned by one of these layers corresponds to the alignment between the lyrics and the singing. \n Decoder Pretraining To reduce computation required to train the lyrics conditional model, we use a pretrained unconditional top-level prior as our decoder and introduce the lyrics encoder using model surgery  (Berner et al., 2019) . We initialize the output projection weights in the MLP and the attention layers of these residual blocks to zeros  (Zhang et al., 2019a) , so that the added layers perform the identity function at initialization. Thus, at initialization the model behaves identically as the pretrained decoder, but there is still a gradient with respect to the encoder state and parameters 2  , allowing the model to learn to use the encoder. \n Sampling After we have trained our VQ-VAE, upsamplers, and top level priors, we can then use them to sample novel songs. Ancestral sampling: We first generate the top level codes one token at a time by the usual ancestral sampling process (see Figure  2a ): generating the first token, then passing all previously generated tokens into the model as inputs and outputting the next token conditioned on all previous tokens. We then run our conditioning wavenet on the top level codes to produce the conditioning information for the middle level and sample ancestrally from it too, and do the same for the bottom level. Windowed sampling: To sample segments longer than the context length, we use windowed sampling, where we move ahead our sampling window by half our context and continue sampling conditioned on this half context (See Figure  2b ). We can trade off speed for quality by using a smaller hop length here. Primed sampling: Instead of sampling the entire token sequence from the model, we can also run a forward pass of the VQ-VAE to obtain the top, middle, and bottom level codes corresponding to a segment from an actual song, as shown in Figure  2c . We can use these as the initial tokens in our ancestral sampling process and continue sampling from these to produce novel completions of the song. \n Experiments \n Dataset We scraped a new dataset of 1.2 million songs (600k of which in English), paired with the lyrics and metadata from LyricWiki (LyricWiki). The metadata includes artist, album, genre, and year of the release, along with common moods or playlist keywords associated with each song. We train on 32 bit, 44.1 kHz raw audio and perform data augmentation by randomly downmixing the right and left channels to produce mono channel audio. \n Training Details For the music VQ-VAE, we use 3 levels of bottlenecks compressing 44 kHz audio in dimensionality by 8x, 32x, and 128x respectively, with a codebook size of 2048 for each level. The VQ-VAE has 2 million parameters and is trained on 9-second audio clips on 256 V100 for 3 days. We used exponential moving average to update the codebook following  Razavi et al. (2019) . For our prior and upsampler models, we use a context of 8192 tokens of VQ-VAE codes, which corresponds to approximately 24, 6, and 1.5 seconds of raw audio at the top, middle, and bottom level, respectively. The upsamplers have one billion parameters and are trained on 128 V100s for 2 weeks, and the top-level prior has 5 billion parameters and is trained on 512 V100s for 4 weeks. We use Adam with learning rate 0.00015 and weight decay of 0.002. For lyrics conditioning, we reuse the prior and add a small encoder, after which we train the model on 512 V100s for 2 weeks. The detailed hyperparameters for our models and training are provided in Appendix B.3. \n Samples We trained a sequence of models with increasing sample quality. Our first model was trained on the MAESTRO dataset using 22 kHz VQ-VAE codes and relatively small prior models. We observed that this could generate high fidelity classical music samples with piano and occasional violin. We then collected a larger and more diverse dataset of songs with genre and artist labels. The same model when trained on this new dataset was able to produce diverse samples other than classical music, and demonstrated musicality and coherence over more than a minute. Despite the novelty of being able to generate generally high fidelity and coherent songs, sample quality was still limited by a number of factors. First, the use of 22 kHz sampling rate along with small upsamplers introduced noise both in the upsampling and decoding steps, which we hear as grainy texture. We improved fidelity by using 44 kHz VQ-VAE and 1B parameter upsamplers in all subsequent experiments at the expense of longer rendering time. Second, the 1B top-level prior was not big enough to produce singing and diverse musical timbres. We first explored increasing the model size to 5 billion parameters. Larger capacity allowed better modeling of the broader distribution of songs, resulting in samples with better musicality, longer coherence and initial singing. While there is an overall qualitative improvement, the unconditional model still struggled to sing recognizable words. Training a seq2seq model with lyric conditioning and limiting the dataset only to songs primarily in English made singing both intelligible and controllable. The final model, which we call Jukebox, uses all these improvements. Because everyone experiences music differently, it is generally tricky and not very meaningful to evaluate samples by the mean opinion score or FID-like metrics. We manually evaluate coherence, musicality, diver-sity, and novelty of generated samples. The links to curated examples are embedded in text. Coherence: We find the samples stay very coherent musically through the context length of the top-level prior (approximately 24 seconds), and they maintain similar harmonies and textures as we slide the window to generate longer samples. However, because the top-level does not have the context of the entire song, we do not hear long term musical patterns, and we would never hear choruses or melodies that repeat. The generations progress through beginnings of songs (for example applause or slow instrumental warm-ups), through sections that sound chorus-like, through instrumental interludes, and then fading or otherwise wrapping up at the end. The top-level prior always knows what fraction of the song is complete time-wise, so it is able to imitate appropriate beginnings, middles and ends. \n Musicality: The samples frequently imitate familiar musical harmonies and the lyrics are usually set in ways that are very natural. Frequently the highest or longest notes of the melody match words that a human singer would choose to emphasize, and the lyrics are almost always rendered in ways that capture the prosody of the phrases. This is noticeable in hip hop generations, where the model reliably captures the rhythm of spoken text. We do find that the generated melodies are usually less interesting than human composed melodies. In particular, we do not hear the antecedent and consequent pattern familiar to many human melodies, and we rarely hear choruses that are melodically memorable. Diversity: Likelihood training encourages covering of all modes, so we expect the model to produce diverse samples. -Re-renditions: We generate multiple samples conditioned on artist and lyrics combinations that exist in our training data. While occasionally drum and bass lines or melodic intervals echo the original versions, we find that none of the generated samples is noticeably similar to the original songs. We also generate multiple songs conditioned on the same artist and lyrics as Sample 1 to obtain Samples 9-12. All five sound interesting in their own ways with different moods and melodies with Sample 10 playing a harmonic at 00:14 as part of a blues riff, showing that the model has learned a wide range of singing and playing styles. -Completions: We prime the model with 12 seconds of existing songs and ask it to complete them in the same styles. When the priming samples include singing, the continuations are more likely to imitate the original tunes and rhythms. Songs primed with more generic or common intros tend to be more diverse. Even generated samples that are close to the originals early on deviate completely into new musical material after about 30 seconds. Re-renditions and completions are interesting and diverse, but overall, there is still room for improvement in music quality compared to the original songs. -Full tree: To understand diversity in a more systematic way, we generate multiple continuations from the same segment. We start with a one-minute sample and independently sample four times per one-minute extension. By the three minute mark, there are 16 completions. We can think of this branching tree as exploring different possibilities obtained by ancestral sampling. In the generated songs in the link, we hear diversity in singing and development even when the same initial segment is used. We note that this particular sample follows the lyrics more successfully than many. For certain genres like hip hop and rap, where linearly moving the window does not yield good lyrics alignment, the chance of obtaining plausible singing is lower. Novelty: With the ability to condition on various styles, lyrics, and raw audio, we would like Jukebox to be a useful tool for both professional musicians and music enthusiasts alike. In this section, we are interested in exploring capabilities and applications of Jukebox. -Novel styles: We generate songs in an unusual genre typically not associated with an artist. In general, we find that it is fairly difficult to generalize to a novel style of singing while using the same voice as the artist embedding overpowers other information. In Joe Bonamassa and Frank Sinatra samples, we hear a modest variation in instrumentation, energy, and ambience depending on the genre embedding. However, our attempts to mix country singer Alan Jackson with unusual genres like hip hop and punk did not seem to move the samples away from a country style in meaningful ways. -Novel voices: We pick artists whose voices are reproduced reasonably well by the model, and interpolate their style embeddings to synthesize new voices. Some blending, for instance, between Frank Sinatra and Alan Jackson in Sample 4, still sounds similar to Frank Sinatra. In most cases, the model renders in a vaguely recognizable but distinct voice that preserves different vocal attributes. Samples 1 and 2 conditioned on the Céline Dion embeddings divided by two have slightly different timbre and tone but capture her unique vibrato. We also experiment with changing the style embedding in the middle of a song to create a duet (Sample 7). This is another way of guiding generation during sampling. Continuing in another voice works best when the segment ends in an interlude; otherwise, the model blends voices in the middle of a word or a sentence. -Novel lyrics: We ask Jukebox to sing poems and novel verses generated by GPT-2  (Radford et al.)  to demonstrate that it can indeed sing new lyrics. While the training data consists of song lyrics with limited vocabulary and constrained structure, the model has learned to follow along most prompts and sing even new words that are reasonably pronounceable (including technical terms from the deep learning literature). To get the best results, however, we find that it is useful to spell out difficult words or acronyms as they are spoken. The generations are noticeably higher quality if the text matches the distribution of lyrics for the given artist, both in terms of length, and of rhyming or rhythmic qualities. For example, hip hop lyrics tend to be longer than most other genres, and the commonly emphasized syllables easily form clear rhythms. -Novel riffs: Another useful application of Jukebox is the ability to record an incomplete idea and explore various continuations without ever needing to tabulate in symbolic representations, which would lose details of timbre and mood. We curate recordings of novel riffs by our in-house musicians and prime the model during sampling. Sample 6 starts with a musical style not widely used in Elton John\'s songs. The model still carries out the tune and develops it further. Similarly, the beginning of Sample 1 is a progressive jazz piece with a 5/4 polymeter, which has never been used in hip hop. Despite this novelty, the rhythm persists throughout the song and is incorporated naturally with rapping. \n VQ-VAE Ablations Spectral  The first row is the ground-truth, and the second row shows the spectrograms of audio outputs from our VQ-VAE. In the third row, we remove the spectral loss, and see that the middle and top level lose high-frequency information. In the fourth row, we use a hierarchical VQ-VAE  (Razavi et al., 2019)  instead of separate auto-encoders (Figure  1 ), and we see the middle and top levels are not used for encoding pertinent information. Finally, the fifth row shows a baseline with the Opus codec that encodes audio at constant bitrates comparable to our VQ-VAE. It also fails to capture higher frequencies and adds noticeable artifacts at the highest level of compression. We compare raw audio VQ-VAEs when trained with varying compression ratios, objectives, and architectures. As we use nonautoregressive decoders with continuous representation for output, we report spectral convergence  (Sturmel & Daudet, 2011) , which measures the amount of spectral error relative to signal, as test error and proxy for reconstruction fidelity. We evaluate on 5000 held-out 3-second audio segments and report the average in decibels. All models in this section are trained with a batch size of 32, 3-second audio clips sampled at 44 kHz. As before, we use hop lengths of 8, 32, and 128 for the bottom, middle and top level respectively. In Table  1 , we see that increasing the hop size results in higher reconstruction error. Figure  4  indeed shows that a significant amount of information, especially higher frequencies, is missing at middle and top levels across all ablations we ran. This is expected as audio is compressed more with larger hop sizes. To mitigate codebook collapse, we restart dead codes near random encoder embeddings. In Figure  5 , we see that this yields higher codebook usage even from early on in training. Models trained without random restarts can converge to the same test error and codebook usage but require more training steps. With poor initialization, these models sometimes end up with suboptimal codes hurting reconstruction fidelity. Codebook size also matters, as it sets a limit on channel capacity through the bottleneck layers. In Table  2 , we find that reconstruction error increases considerably when the codebook size is reduced from 2048 to 256. We also compare with a model that uses continuous representations without vector quantization. We can think of this model as using a vastly large codebook with all encoder embeddings. This achieves almost perfect reconstruction with negligible spectral error. When the model is trained with L2 loss only, reconstructions tend to sound muddy from missing high frequencies, and this problem is exacerbated as hop size is increased. In Figure  4 , we see that top-level codes trained without spectral loss do not capture much information beyond 2 kHz, and obtain worse reconstructions (Table  3 ). However, we observe that while spectral loss helps encode more information, it also adds distortion artifacts which we hear as scratchy noise. Lastly, we train a raw audio hierarchical VQ-VAE  (Razavi et al., 2019)  and find that it is generally difficult to push information to higher levels. This model is trained twice as long as the previous models, but middle and top-level reconstructions as shown in Figure  4  are not capturing much. It is possible that higher level codes may have collapsed before bottom level starts to reconstruct the audio well. Making the bottom layers explicitly model residuals pushed more information to the top. But, we found separate autoencoders to be cleaner and more effective. \n Related Work Generative modeling in deep learning: Generative models aim to learn the distribution of data by either explicitly by modeling the distribution or implicitly by constructing means to sample from it  (Goodfellow, 2016) . Modeling the interdependency within high-dimensional data was traditionally considered extremely difficult, but starting with Deep Boltzmann Machines  (Salakhutdinov & Hinton, 2009) , various kinds of deep generative models have been introduced. Generative Adversarial Networks (GANs)  (Goodfellow et al., 2014)  use generator and discriminator networks that contest each other to make the generated samples as indistinguishable as possible from the data, and they are renowned for their ability to generate high-quality pictures  (Zhang et al., 2019b; Brock et al., 2019) . Autoregressive generative models such as NADE  (Uria et al., 2016) , PixelCNN (Van den  Oord et al., 2016) , and Transformers  (Vaswani et al., 2017)  use the chain rule of probability to factorize the joint distribution of data into a product of simpler distributions, and flow-based models  (Dinh et al., 2015; 2017; Rezende & Mohamed, 2015; Kingma & Dhariwal, 2018 ) learn a series of invertible transformations that maps the data distribution with a simpler one such as a Gaussian distribution. Autoregressive flows  (Papamakarios et al., 2017; Kingma et al., 2016)  combine the two ideas to achieve faster density estimation or data generation. Variational autoencoders (VAEs)  (Rezende et al., 2014; Kingma & Welling, 2014 ) impose a Gaussian prior on the latent code in an encoder-decoder setup from which data can be sampled. Generative models for music: Generative modeling of symbolic music dates back to more than half a century, when Hiller Jr & Isaacson (1957) introduced the first computergenerated music based on Markov chains. There exists a variety of earlier approaches using rule-based systems  (Moorer, 1972) , chaos and self-similarity  (Pressing, 1988) , cellular automata  (Beyls, 1989) , concatenative synthesis  (Jehan, 2005) , and constraint programming  (Anders & Miranda, 2011) . More recent data-driven approaches include DeepBach  (Hadjeres et al., 2017)  and Coconet  (Huang et al., 2017)  which use Gibbs sampling to produce notes in the style of Bach chorals, MidiNet  (Yang et al., 2017)  and MuseGAN  (Dong et al., 2018)  which use generative adversarial networks, MusicVAE  (Roberts et al., 2018)  and HRNN  (Wu et al., 2019)  which use hierarchical recurrent networks, and Music Transformer  (Huang et al., 2019a)  and MuseNet  (Payne, 2019)  which use Transformers to autoregressively predict MIDI note events. There also have been a number of approaches for synthesizing music conditioned on symbolic music information, such as NSynth  (Engel et al., 2017)  which uses WaveNet-style autoencoder, Mel2Mel  (Kim et al., 2019)  and Wave2Midi2Wave (Hawthorne et al., 2019) which synthesize music using WaveNet conditioned on a piano roll representation, and GanSynth  (Engel et al., 2019)  which uses generative adversarial networks to produce magnitude spectrograms together with instananeous frequencies for easier spectrogram inversion. Generative models for music can also be used for music style transfer, as seen in Midi-VAE  (Brunner et al., 2018)  which uses a variational autoencoder to transfer styles between classical and jazz music, LakhNES  (Donahue et al., 2019)  which uses a Transformer architecture to generate chiptune music, and Universal Music Translator Network  (Mor et al., 2019)  which uses a denoising autoencoder that can disentangle musical style and content. Sample-level generation of audio: In recent years, a variety of generative models for raw audio have been introduced. WaveNet  (Oord et al., 2016)  performs autoregressive sampleby-sample probabilistic modeling of raw waveform using a series of dilated convolutions to exponentially increase the context length. It can produce realistic audio either unconditionally or by conditioning on acoustic features or spectrograms. The autoregressive nature of WaveNet makes the sampling notoriously slow, and it uses a categorical distribution for audio samples which introduces quantization noise. Parallel WaveNet  (Oord et al., 2018)  improves upon this by instead using a mixture of logistics distribution, a continuous probability distribution, and performing probability density distillation which learns a parallel feed-forward network from a pre-trained autoregressive model, allowing faster sampling of high fidelity audio. ClariNet  (Ping et al., 2019)  achieves similar audio quality using a simple Gaussian distribution instead and thus having a closed-form loss function, eliminating the need for Monte-Carlo sampling. SampleRNN  (Mehri et al., 2017)  uses a multi-scale, hierarchical recurrent neural network with convolutional upsampling to model long-range complex structures. Wa-veRNN  (Kalchbrenner et al., 2018)  uses recurrent neural networks that operate separately on the most significant and the least significant bytes, which can be efficiently deployed in mobile devices while having comparable audio quality to WaveNet. WaveGlow  (Prenger et al., 2019 ) is a flow-based model for parallel sample-level audio synthesis, which can be trained with a straightforward maximum-likelihood estimation and thus is advantageous to the two-stage training process needed for distillation. Parallel WaveGAN  (Yamamoto et al., 2020)  and MelGAN  (Kumar et al., 2019)  are GAN-based approaches directly modeling audio waveforms, achieving similar quality as WaveNet and WaveGlow models with significantly fewer parameters. While the approaches above serve as sophisticated generative models for raw audio to be conditioned on a compact and controllable representation of audio such as Mel spectrograms, Mel-Net  (Vasquez & Lewis, 2019)     (Razavi et al., 2019)  do not suffer from this issue, and thus we use their approach. Speech synthesis: Producing natural human voice entails an understanding of linguistic features, mapping of sounds, and steerability of expression. Many text-to-speech (TTS) systems rely on highly engineered features  (Klatt, 1980) , carefully curated sound segments  (Hunt & Black, 1996) , statistical parametric modeling  (Zen et al., 2009) , and often complex pipelines as described in  (Arık et al., 2017) . These approaches are fairly involved and produce unnatural or inarticulate voices. More recent works like Deep Voice 3  (Ping et al., 2018) , Tacotron 2  (Shen et al., 2018) , and Char2Wav  (Sotelo et al., 2017)  learn speech synthesis endto-end using sequence-to-sequence architecture  (Sutskever et al., 2014) . The design space is vast, but in general, typical approaches comprise of a bidirectional encoder, a decoder, and a vocoder to build text representations, audio features, and the final raw waveforms. To generate multiple voices, text-to-speech models can also condition on the speaker identity  (Oord et al., 2016; Gibiansky et al., 2017; Jia et al., 2018)  as well as text prompt. By learning and manipulating auxiliary embeddings, models can mimic a new voice  (Arık et al., 2018a; Taigman et al., 2018)  at test time. These methods, however, require labeled data. Ideas like clustering  (Dehak et al., 2011) , priming  (Wang et al., 2018) , and variational autoencoders  (Hsu et al., 2019; Akuzawa et al., 2018)  have been used to learn broader styles of speech and control expressivity in an unsupervised way. There are also works on synthesizing singing by additionally controlling pitch and timbre. Similar to TTS literature, early works use concatenative methods  (Bonada & Serra, 2007)  that join short segments of curated singing, and statistical parametric methods  (Saino et al., 2006; Oura et al., 2010)  which allow modeling of timbre from training data. Both approaches impose fairly strong assumptions resulting in noticeable artifacts.  (Blaauw & Bonada, 2017)  train a neural TTS model with a parametric vocoder to separate pitch and timbre which can be controlled at generation time. \n Future work While our approach represents a step forward in the ability to generate coherent long raw audio music samples, we recognize several directions for future work. Great music generation should be high quality over all time scales: it should have a developing musical and emotional structure across the entire piece, local notes and harmonies that always make sense, nuanced and appropriate small timbral and textural details, and audio recording quality that balances and blends the multiple voices well, and without unwanted noise. We view our current model as stronger on the mid-range time scales: often the model generates samples that locally sound very good, with interesting and diverse harmonies, rhythms, instruments, and voices. We have frequently been very impressed how the melody and rhythm generated suits a particular lyric extremely well. However, while the samples stay consistent over longer time scales, we notice they don\'t have traditional larger music structures (such as choruses that repeat, or melodies that have a question and answer form). Additionally, on the smallest scale, we sometimes hear audio noise or scratchiness. Beyond the quality of the samples, we also would look to diversify the languages and styles the model is able to generate. Our current model has been trained only on songs whose primary language as detected by  (Sites, 2013)  is English. In the future, we would look to include other languages and artists. We believe this will be of interest both for generating strictly in those styles, and because historically we have seen much creativity and development coming from unusual blends of existing musical styles. Finally, we consider it very important that computer music generation also serves as a tool for human musicians, and increasingly those interested in music but without formal training.   (Kingma et al., 2016) , and distill the information from our current model into it  (Oord et al., 2018) . The distillation works by generating samples from the parallel sampler and evaluating it likelihood and entropy using the parallel likelihood evaluator, and then optimising the sampler by minimising the KL divergence of it from our current model. \n Conclusion We have introduced Jukebox, a model that generates raw audio music imitating many different styles and artists. We can condition this music on specific artists and genres, and can optionally specify the lyrics for the sample. We laid out the details necessary to train a Hierarchical VQ-VAE to compress the music effectively into tokens. While previous work has generated raw audio music in the 20-30 second range, our model is capable of generating pieces that are multiple minutes long, and with recognizable singing in natural-sounding voices. \n Acknowledgement We would like to thank John Schulman and Will Guss for producing and performing novel riffs for our sampling experiments, and Rewon Child, Aditya Ramesh, Ryan Lowe and Jack Clark for providing feedback for initial drafts of this paper. We have three separate raw audio VQ-VAEs to produce discrete codes at varying hop sizes for the bottom, middle, and top priors. All autoencoders comprise non-causal, dilated 1-D convolutions, and are trained independently using nonautoregressive reconstruction losses. Basic building blocks in these networks share the same architecture, as shown in Figure  7 . Each encoder block consists of a downsampling convolution, a residual network, and a 1D convolution with a kernel size of 3. Dilation is grown by a factor of 3 in these residual networks to increase the receptive field. The decoder block mirrors this exactly with a 1D convolution with the kernel size of 3, a residual network with dilation contracting across depth, and an upsampling transposed convolution. Here, all resampling convolutions use a kernel size of 4 and stride 2 so that each building block changes the hop length by a factor of 2. To get higher compression in time, we simply stack more of these blocks. For example, using seven blocks yields a hop length of 128 for the top level autoencoder. Each residual network has four residual blocks in the middle and top VQ-VAEs resulting in a receptive field of 120 ms and 480 ms for the respective discrete tokens. Because increasing the residual depth helped improve reconstruction quality slightly, we doubled the number of residual blocks for the bottom level. This dramatically increases the receptive field to about 2 seconds per code but the actual receptive field is mostly local. We also experimented with having a single decoder and modeling the residuals to separate out learned representations as in  (Razavi et al., 2019) , hoping upsampling priors would simply fill in local musical structure. However, pushing information to the top level was quite challenging as the bottommost level reconstructs almost perfectly early on in training. When we add auxiliary objectives to encourage the top to be used more, the top-level codes add serious distortions to the final output. A similar challenge is shown in  (Dieleman et al., 2018) . \n B.2. Music Priors and Upsamplers Architectural details of our music prior and upsampler models are depicted in Figure  8 . They perform autoregressive modeling of tokens at each level, conditioned on information such as artist and genre, as well as the tokens from the upper level in the case of the upsamplers (Figure  8a ). Each artist and genre are learned as embedding vectors, whose sum is provided as the very first token in each sequence. In addition, positional embedding is learned as a function of each position\'s absolute and relative timing in the duration of the song. In upsampler models, upper-level tokens (b) The bottleneck takes the sequence of embeddings from the encoder and maps it into a sequence of code vectors from the codebook. This sequence of code indices is used as a discrete representation to be modeled by the priors. Larger codebooks improve fidelity but may be more difficult to compress.  are upsampled by the conditioner network, using WaveNetstyle dilated convolutions followed by a transposed 1-D convolutional layer (Figure  8b ). When the model is trained on lyrics, the top-level prior takes lyrics data corresponding to each audio segment and uses them to train an encoder-decoder Transformer as shown in Figure  8c . All transformer stacks use sparse self-attention layers with the three factorized attention types (row, column, and previous-row) repeating, and encoder-decoder attention layers, when present, are interleaved with the other attention types. Each layer consists of residual connections of an attention and an MLP feedforward network, each prepended by layer normalization (see Figure  8d ). Figure 1 . 1 Figure1. We first train three separate VQ-VAE models with different temporal resolutions. At each level, the input audio is segmented and encoded into latent vectors ht, which are then quantized to the closest codebook vectors ez t . The code zt is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction, since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels, where the least abstract bottom-level codes result in the highest-quality audio, as shown in Figure4. For the detailed structure of each component, see Figure7. \n (a) Ancestral sampling: Priors for the VQ-VAE codes are trained using a cascade of Transformer models, shown in blue. Each model takes conditioning information such as genre, artist, timing, and lyrics, and the upsampler models are also conditioned on the codes from the upper levels. To generate music, the VQ-VAE codes are sampled from top to bottom using the conditioning information for control, after which the VQ-VAE decoder can convert the bottom-level codes to audio. time new samples (b) Windowed sampling: To generate music longer than the model\'s context length (12 in this figure), we repeatedly sample continuations at each level, using overlapping windows of previous codes as the context. The overlap amount is a hyperparameter, and the figure shows an example of 75% overlap with hop length 3. Primed sampling: The model can generate continuations of an existing audio signal by converting it into the VQ-VAE codes and sampling the subsequent codes in each level. \n Figure 2 . 2 Figure 2. Sampling methods for generating music \n Figure 3 . 3 Figure 3. Lyrics-singing alignment learned by one of the encoderdecoder attention layers. The x-axis is the position of music queries, and the y-axis is the position of lyric keys. The positions attended to by the decoder correspond to the characters being sung. \n Figure 4 . 4 Figure 4. Comparison of reconstructions from different VQ-VAEs, x-axis is time and y-axis is frequency. The columns from left to right are bottom-, middle-, and top-level reconstructions at hop lengths 8, 32, and 128 respectively, visualized as Mel spectrograms.The first row is the ground-truth, and the second row shows the spectrograms of audio outputs from our VQ-VAE. In the third row, we remove the spectral loss, and see that the middle and top level lose high-frequency information. In the fourth row, we use a hierarchical VQ-VAE (Razavi et al., 2019)  instead of separate auto-encoders (Figure1), and we see the middle and top levels are not used for encoding pertinent information. Finally, the fifth row shows a baseline with the Opus codec that encodes audio at constant bitrates comparable to our VQ-VAE. It also fails to capture higher frequencies and adds noticeable artifacts at the highest level of compression. \n Figure 5 . 5 Figure 5. Entropy of codebook with 2048 codes, i.e 11 bits, over training. Reviving dead codes near random encoder outputs ensures good codebook utilization from the start of training. \n The decoder reconstructs the raw audio from latent representations. It is a mirror of the encoder where dilations constracts by a factor of 3 down to 1 at the last block. The final Conv1D projects to the desired number of audio channels and also acts as a smoothing operation after a sequence of transposed convolutions. \n Figure 7 . 7 Figure 7. Components of the VQ-VAE model \n Table 1 . 1 Reconstruction fidelity degrades with higher compression. Level convergence (dB) Hop length Without restart With restart Bottom Middle Top 8 32 128 −21.1 −12.4 −8.3 −23.0 −12.4 −8.3 Restarting dead codes near random encoder outputs mitigates learn- ing suboptimal codes. Codebook size Spectral convergence (dB) 256 2048 No quantization −15.9 −23.0 −40.5 \n Table 2 . 2 Bottom-level VQ-VAE reconstruction results with differ- ent codebook sizes. Using larger codebooks helps reconstruction because it allows more information to be encoded at the bottleneck layers. Removing the bottleneck entirely yields almost perfect reconstruction. \n Table 3 . 3 Top-level codes are generally difficult to train well without spectral loss or with a single hierarchical autoencoder. Resulting reconstructions may lose some to most of information. Ablation Spectral convergence (dB) None Without spectral loss With single autoencoder −8.3 −6.3 2.9 \n takes a different approach of hierarchically generating accurate high-resolution Mel spec-trograms, after which a simple gradient-based optimization can produce high-fidelity audio.VQ-VAE: Oord et al. (2017)  introduced VQ-VAE, an approach of downsampling extremely long context inputs to a shorter-length discrete latent encoding using a vector quantization, and they showed that it can generate both highquality images and audio, as well as learn unsupervized representations of phonemes. Razavi et al. (2019)  extended the above model by introducing a hierarchy of discrete representations for images and showed that the resulting model can learn to separate high-level semantics into the highest level of discrete codes which have the largest receptive field, while capturing local features like textures in the lower levels with smaller receptive fields. They used the hierarchical model to generate high-diversity and high-fidelity images for the conditional ImageNet and FFHQ datasets. Dieleman et al. (2018)  tried variants of this approach where instead of a single encoder there are successive encoders that each further compress the lossy discrete encodings from the previous levels. A downside of this approach is that information is lost at each step and requires separate training for each VQ-VAE level, and it leads to a hierarchy collapse problem. De Fauw et al. (2019) used AR decoders which are known to cause the problem of ignoring the latent variables, and they suggested ways to mitigate it. The feed-forward decoders from \n While we are able to steer our current model somewhat through lyric and midi conditioning, we can imagine many other possible ways for humans to influence the generations, including indicating the mood or dynamic at various sections, or controlling when drums, singers, or other instruments should play.The current model takes around an hour to generate 1 minute of top level tokens. The upsampling process is very slow, as it proceeds sequentially through the sample. Currently it takes around 8 hours to upsample one minute of top level tokens. We can create a human-in-the-loop co-composition process at the top level only, using the VQ-VAE decoders to get a fast upsampling of the top level tokens to hear a very rough sense of what the model generates. The top-level model generates multiple samples, the person picks a fa-vorite (listening to the rough VQ-VAE decoding), and then the model continues generating multiple samples continuing the favorite. This process would be significantly improved with faster generation and Transformer upsampling steps. Our models have fast parallel evaluation of likelihood but slow autoregressive sampling. We can instead use a model with fast parallel sampling but slow autoregressive likeli-hood evaluation \n Hawthorne, C., Stasyuk, A., Roberts, A., Simon, I., Huang, C.-Z. A., Dieleman, S., Elsen, E., Engel, J., and Eck, D. Enabling factorized piano music modeling and generation with the MAESTRO dataset. In International Conference on Learning Representations, 2019. B. Experimental details B.1. Music VQ-VAE Hennequin, R., Khlif, A., Voituret, F., and Moussallam, M. Spleeter: A fast and state-of-the art music source separa-tion tool with pre-trained models. Late-Breaking/Demo ISMIR 2019, November 2019. Deezer Research. Hiller Jr, L. A. and Isaacson, L. M. Musical composition with a high speed digital computer. In Audio Engineering Society Convention 9. Audio Engineering Society, 1957. \n The encoder compresses the raw audio input into a sequence of embeddings. The length of this latent representation relative to the raw audio duration determines the amount of compression, and is an important factor for the trade-off between fidelity and coherence. ×L Dilated Conv1D Conv1D ×D x t Conv1D + h t Codebook (a) Gradient Passthrough Nearest-Neighbor Search z t h t Codebook Lookup e zt \n\t\t\t Richard Feynmann famously said, "What I cannot create, I do not understand" \n\t\t\t The gradient also needs to break symmetry with the encoder output features, which is the case here since the weights of the input projections in the attention are not zero.', acknowledgement=None, annex='A. Scalable Transformer We make the Sparse Transformer  (Child et al., 2019)  more scalable and easier to implement by a few small changes. We implement a simpler attention pattern that has the same performance without needing custom kernels to implement. We simplify the initialization by using the same initalization scale in the whole model without rescaling the weights based on fan-in and depth, and we optimize the memory footprint with fully half-precision training, i.e. storing the model weights, gradients and the optimizer states in half precision and performing computations in half precision as well. To cope with the narrower dynamic range of the fp16 format, we use dynamic scaling of the gradient and Adam optimizer states. Axis-aligned attention patterns: The Sparse Transformer  (Child et al., 2019)  sparsifies the attention pattern by reshaping the input sequence into a 2-D sequence of shape (blocks, block length) to use factorized attention. They observe that the strided attention pattern works best for images and audio because it does not have the state bottleneck of the fixed attention. However, their implementation require specialized CUDA kernels. We can obtain a similar pattern by doing masked row, masked column, and unmasked previous-row attention. While the masked row captures the local context, the masked column and unmasked previous-row attention captures the context of all previous rows. We observe the same computational speed as well as training loss with this pattern. Each of these can be implemented directly as a dense attention by transposing or slicing the input sequence along appropriate axes, and thus do not require special CUDA kernels to implement. This can be easily extended to video too. Complementary to our work, a similar pattern was introduced in  (Ho et al., 2019)  where they also used axis-aligned attention but instead used a two-stream architecture. Half-precision parameters and optimizer state with dynamic scaling: To allow training large models,  (Child et al., 2019)  uses recompute with gradient checkpointing, performs computations using half precision activations and gradients, and uses dynamic loss scaling. While this speeds up training on Volta cores, one still has a high memory usage from storing the parameters and Adam state in full float precision. To scale our models further, we store our matmul parameters and their Adam state in half precision, thus halving our memory usage. We use a single parameter s to set the scale of all weights and initialize all matmul and input/output embeddings 3 to N (0, s), and position embeddings to N (0, 2s). The initialization ensures all parameters are in a similar dynamic range, and allows us to train in half preci- 3 We share the input and output embedding (b) Combining two of the attention patterns, each position can attend to any of the previous positions, while not causing a state bottleneck as in fixed sparse attention  (Child et al., 2019) . Figure  6 . Axis-aligned attention patterns sion completely without loss in training performance. For the Adam state tensors (m_t, v_t) we do dynamic scaling. For each iteration and for every parameter, we rescale its state tensors before casting so that their maximum corresponds to the maximum value of the float16 range, thus maximizing the use of the float16 range. Thus, we store the state m_t as the tuple (scale, (m_t/scale).half()), where scale = m_t.max()/float16.max(), and similarly for v_t. The above lets us fit models of size 1B parameters into memory for our large context of 8192 tokens. To train even larger models, we use GPipe  (Huang et al., 2019b) . (a) The structure of our prior models, performing next-token prediction at each level. The Transformer takes the embeddings of the tokens z1:T −1 prepended by the sum of the artist and genre embeddings, in addition to the time embedding that encodes relative and absolute timing of the segments in the duration of the song. The upsampler priors additionally take the tokens from the upper level, which are fed to the conditioner network and added to the input sequence. The top-level prior takes lyrics as conditioning information as well (see Figure  8c ). \n ×D \n Dilated Conv1D Conv1D Token Embedding (b) The conditioner network takes the tokens from the upper level, and their embedding vectors go through non-causal WaveNet-like layers with increasingly dilated convolutions. The transposed 1-D convolution upsamples the sequence to the higher temporal resolution of the current level.')
GrobidDocument(grobid_version='0.7.2', grobid_timestamp='2023-02-12T16:33+0000', header=GrobidBiblio(authors=[GrobidAuthor(full_name='Jianfei Gao', given_name='Jianfei', middle_name=None, surname='Gao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Yangze Zhou', given_name='Yangze', middle_name=None, surname='Zhou', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Bruno Ribeiro', given_name='Bruno', middle_name=None, surname='Ribeiro', email=None, orcid=None, affiliation=None)], index=None, id=None, unstructured=None, date=None, title='Double Permutation Equivariance for Knowledge Graph Completion', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), pdf_md5='AD322F34D3F6348978627DA785751840', language_code='en', citations=[GrobidBiblio(authors=[GrobidAuthor(full_name='D J Aldous', given_name='D', middle_name='J', surname='Aldous', email=None, orcid=None, affiliation=None)], index=0, id='b0', unstructured=None, date='1981', title='Representations for partially exchangeable arrays of random variables', book_title=None, series_title=None, editors=None, journal='Journal of Multivariate Analysis', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='11', issue='4', pages='581-598', first_page='581', last_page='598', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Barceló', given_name='P', middle_name=None, surname='Barceló', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Kostylev', given_name='E', middle_name=None, surname='Kostylev', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Monet', given_name='M', middle_name=None, surname='Monet', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Pérez', given_name='J', middle_name=None, surname='Pérez', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Reutter', given_name='J', middle_name=None, surname='Reutter', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J.-P Silva', given_name='J.-P', middle_name=None, surname='Silva', email=None, orcid=None, affiliation=None)], index=1, id='b1', unstructured=None, date='2020', title='The logical expressiveness of graph neural networks', book_title='8th International Conference on Learning Representations (ICLR', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Barcelo', given_name='P', middle_name=None, surname='Barcelo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Galkin', given_name='M', middle_name=None, surname='Galkin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Morris', given_name='C', middle_name=None, surname='Morris', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M R Orth', given_name='M', middle_name='R', surname='Orth', email=None, orcid=None, affiliation=None)], index=2, id='b2', unstructured=None, date='2022', title='Weisfeiler and leman go relational', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2211.17113', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='B Bevilacqua', given_name='B', middle_name=None, surname='Bevilacqua', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Frasca', given_name='F', middle_name=None, surname='Frasca', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Lim', given_name='D', middle_name=None, surname='Lim', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Srinivasan', given_name='B', middle_name=None, surname='Srinivasan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Cai', given_name='C', middle_name=None, surname='Cai', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Balamurugan', given_name='G', middle_name=None, surname='Balamurugan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M M Bronstein', given_name='M', middle_name='M', surname='Bronstein', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Maron', given_name='H', middle_name=None, surname='Maron', email=None, orcid=None, affiliation=None)], index=3, id='b3', unstructured=None, date='2021', title='Equivariant subgraph aggregation networks', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Bordes', given_name='A', middle_name=None, surname='Bordes', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Weston', given_name='J', middle_name=None, surname='Weston', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Collobert', given_name='R', middle_name=None, surname='Collobert', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None)], index=4, id='b4', unstructured=None, date='2011', title='Learning structured embeddings of knowledge bases', book_title='Twentyfifth AAAI conference on artificial intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Bordes', given_name='A', middle_name=None, surname='Bordes', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Usunier', given_name='N', middle_name=None, surname='Usunier', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Garcia-Duran', given_name='A', middle_name=None, surname='Garcia-Duran', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Weston', given_name='J', middle_name=None, surname='Weston', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Yakhnenko', given_name='O', middle_name=None, surname='Yakhnenko', email=None, orcid=None, affiliation=None)], index=5, id='b5', unstructured=None, date='2013', title='Translating embeddings for modeling multi-relational data', book_title=None, series_title=None, editors=None, journal='Advances in neural information processing systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='26', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M M Bronstein', given_name='M', middle_name='M', surname='Bronstein', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Bruna', given_name='J', middle_name=None, surname='Bruna', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Lecun', given_name='Y', middle_name=None, surname='Lecun', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Szlam', given_name='A', middle_name=None, surname='Szlam', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Vandergheynst', given_name='P', middle_name=None, surname='Vandergheynst', email=None, orcid=None, affiliation=None)], index=6, id='b6', unstructured=None, date='2017', title='Geometric deep learning: going beyond euclidean data', book_title=None, series_title=None, editors=None, journal='IEEE Signal Processing Magazine', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='34', issue='4', pages='18-42', first_page='18', last_page='42', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='B P Chamberlain', given_name='B', middle_name='P', surname='Chamberlain', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Shirobokov', given_name='S', middle_name=None, surname='Shirobokov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Rossi', given_name='E', middle_name=None, surname='Rossi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Frasca', given_name='F', middle_name=None, surname='Frasca', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Markovich', given_name='T', middle_name=None, surname='Markovich', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Hammerla', given_name='N', middle_name=None, surname='Hammerla', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M M Bronstein', given_name='M', middle_name='M', surname='Bronstein', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Hansmire', given_name='M', middle_name=None, surname='Hansmire', email=None, orcid=None, affiliation=None)], index=7, id='b7', unstructured=None, date='2022', title='Graph neural networks for link prediction with subgraph sketching', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='2209.15486', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Y Chen', given_name='Y', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Mishra', given_name='P', middle_name=None, surname='Mishra', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Franceschi', given_name='L', middle_name=None, surname='Franceschi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Minervini', given_name='P', middle_name=None, surname='Minervini', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Stenetorp', given_name='P', middle_name=None, surname='Stenetorp', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Riedel', given_name='S', middle_name=None, surname='Riedel', email=None, orcid=None, affiliation=None)], index=8, id='b8', unstructured=None, date='2022', title='Refactor gnns: Revisiting factorisation-based models from a message-passing perspective', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Z Chen', given_name='Z', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Chen', given_name='L', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Villar', given_name='S', middle_name=None, surname='Villar', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Bruna', given_name='J', middle_name=None, surname='Bruna', email=None, orcid=None, affiliation=None)], index=9, id='b9', unstructured=None, date=None, title='Can graph neural networks count substructures?', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2020', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Z Chen', given_name='Z', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wang', given_name='Y', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Zhao', given_name='B', middle_name=None, surname='Zhao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Cheng', given_name='J', middle_name=None, surname='Cheng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Zhao', given_name='X', middle_name=None, surname='Zhao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Duan', given_name='Z', middle_name=None, surname='Duan', email=None, orcid=None, affiliation=None)], index=10, id='b10', unstructured=None, date='2020', title='Knowledge graph completion: A review', book_title=None, series_title=None, editors=None, journal='Ieee Access', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='8', issue=None, pages='192435-192456', first_page='192435', last_page='192456', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Cheng', given_name='K', middle_name=None, surname='Cheng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Liu', given_name='J', middle_name=None, surname='Liu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Wang', given_name='W', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Sun', given_name='Y', middle_name=None, surname='Sun', email=None, orcid=None, affiliation=None)], index=11, id='b11', unstructured=None, date='2022', title='Rlogic: Recursive logical rule learning from knowledge graphs', book_title='Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='179-189', first_page='179', last_page='189', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L De Raedt', given_name='L', middle_name=None, surname='De Raedt', email=None, orcid=None, affiliation=None)], index=12, id='b12', unstructured=None, date='2008', title='Logical and relational learning', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Springer Science & Business Media', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Defferrard', given_name='M', middle_name=None, surname='Defferrard', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Bresson', given_name='X', middle_name=None, surname='Bresson', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Vandergheynst', given_name='P', middle_name=None, surname='Vandergheynst', email=None, orcid=None, affiliation=None)], index=13, id='b13', unstructured=None, date='2016', title='Convolutional neural networks on graphs with fast localized spectral filtering', book_title=None, series_title=None, editors=None, journal='Advances in neural information processing systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='29', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Dettmers', given_name='T', middle_name=None, surname='Dettmers', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Minervini', given_name='P', middle_name=None, surname='Minervini', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Stenetorp', given_name='P', middle_name=None, surname='Stenetorp', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Riedel', given_name='S', middle_name=None, surname='Riedel', email=None, orcid=None, affiliation=None)], index=14, id='b14', unstructured=None, date='2018', title='Convolutional 2d knowledge graph embeddings', book_title='Proceedings of the AAAI conference on artificial intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='32', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L A Galárraga', given_name='L', middle_name='A', surname='Galárraga', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Teflioudi', given_name='C', middle_name=None, surname='Teflioudi', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Hose', given_name='K', middle_name=None, surname='Hose', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Suchanek', given_name='F', middle_name=None, surname='Suchanek', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Amie', given_name=None, middle_name=None, surname='Amie', email=None, orcid=None, affiliation=None)], index=15, id='b15', unstructured=None, date='2013', title='association rule mining under incomplete evidence in ontological knowledge bases', book_title='Proceedings of the 22nd international conference on World Wide Web', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='413-422', first_page='413', last_page='422', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Galkin', given_name='M', middle_name=None, surname='Galkin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Denis', given_name='E', middle_name=None, surname='Denis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Wu', given_name='J', middle_name=None, surname='Wu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W L Hamilton', given_name='W', middle_name='L', surname='Hamilton', email=None, orcid=None, affiliation=None)], index=16, id='b16', unstructured=None, date='2021', title='Nodepiece: Compositional and parameter-efficient representations of large knowledge graphs', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Galkin', given_name='M', middle_name=None, surname='Galkin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Zhu', given_name='Z', middle_name=None, surname='Zhu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Ren', given_name='H', middle_name=None, surname='Ren', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Tang', given_name='J', middle_name=None, surname='Tang', email=None, orcid=None, affiliation=None)], index=17, id='b17', unstructured=None, date='2022', title='Inductive logical query answering in knowledge graphs', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=[GrobidAuthor(full_name='A H Oh', given_name='A', middle_name='H', surname='Oh', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Agarwal', given_name='A', middle_name=None, surname='Agarwal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Belgrave', given_name='D', middle_name=None, surname='Belgrave', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Cho', given_name='K', middle_name=None, surname='Cho', email=None, orcid=None, affiliation=None)], journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Gilmer', given_name='J', middle_name=None, surname='Gilmer', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S S Schoenholz', given_name='S', middle_name='S', surname='Schoenholz', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P F Riley', given_name='P', middle_name='F', surname='Riley', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Vinyals', given_name='O', middle_name=None, surname='Vinyals', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G E Dahl', given_name='G', middle_name='E', surname='Dahl', email=None, orcid=None, affiliation=None)], index=18, id='b18', unstructured=None, date='2017', title='Neural message passing for quantum chemistry', book_title='International conference on machine learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1263-1272', first_page='1263', last_page='1272', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Gori', given_name='M', middle_name=None, surname='Gori', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Monfardini', given_name='G', middle_name=None, surname='Monfardini', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Scarselli', given_name='F', middle_name=None, surname='Scarselli', email=None, orcid=None, affiliation=None)], index=19, id='b19', unstructured=None, date='2005', title='A new model for learning in graph domains', book_title='Proceedings. 2005 IEEE international joint conference on neural networks', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='2', issue=None, pages='729-734', first_page='729', last_page='734', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='W Hamilton', given_name='W', middle_name=None, surname='Hamilton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Ying', given_name='Z', middle_name=None, surname='Ying', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Leskovec', given_name='J', middle_name=None, surname='Leskovec', email=None, orcid=None, affiliation=None)], index=20, id='b20', unstructured=None, date='2017', title='Inductive representation learning on large graphs', book_title=None, series_title=None, editors=None, journal='Advances in neural information processing systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='30', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Heckerman', given_name='D', middle_name=None, surname='Heckerman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Meek', given_name='C', middle_name=None, surname='Meek', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Koller', given_name='D', middle_name=None, surname='Koller', email=None, orcid=None, affiliation=None)], index=21, id='b21', unstructured=None, date='2007', title='Probabilistic entity-relationship models, prms, and plate models', book_title='Introduction to statistical relational learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='201-238', first_page='201', last_page='238', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='S Hochreiter', given_name='S', middle_name=None, surname='Hochreiter', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Schmidhuber', given_name='J', middle_name=None, surname='Schmidhuber', email=None, orcid=None, affiliation=None)], index=22, id='b22', unstructured=None, date='1997', title='Long short-term memory', book_title=None, series_title=None, editors=None, journal='Neural computation', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='9', issue='8', pages='1735-1780', first_page='1735', last_page='1780', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Kejriwal', given_name='M', middle_name=None, surname='Kejriwal', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C A Knoblock', given_name='C', middle_name='A', surname='Knoblock', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Szekely', given_name='P', middle_name=None, surname='Szekely', email=None, orcid=None, affiliation=None)], index=23, id='b23', unstructured=None, date='2021', title='Knowledge graphs: Fundamentals, techniques, and applications', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='MIT Press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Kersting', given_name='K', middle_name=None, surname='Kersting', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L De Raedt', given_name='L', middle_name=None, surname='De Raedt', email=None, orcid=None, affiliation=None)], index=24, id='b24', unstructured=None, date='2007', title='Bayesian logic programming: theory and tool', book_title='Statistical Relational Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='291', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Kipf', given_name='T', middle_name=None, surname='Kipf', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Welling', given_name='M', middle_name=None, surname='Welling', email=None, orcid=None, affiliation=None)], index=25, id='b25', unstructured=None, date='2017', title='Semi-supervised classification with graph convolutional networks', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Koller', given_name='D', middle_name=None, surname='Koller', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Friedman', given_name='N', middle_name=None, surname='Friedman', email=None, orcid=None, affiliation=None)], index=26, id='b26', unstructured=None, date='2009', title='Probabilistic graphical models: principles and techniques', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='MIT press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Koller', given_name='D', middle_name=None, surname='Koller', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Friedman', given_name='N', middle_name=None, surname='Friedman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Džeroski', given_name='S', middle_name=None, surname='Džeroski', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Sutton', given_name='C', middle_name=None, surname='Sutton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Mccallum', given_name='A', middle_name=None, surname='Mccallum', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Pfeffer', given_name='A', middle_name=None, surname='Pfeffer', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Abbeel', given_name='P', middle_name=None, surname='Abbeel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M.-F Wong', given_name='M.-F', middle_name=None, surname='Wong', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Meek', given_name='C', middle_name=None, surname='Meek', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Neville', given_name='J', middle_name=None, surname='Neville', email=None, orcid=None, affiliation=None)], index=27, id='b27', unstructured=None, date='2007', title='Introduction to statistical relational learning', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='MIT press', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Kroetsch', given_name='M', middle_name=None, surname='Kroetsch', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Weikum', given_name='G', middle_name=None, surname='Weikum', email=None, orcid=None, affiliation=None)], index=28, id='b28', unstructured=None, date='2016', title='Special issue on knowledge graphs', book_title=None, series_title=None, editors=None, journal='Journal of Web Semantics', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='37', issue='38', pages='53-54', first_page='53', last_page='54', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='N Lao', given_name='N', middle_name=None, surname='Lao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W W Cohen', given_name='W', middle_name='W', surname='Cohen', email=None, orcid=None, affiliation=None)], index=29, id='b29', unstructured=None, date='2010', title='Relational retrieval using a combination of path-constrained random walks', book_title=None, series_title=None, editors=None, journal='Machine learning', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='81', issue='1', pages='53-67', first_page='53', last_page='67', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Y Lin', given_name='Y', middle_name=None, surname='Lin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Liu', given_name='Z', middle_name=None, surname='Liu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Sun', given_name='M', middle_name=None, surname='Sun', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Liu', given_name='Y', middle_name=None, surname='Liu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Zhu', given_name='X', middle_name=None, surname='Zhu', email=None, orcid=None, affiliation=None)], index=30, id='b30', unstructured=None, date='2015', title='Learning entity and relation embeddings for knowledge graph completion', book_title='Twenty-ninth AAAI conference on artificial intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Maron', given_name='H', middle_name=None, surname='Maron', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Ben-Hamu', given_name='H', middle_name=None, surname='Ben-Hamu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Serviansky', given_name='H', middle_name=None, surname='Serviansky', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Lipman', given_name='Y', middle_name=None, surname='Lipman', email=None, orcid=None, affiliation=None)], index=31, id='b31', unstructured=None, date='2019', title='Provably powerful graph networks', book_title='Advances in Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2156-2167', first_page='2156', last_page='2167', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Maron', given_name='H', middle_name=None, surname='Maron', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='O Litany', given_name='O', middle_name=None, surname='Litany', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Chechik', given_name='G', middle_name=None, surname='Chechik', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Fetaya', given_name='E', middle_name=None, surname='Fetaya', email=None, orcid=None, affiliation=None)], index=32, id='b32', unstructured=None, date='2020', title='On learning sets of symmetric elements', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='6734-6744', first_page='6734', last_page='6744', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C Meilicke', given_name='C', middle_name=None, surname='Meilicke', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Fink', given_name='M', middle_name=None, surname='Fink', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Wang', given_name='Y', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Ruffinelli', given_name='D', middle_name=None, surname='Ruffinelli', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Gemulla', given_name='R', middle_name=None, surname='Gemulla', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Stuckenschmidt', given_name='H', middle_name=None, surname='Stuckenschmidt', email=None, orcid=None, affiliation=None)], index=33, id='b33', unstructured=None, date='2018', title='Fine-grained evaluation of rule-and embedding-based systems for knowledge graph completion', book_title='International semantic web conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Springer', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='3-20', first_page='3', last_page='20', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='C Morris', given_name='C', middle_name=None, surname='Morris', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Ritzert', given_name='M', middle_name=None, surname='Ritzert', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Fey', given_name='M', middle_name=None, surname='Fey', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W L Hamilton', given_name='W', middle_name='L', surname='Hamilton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J E Lenssen', given_name='J', middle_name='E', surname='Lenssen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Rattan', given_name='G', middle_name=None, surname='Rattan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Grohe', given_name='M', middle_name=None, surname='Grohe', email=None, orcid=None, affiliation=None)], index=34, id='b34', unstructured=None, date='2019-07', title='Weisfeiler and leman go neural: Higher-order graph neural networks', book_title='Proceedings of the AAAI Conference on Artificial Intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='33', issue=None, pages='4602-4609', first_page='4602', last_page='4609', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Murphy', given_name='R', middle_name=None, surname='Murphy', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Srinivasan', given_name='B', middle_name=None, surname='Srinivasan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Rao', given_name='V', middle_name=None, surname='Rao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Ribeiro', given_name='B', middle_name=None, surname='Ribeiro', email=None, orcid=None, affiliation=None)], index=35, id='b35', unstructured=None, date='2019', title='Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Murphy', given_name='R', middle_name=None, surname='Murphy', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Srinivasan', given_name='B', middle_name=None, surname='Srinivasan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Rao', given_name='V', middle_name=None, surname='Rao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Ribeiro', given_name='B', middle_name=None, surname='Ribeiro', email=None, orcid=None, affiliation=None)], index=36, id='b36', unstructured=None, date='2019', title='Relational pooling for graph representations', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='4663-4673', first_page='4663', last_page='4673', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='R Murphy', given_name='R', middle_name=None, surname='Murphy', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Srinivasan', given_name='B', middle_name=None, surname='Srinivasan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Rao', given_name='V', middle_name=None, surname='Rao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Ribeiro', given_name='B', middle_name=None, surname='Ribeiro', email=None, orcid=None, affiliation=None)], index=37, id='b37', unstructured=None, date='2019', title='Relational pooling for graph representations', book_title='Proceedings of the 36th International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Neville', given_name='J', middle_name=None, surname='Neville', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Jensen', given_name='D', middle_name=None, surname='Jensen', email=None, orcid=None, affiliation=None)], index=38, id='b38', unstructured=None, date='2007', title='Relational dependency networks', book_title=None, series_title=None, editors=None, journal='Journal of Machine Learning Research', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='8', issue='3', pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Nickel', given_name='M', middle_name=None, surname='Nickel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Tresp', given_name='V', middle_name=None, surname='Tresp', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H.-P Kriegel', given_name='H.-P', middle_name=None, surname='Kriegel', email=None, orcid=None, affiliation=None)], index=39, id='b39', unstructured=None, date='2011', title='A three-way model for collective learning on multi-relational data', book_title='Icml', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Nickel', given_name='M', middle_name=None, surname='Nickel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Murphy', given_name='K', middle_name=None, surname='Murphy', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Tresp', given_name='V', middle_name=None, surname='Tresp', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Gabrilovich', given_name='E', middle_name=None, surname='Gabrilovich', email=None, orcid=None, affiliation=None)], index=40, id='b40', unstructured=None, date='2015', title='A review of relational machine learning for knowledge graphs', book_title='Proceedings of the IEEE', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='104', issue=None, pages='11-33', first_page='11', last_page='33', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Nickel', given_name='M', middle_name=None, surname='Nickel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Rosasco', given_name='L', middle_name=None, surname='Rosasco', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Poggio', given_name='T', middle_name=None, surname='Poggio', email=None, orcid=None, affiliation=None)], index=41, id='b41', unstructured=None, date='2016', title='Holographic embeddings of knowledge graphs', book_title='Proceedings of the AAAI Conference on Artificial Intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='30', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Paulheim', given_name='H', middle_name=None, surname='Paulheim', email=None, orcid=None, affiliation=None)], index=42, id='b42', unstructured=None, date='2017', title='Knowledge graph refinement: A survey of approaches and evaluation methods', book_title=None, series_title=None, editors=None, journal='Semantic web', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='8', issue='3', pages='489-508', first_page='489', last_page='508', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Qian Huang', given_name='Qian', middle_name=None, surname='Huang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H R Leskovec', given_name='H', middle_name='R', surname='Leskovec', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J', given_name='J', middle_name=None, surname=None, email=None, orcid=None, affiliation=None)], index=43, id='b43', unstructured=None, date='2022', title='Few-shot relational reasoning via connection subgraph pretraining', book_title='Neural Information Processing Systems', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='L D Raedt', given_name='L', middle_name='D', surname='Raedt', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Kersting', given_name='K', middle_name=None, surname='Kersting', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Natarajan', given_name='S', middle_name=None, surname='Natarajan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Poole', given_name='D', middle_name=None, surname='Poole', email=None, orcid=None, affiliation=None)], index=44, id='b44', unstructured=None, date='2016', title='Statistical relational artificial intelligence: Logic, probability, and computation', book_title=None, series_title=None, editors=None, journal='Synthesis lectures on artificial intelligence and machine learning', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='10', issue='2', pages='1-189', first_page='1', last_page='189', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D Ruffinelli', given_name='D', middle_name=None, surname='Ruffinelli', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Broscheit', given_name='S', middle_name=None, surname='Broscheit', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Gemulla', given_name='R', middle_name=None, surname='Gemulla', email=None, orcid=None, affiliation=None)], index=45, id='b45', unstructured=None, date='2020', title='You can teach an old dog new tricks! on training knowledge graph embeddings', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='D E Rumelhart', given_name='D', middle_name='E', surname='Rumelhart', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G E Hinton', given_name='G', middle_name='E', surname='Hinton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R J Williams', given_name='R', middle_name='J', surname='Williams', email=None, orcid=None, affiliation=None)], index=46, id='b46', unstructured=None, date='1986', title='Learning representations by back-propagating errors', book_title=None, series_title=None, editors=None, journal='nature', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='323', issue=None, pages='533-536', first_page='533', last_page='536', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Sadeghian', given_name='A', middle_name=None, surname='Sadeghian', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Armandpour', given_name='M', middle_name=None, surname='Armandpour', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Ding', given_name='P', middle_name=None, surname='Ding', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Z Wang', given_name='D', middle_name='Z', surname='Wang', email=None, orcid=None, affiliation=None)], index=47, id='b47', unstructured=None, date='2019', title='Drum: End-to-end differentiable rule mining on knowledge graphs', book_title=None, series_title=None, editors=None, journal='Advances in Neural Information Processing Systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='32', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Schlichtkrull', given_name='M', middle_name=None, surname='Schlichtkrull', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T N Kipf', given_name='T', middle_name='N', surname='Kipf', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Bloem', given_name='P', middle_name=None, surname='Bloem', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R V D Berg', given_name='R', middle_name='V D', surname='Berg', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='I Titov', given_name='I', middle_name=None, surname='Titov', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Welling', given_name='M', middle_name=None, surname='Welling', email=None, orcid=None, affiliation=None)], index=48, id='b48', unstructured=None, date='2018', title='Modeling relational data with graph convolutional networks', book_title='European semantic web conference', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='Springer', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='593-607', first_page='593', last_page='607', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='E W Schneider', given_name='E', middle_name='W', surname='Schneider', email=None, orcid=None, affiliation=None)], index=49, id='b49', unstructured=None, date='1973', title='Course modularization applied: The interface system and its implications for sequence control and data analysis', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='J Schulman', given_name='J', middle_name=None, surname='Schulman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Zoph', given_name='B', middle_name=None, surname='Zoph', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Kim', given_name='C', middle_name=None, surname='Kim', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Hilton', given_name='J', middle_name=None, surname='Hilton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Menick', given_name='J', middle_name=None, surname='Menick', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Weng', given_name='J', middle_name=None, surname='Weng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J F C Uribe', given_name='J', middle_name='F C', surname='Uribe', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Fedus', given_name='L', middle_name=None, surname='Fedus', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Metz', given_name='L', middle_name=None, surname='Metz', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Pokorny', given_name='M', middle_name=None, surname='Pokorny', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R G Lopes', given_name='R', middle_name='G', surname='Lopes', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Zhao', given_name='S', middle_name=None, surname='Zhao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Vijayvergiya', given_name='A', middle_name=None, surname='Vijayvergiya', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Sigler', given_name='E', middle_name=None, surname='Sigler', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Perelman', given_name='A', middle_name=None, surname='Perelman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Voss', given_name='C', middle_name=None, surname='Voss', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Heaton', given_name='M', middle_name=None, surname='Heaton', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Parish', given_name='J', middle_name=None, surname='Parish', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Cummings', given_name='D', middle_name=None, surname='Cummings', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Nayak', given_name='R', middle_name=None, surname='Nayak', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Balcom', given_name='V', middle_name=None, surname='Balcom', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Schnurr', given_name='D', middle_name=None, surname='Schnurr', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Kaftan', given_name='T', middle_name=None, surname='Kaftan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Hallacy', given_name='C', middle_name=None, surname='Hallacy', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Turley', given_name='N', middle_name=None, surname='Turley', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Deutsch', given_name='N', middle_name=None, surname='Deutsch', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='V Goel', given_name='V', middle_name=None, surname='Goel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Ward', given_name='J', middle_name=None, surname='Ward', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Konstantinidis', given_name='A', middle_name=None, surname='Konstantinidis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Zaremba', given_name='W', middle_name=None, surname='Zaremba', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Ouyang', given_name='L', middle_name=None, surname='Ouyang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Bogdonoff', given_name='L', middle_name=None, surname='Bogdonoff', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Gross', given_name='J', middle_name=None, surname='Gross', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Medina', given_name='D', middle_name=None, surname='Medina', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Yoo', given_name='S', middle_name=None, surname='Yoo', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='T Lee', given_name='T', middle_name=None, surname='Lee', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Lowe', given_name='R', middle_name=None, surname='Lowe', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Mossing', given_name='D', middle_name=None, surname='Mossing', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Huizinga', given_name='J', middle_name=None, surname='Huizinga', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R Jiang', given_name='R', middle_name=None, surname='Jiang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Wainwright', given_name='C', middle_name=None, surname='Wainwright', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Almeida', given_name='D', middle_name=None, surname='Almeida', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Lin', given_name='S', middle_name=None, surname='Lin', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Zhang', given_name='M', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Xiao', given_name='K', middle_name=None, surname='Xiao', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K Slama', given_name='K', middle_name=None, surname='Slama', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Bills', given_name='S', middle_name=None, surname='Bills', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Gray', given_name='A', middle_name=None, surname='Gray', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Leike', given_name='J', middle_name=None, surname='Leike', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Pachocki', given_name='J', middle_name=None, surname='Pachocki', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Tillet', given_name='P', middle_name=None, surname='Tillet', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Jain', given_name='S', middle_name=None, surname='Jain', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Brockman', given_name='G', middle_name=None, surname='Brockman', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='N Ryder', given_name='N', middle_name=None, surname='Ryder', email=None, orcid=None, affiliation=None)], index=50, id='b50', unstructured=None, date='2022-11', title='ChatGPT: Optimizing language models for dialogue', book_title='Official OpenAI Blog', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='M Schuster', given_name='M', middle_name=None, surname='Schuster', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='K K Paliwal', given_name='K', middle_name='K', surname='Paliwal', email=None, orcid=None, affiliation=None)], index=51, id='b51', unstructured=None, date='1997', title='Bidirectional recurrent neural networks', book_title=None, series_title=None, editors=None, journal='IEEE transactions on Signal Processing', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='45', issue='11', pages='2673-2681', first_page='2673', last_page='2681', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Shen', given_name='T', middle_name=None, surname='Shen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='F Zhang', given_name='F', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Cheng', given_name='J', middle_name=None, surname='Cheng', email=None, orcid=None, affiliation=None)], index=52, id='b52', unstructured=None, date='2022', title='A comprehensive overview of knowledge graph completion', book_title=None, series_title=None, editors=None, journal='Knowledge-Based Systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='109597', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='A Singhal', given_name='A', middle_name=None, surname='Singhal', email=None, orcid=None, affiliation=None)], index=53, id='b53', unstructured=None, date='2012-05', title='Introducing the Knowledge Graph: things, not strings', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note='Official Google Blog', doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='B Srinivasan', given_name='B', middle_name=None, surname='Srinivasan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='B Ribeiro', given_name='B', middle_name=None, surname='Ribeiro', email=None, orcid=None, affiliation=None)], index=54, id='b54', unstructured=None, date='2020', title='On the equivalence between positional node embeddings and structural graph representations', book_title='Eighth International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Z Sun', given_name='Z', middle_name=None, surname='Sun', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Hu', given_name='W', middle_name=None, surname='Hu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Q Zhang', given_name='Q', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Qu', given_name='Y', middle_name=None, surname='Qu', email=None, orcid=None, affiliation=None)], index=55, id='b55', unstructured=None, date='2018', title='Bootstrapping entity alignment with knowledge graph embedding', book_title=None, series_title=None, editors=None, journal='IJCAI', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='18', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Z Sun', given_name='Z', middle_name=None, surname='Sun', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z.-H Deng', given_name='Z.-H', middle_name=None, surname='Deng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J.-Y Nie', given_name='J.-Y', middle_name=None, surname='Nie', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Tang', given_name='J', middle_name=None, surname='Tang', email=None, orcid=None, affiliation=None)], index=56, id='b56', unstructured=None, date='2019', title='Rotate: Knowledge graph embedding by relational rotation in complex space', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Z Sun', given_name='Z', middle_name=None, surname='Sun', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Wang', given_name='C', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Hu', given_name='W', middle_name=None, surname='Hu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Chen', given_name='M', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Dai', given_name='J', middle_name=None, surname='Dai', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Zhang', given_name='W', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Qu', given_name='Y', middle_name=None, surname='Qu', email=None, orcid=None, affiliation=None)], index=57, id='b57', unstructured=None, date='2020', title='Knowledge graph alignment network with gated multi-hop neighborhood aggregation', book_title='Proceedings of the AAAI Conference on Artificial Intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='34', issue=None, pages='222-229', first_page='222', last_page='229', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='I Sutskever', given_name='I', middle_name=None, surname='Sutskever', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Tenenbaum', given_name='J', middle_name=None, surname='Tenenbaum', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='R R Salakhutdinov', given_name='R', middle_name='R', surname='Salakhutdinov', email=None, orcid=None, affiliation=None)], index=58, id='b58', unstructured=None, date='2009', title='Modelling relational data using bayesian clustered tensor factorization', book_title=None, series_title=None, editors=None, journal='Advances in neural information processing systems', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='22', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Teru', given_name='K', middle_name=None, surname='Teru', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Denis', given_name='E', middle_name=None, surname='Denis', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Hamilton', given_name='W', middle_name=None, surname='Hamilton', email=None, orcid=None, affiliation=None)], index=59, id='b59', unstructured=None, date='2020', title='Inductive relation prediction by subgraph reasoning', book_title='International Conference on Machine Learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='9448-9457', first_page='9448', last_page='9457', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Toutanova', given_name='K', middle_name=None, surname='Toutanova', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Chen', given_name='D', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None)], index=60, id='b60', unstructured=None, date='2015', title='Observed versus latent features for knowledge base and text inference', book_title='Proceedings of the 3rd workshop on continuous vector space models and their compositionality', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='57-66', first_page='57', last_page='66', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Trouillon', given_name='T', middle_name=None, surname='Trouillon', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Welbl', given_name='J', middle_name=None, surname='Welbl', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Riedel', given_name='S', middle_name=None, surname='Riedel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='É Gaussier', given_name='É', middle_name=None, surname='Gaussier', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Bouchard', given_name='G', middle_name=None, surname='Bouchard', email=None, orcid=None, affiliation=None)], index=61, id='b61', unstructured=None, date='2016', title='Complex embeddings for simple link prediction', book_title='International conference on machine learning', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='PMLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2071-2080', first_page='2071', last_page='2080', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='T Trouillon', given_name='T', middle_name=None, surname='Trouillon', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='C Dance', given_name='C', middle_name=None, surname='Dance', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='E Gaussier', given_name='E', middle_name=None, surname='Gaussier', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Welbl', given_name='J', middle_name=None, surname='Welbl', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Riedel', given_name='S', middle_name=None, surname='Riedel', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Bouchard', given_name='G', middle_name=None, surname='Bouchard', email=None, orcid=None, affiliation=None)], index=62, id='b62', unstructured=None, date='2017', title='Knowledge graph completion via complex tensor factorization', book_title=None, series_title=None, editors=None, journal='Journal of Machine Learning Research', journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='18', issue='130', pages='1-38', first_page='1', last_page='38', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Veličković', given_name='P', middle_name=None, surname='Veličković', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Cucurull', given_name='G', middle_name=None, surname='Cucurull', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Casanova', given_name='A', middle_name=None, surname='Casanova', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Romero', given_name='A', middle_name=None, surname='Romero', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Lio', given_name='P', middle_name=None, surname='Lio', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None)], index=63, id='b63', unstructured=None, date='2017', title='Graph attention networks', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id='1710.10903', pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='P Veličković', given_name='P', middle_name=None, surname='Veličković', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='G Cucurull', given_name='G', middle_name=None, surname='Cucurull', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Casanova', given_name='A', middle_name=None, surname='Casanova', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='A Romero', given_name='A', middle_name=None, surname='Romero', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='P Lio', given_name='P', middle_name=None, surname='Lio', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Bengio', given_name='Y', middle_name=None, surname='Bengio', email=None, orcid=None, affiliation=None)], index=64, id='b64', unstructured=None, date='2018', title='Graph attention networks', book_title=None, series_title=None, editors=None, journal=None, journal_abbrev=None, publisher='ICLR', institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='H Wang', given_name='H', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='H Ren', given_name='H', middle_name=None, surname='Ren', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Leskovec', given_name='J', middle_name=None, surname='Leskovec', email=None, orcid=None, affiliation=None)], index=65, id='b65', unstructured=None, date='2021', title='Relational message passing for knowledge graph completion', book_title='Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='1697-1707', first_page='1697', last_page='1707', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Z Wang', given_name='Z', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Zhang', given_name='J', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Feng', given_name='J', middle_name=None, surname='Feng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Chen', given_name='Z', middle_name=None, surname='Chen', email=None, orcid=None, affiliation=None)], index=66, id='b66', unstructured=None, date='2014', title='Knowledge graph embedding by translating on hyperplanes', book_title='Proceedings of the AAAI conference on artificial intelligence', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume='28', issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='Z Wang', given_name='Z', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Q Lv', given_name='Q', middle_name=None, surname='Lv', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='X Lan', given_name='X', middle_name=None, surname='Lan', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Zhang', given_name='Y', middle_name=None, surname='Zhang', email=None, orcid=None, affiliation=None)], index=67, id='b67', unstructured=None, date='2018', title='Cross-lingual knowledge graph alignment via graph convolutional networks', book_title='Proceedings of the 2018 conference on empirical methods in natural language processing', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='349-357', first_page='349', last_page='357', note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Xu', given_name='K', middle_name=None, surname='Xu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='W Hu', given_name='W', middle_name=None, surname='Hu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='J Leskovec', given_name='J', middle_name=None, surname='Leskovec', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='S Jegelka', given_name='S', middle_name=None, surname='Jegelka', email=None, orcid=None, affiliation=None)], index=68, id='b68', unstructured=None, date='2019', title='How powerful are graph neural networks?', book_title='International Conference on Learning Representations', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages=None, first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None), GrobidBiblio(authors=[GrobidAuthor(full_name='K Xu', given_name='K', middle_name=None, surname='Xu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='L Wang', given_name='L', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='M Yu', given_name='M', middle_name=None, surname='Yu', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Feng', given_name='Y', middle_name=None, surname='Feng', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Y Song', given_name='Y', middle_name=None, surname='Song', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='Z Wang', given_name='Z', middle_name=None, surname='Wang', email=None, orcid=None, affiliation=None), GrobidAuthor(full_name='D Yu', given_name='D', middle_name=None, surname='Yu', email=None, orcid=None, affiliation=None)], index=69, id='b69', unstructured=None, date=None, title='Cross-lingual knowledge graph alignment via graph matching neural network', book_title='Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics (ACL)', series_title=None, editors=None, journal=None, journal_abbrev=None, publisher=None, institution=None, issn=None, eissn=None, volume=None, issue=None, pages='2019', first_page=None, last_page=None, note=None, doi=None, pmid=None, pmcid=None, arxiv_id=None, pii=None, ark=None, istex_id=None, url=None)], abstract='This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (& node) attributes (relations & node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve 100% Hits@10 test accuracy in both the WN18RRv1 and NELL995v1 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.', body='Introduction Knowledge graphs (KGs) are generally defined as structured representations of collections of facts in the form of a set of triplets S ⊆ V × R × V, where (i, r, j) ∈ S define two entities i (head entity) and j (tail entity) connected by a relation r, where both nodes and relations are finite: N = |V| < ∞ and R = |R| < ∞. In some applications KGs naturally define conjunctive logical statements (as in Figure  1 (a)): (i, Father, j) ∧ (j, Father, u) ∧ (i, Grand, u) ∧ (i, Father, u) ∧ etc., where R = {Father, Grand, . . .} and V = {i, j, u, . . .}. Unfortunately, KGs are often incomplete. Hence, the task of predicting missing relations (e.g., predict missing (i, r, j) ∈ V × R × V) is both widely-studied and a key task 1 Department of Computer Science, Purdue University, West Lafayette, IN, USA 2 Department of Statistics, Purdue University, West Lafayette, IN, USA. Correspondence to: Jianfei Gao <gao462@purdue.edu>. in knowledge base construction, often denoted as knowledge graph completion  (Bordes et al., 2013; Nickel et al., 2015; Teru et al., 2020) . As defined above (and in the literature  (Lin et al., 2015; Chen et al., 2020b; Kejriwal et al., 2021; Shen et al., 2022) ), knowledge graph completion is the task of predicting attributed edges, where not only we need to identify that a pair of nodes (i, j) ∈ V × V is a missing edge in the KG, but also determine which relation r ∈ R the edge (i, j) has. Treating KGs as attributed graphs allows researchers to adapt Graph Neural Network (GNN) methods used for link prediction with only minor modifications: Distinct pooling operations for each edge type (regularized to avoid overfitting), and changing the output from binary classification (edge prediction) to multi-label classification (predicting R relation labels). Roughly, this is the formula followed by RGCN  (Schlichtkrull et al., 2018) , GraIL  (Teru et al., 2020) , NodePiece  (Galkin et al., 2021) ,  NBFNet (Zhu et al., 2021) , and ReFactorGNNs  (Chen et al., 2022) , among others. However, theoretically, are KGs just attributed graphs? Contributions. In this work we argue that some KGs belong to a new class of graphs (which we denote as doubly exchangeable attributed graphs) whose node and pairwise representations must be equivariant to the action of the permutation group composed by the permutation subgroups of node ids, edge attributes (relations), and node arXiv:2302.01313v1 [cs.LG] 2 Feb 2023 attributes (theoretically modeled as self-edge attributes without loss of generality). This equivariance imposes a type of structural learning akin to inductively learning to answer a subset of Horn clauses in the test KG where both entities and relations are subject to universal quantifiers (Definition 4.9), e.g., for a given pair i, j ∈ V test , and ∀r 1 , r 2 ∈ R test , ∀u ∈ V test , (i, r 1 , u) ∧ (u, r 2 , j) =⇒ (i, r 2 , j). If S train 1 , S train 2 , . . . ⊆ V train × R train × V train are the training KGs, this equivariance allows the trained predictor to perform predictions over S test ⊆ V test × R test × V test in the test KG, where V test and V train , R test and R train are all potentially distinct sets (with potentially distinct sizes). We believe our double-permutation equivariance could be similarly impactful for KG machine learning as (single) permutation-equivariance has been for graph machine learning. Our work will focus on inductive KG completion tasks, but the same methods can also be applied transductively. There is no existing KG completion task in the literature that can test the full power of our approach. Our experimental results use the inductive WN18RR-v1 and NELL995-v1 KG completion tasks, which have new nodes in test but not more and completely new relations. We also design two harder synthetic tasks. No existing KG completion method is able to perform one of our synthetic tasks (see Table  2 ), where the test KG has more and new relations than the training KG. \n Theory review: What are attributed graphs? An exchangeability perspective Theoretically, a multigraph -we will refer to multigraphs as graphs-with N ≥ 2 vertices is a sequence of edges A (N,R) = (A 1,1 , A 1,2 , . . . , A N,N ) ∈ A N 2 R , where A R is some arbitrary domain that encodes R ≥ 1 relation attributes -e.g., in KGs A R is a vector representing multiple edges and their attributes (i.e., multiple relations). Without loss of generality, node attributes will be defined as a special type of edge reserved for self-loops A i,i . In most applications, what distinguishes a graph from a sequence is the assumption that the choice of node ids to create this sequence is arbitrary. Hence, any prediction that uses (A 1,1 , . . . , A N,N ) as input should be invariant 1  to the permutation of node ids  (Srinivasan & Ribeiro, 2020) . In statistics, this property is known as joint (array) exchangeability  (Aldous, 1981) . GNNs (without positional encoding) are permutation-equivariant representation functions, possessing the correct invariances for node and graph-wide classification tasks  (Xu et al., 2019a; Morris et al., 2019; Srinivasan & Ribeiro, 2020) . Link prediction is better served by equivariant pairwise representations  (Srinivasan & Ribeiro, 2020) . More precisely, and without loss of generality, let V (N ) = {1, . . . , N } be the set of nodes (i.e., node ids). For consistency of the notation with knowledge graph, we denote (A (N,R) ) i,r,j = A i,j,r . Let π ∈ S N be a permutation from the symmetric group S N with degree N , and R) ) i,r,j be the action of permutation π on the sequence (A 1,1 , . . . , A N,N ), which permutes node ids according to π, that is, π (N ) . A function that outputs a d-dimensional node representations of any-size graphs is defined as (π • A (N,R) ) π•i,r,π•j = (A (N, • i = π i , ∀(i, r, j) ∈ V (N ) × R (R) × V Γ node : ∪ ∞ R=1 ∪ ∞ N =2 (V (N ) × A N 2 R ) → R d , d ≥ 1, should be invariant to node id permutations. That is, Γ node (i, A (N,R) ) = Γ node (π • i, π • A (N,R) ), ∀i ∈ V. Similarly, a neural network that outputs d-dimensional pairwise representations of any-size graphs is defined as Γ pair : ∪ ∞ R=1 ∪ ∞ N =2 (V (N ) ×V (N ) ×A N 2 R ) → R d . Pairwise representation should also be invariant to the action of any π ∈ S N , i.e., Γ pair ((i, j), A (N,R) ) = Γ pair ((π • i, π • j), π • A (N,R) ). We can also define a graph-wide representation Γ gra : ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R → ∪ ∞ R=1 ∪ ∞ N =2 R N ×R×N ×d , noting that only the mappings between the domain and image that have the same values of R and N are possible. The representation Γ gra is equivariant, that is, for π ∈ S N , π • Γ gra (A (N,R) ) = Γ gra (π • A (N,R) ). GNNs and nearly all recent advances in graph representation learning are driven by the above invariances and equivariances  (Bronstein et al., 2017; Chen et al., 2020a; Defferrard et al., 2016; Gilmer et al., 2017; Gori et al., 2005; Hamilton et al., 2017; Maron et al., 2019; Morris et al., 2019; Murphy et al., 2019b; a; Srinivasan & Ribeiro, 2020; Teru et al., 2020; Xu et al., 2019a; Zhu et al., 2021) . \n Brief Related Work: Knowledge graphs as attributed multigraphs Should there be extra assumptions in some KGs beyond the joint node id permutation exchangeability of attributed graphs? In an excellent introduction on KGs,  Kejriwal et al. (2021)  warns the reader that "however, we must also deal with the uncomfortable notion that KG is still not very well defined (which makes KG representation challenging because no one representation can be held to be "correct")." In what follows we provide a brief history of knowledge graphs in the literature. To the best of our knowledge the term knowledge graph was first introduced by  Schneider (1973)  to describe a tutoring system, where each node describes a concept and each arc (direct edge) describes an attributed association between concepts. By 2012, KGs received renewed interest when Google revealed them as a key ingredient in its successful search engine, "things not strings" as described in  Singhal (2012) . In light of recent advances in large language models  (Schulman et al., 2022) , the discussion whether knowledge can be described by things or strings gain renewed interest. And we believe our work sheds new light into this discussion, since we show that complex logical relations are the product of forcing an invariance (and not directly learned from associations in the data). The view of KGs as attributed (multi)graphs -sometimes denoted heterogeneous graphswas somewhat consolidated in the semantic web literature around 2016  (Kroetsch & Weikum, 2016; Paulheim, 2017)  and by early work on knowledge bases  (Bordes et al., 2011) , that later was able to integrate classical AI methods (based on knowledge bases and logic), statistical relational learning (SRL)  (De Raedt, 2008; Koller et al., 2007; Kersting & De Raedt, 2007; Heckerman et al., 2007; Neville & Jensen, 2007) , and attributed graph completion methods KGs  (Bordes et al., 2013; Nickel et al., 2015; Teru et al., 2020) . In the SRL literature (e.g.,  Raedt et al. (2016) ) the attribute of an edge (i, j) ∈ V × V is sometimes instantiated as either a node r ∈ R or a node r(i, j), where r is the edge attribute (relation) (e.g.,  Heckerman et al. (2007) ). The drawback of adding edge attributes (relations in R) as nodes in a Bayesian network is that Bayesian networks are sequences (non-exchangeable), but, if treated as a graph (exchangeable), nodes and relations would be exchangeable among themselves (which in many KG applications would be incorrect, since V and R are fundamentally distinct sets). Exchangeability w.r.t. node ids in SRL appears in the form of lifting for parameterized templated graphical models, see  Koller & Friedman (2009)  and  Raedt et al. (2016, Chapter 3.1) . In practice, automatically finding these templates is difficult and tends to underperform when compared to more modern attributed graph methods for KG completion. State-of-the-art methods for KG completion treat KGs as attributed (multi)graphs (i.e., only node id exchangeable). They include tensor factorization methods  (Bordes et al., 2013; Trouillon et al., 2016; 2017; Sun et al., 2019)  (mostly applied in transductive KG tasks) and graph neural network methods (GNNs)  (Chen et al., 2022; Schlichtkrull et al., 2018; Galkin et al., 2021; Teru et al., 2020; Wang et al., 2021; Zhu et al., 2021)  (mostly applied in inductive KG tasks), among others. Interestingly, out of those, the most successful embedding methods (tensor or GNNs) tend to impose some form of TransE-style  (Bordes et al., 2013)  translation equivariance in the embeddings (or impose rotation invariance). This embedding equivariance is markedly different from relation equivariance, since here each relation has its own personalized shift. Due to space constraints, a more detailed discussion of related work can be found in Appendix B. 4. Proposal: Define some KGs as double exchangeable attributed (multi)graphs In the following text, we provide definitions and theoretical statements of our proposal in the main paper, while referring all proofs to Appendix A. Our model is intended for a broad class of KGs (but not all KGs may satisfy our conditions). The proposal starts with defining the concept of knowledge graph used in this paper: Definition 4.1 (Knowledge Graph (KG)). A knowledge graph is a multigraph A ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R sampled as A ∼ µ, where µ is some unknown distribution, and A R is the set encoding edge attributes (relations) 2  . For instance, if A has no node attributes, we can define A R ∈ {0, 1} R , where A i,r,j = 1 iff the relation (i, r, j) exists in the knowledge graph A. For homogeneous graph without node features, A 1 = {0, 1}. W.l.o.g. we define V (N ) = {1, . . . , N } and R (R) = {1, . . . , R}. If the KG has node attributes, A R also encodes them, to be used by the set of self-loops {A i,r,i : i ∈ V (N ) , r ∈ R (R,self) } for a special subset of relations R (R,self) R (R) . Often R (R) is described through a bijection to a set of sentences (e.g., 1 → Father, 2 → Grand, . . .). What distinguishes our KG definition from an attributed multigraph (Section 2) is the assumption that the distribution of µ is such that µ(A G ) = µ(A H ) for any isomorphic KGs A G and A H (A G KG H as in Definition 4.2). In this paper we denote this property of µ as double exchangeability. We then define the concept of KG isomorphism as: Definition 4.2 (KG Isomorphism). We say two multigraphs A G , A H ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R are isomorphic (denoted as A G KG A H ) if there exists a node bijection φ : V G → V H and a relation bijection τ : R G → R H preserving the set of relations, i.e., ∀(i, r, j) ∈ V G × R G × V G , (A G ) i,r,j = (A H ) φ(i),τ (r),φ(j) . Remark (vertex and relation set sizes): Note that by Definition 4.1, the set of all knowledge graphs is ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R . Common GNN representations can be learned and applied to graphs of different sizes. Similarly, our representations can be learned and applied to KGs with any number of nodes (N ≥ 2) and any number of relations (R ≥ 1). Invariant KG representations. It follows from Definition 4.1 that any (statistical) loss function (e.g., likelihood, regression (via energy-based models using distances), crossentropy) defined over a knowledge graph A train G must be the same over any isomorphic KGs A H KG A train G , i.e., the loss over A train G must be invariant to permutations of the node ids, edge attributes (relations), and node attributes (types). Consequently, we will design representations that are invariant to these two permutations, as we see later. Definition 4.3 (Permutation actions on KGs). For any KG A (N,R) ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R . As before, let φ ∈ S N be an element of the symmetric group S N (a permutation). The operation φ•A (N,R) is the action of φ on A (N,R) , defined as (N ) . In our definition of KGs, we also need relation permutations, where τ ∈ S R is a relation permutation and the action of τ on A (N,R) is defined as   (φ • A (N,R) ) φ•i,r,φ•j = (A (N,R) ) i,r,j , ∀(i, r, j) ∈ V (N ) × R (R) ×V ∀(i, r, j) ∈ V (N ) × R (R) × V (N ) , (τ • A (N,R) ) i,τ •r,j = (A (N,R) ) i, . ∀A G , A H ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R , we say two triplets (i, r, j) ∈ V G ×R G × V G , (i , r , j ) ∈ V H × R H × V H are isomorphic triplets iff A G and ∈ S N , ∃τ ∈ S R , such that φ • τ • A G = A H and (i , r , j ) = (φ • i, τ • r, φ • j). Definition 4.4 implies isomorphic triplets can only exist between (a) two (distinct) isomorphic KGs A G and A H , or (b) in the same graph A G = A H if ∃φ ∈ S N , ∃τ ∈ S R that are non-trivial automorphism, i.e., φ • τ • A G = A G and φ and τ are not identity maps. Now we can finally define our invariant triplet representation for KGs, which is invariant over isomorphic triplets. Definition 4.5 (Invariant triplet representation for KGs). \n For any KG A ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R . An invariant representation of a triplet (i, r, j), denoted as Γ tri ((i, r, j), A), where Γ tri : ∪ ∞ R=1 ∪ ∞ N =2 (V (N ) ×R (R) ×V (N ) ×A N 2 R ) → R d , d ≥ 1, is such that ∀(i, r, j) ∈ V (N ) × R (N ) × V (N ) , ∀φ ∈ S N , ∀τ ∈ S R , Γ tri ((i, r, j), A) = Γ tri ((φ•i, τ •r, φ•j), φ• τ • A). Remark (quotient group for preserving relations (and node attributes) that do not permute): One can also trivially extend our definitions to restrict exchangeability to a subset of relations. This is achieved by redefining the permutation group S R as its quotient group encompassing just the relations that permute, which then implies a trivial change to the definition of KG isomorphism in Definition 4.2. This is a straightforward modification of our approach. Similar to  (Srinivasan & Ribeiro, 2020) , we can define the most expressive invariant triplet representation. Definition 4.6 (Most-expressive invariant triplet representation). An invariant triplet representation \n Remark (scoring losses): For d = 1, Γ tri : ∪ ∞ R=1 ∪ ∞ N =2 (V (N ) × R (R) × V (N ) × A N 2 R ) → R can Γ tri is most ex- pressive iff ∀A G , A H ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R , ∀(i, r, j) ∈ V G × R G × V G , ∀(i , r , j ) ∈ V H × R H × V H , we will have Γ tri ((i, r, j), A G ) = Γ tri ((i , r , j ), A H ) iff (i, r, j) and (i , r , j ) are isomorphic triplets  (Definition 4.4) . In what follows we define representations for the whole KG (akin to how GNNs provide representations for a whole graph), which we denote as double equivariant KG representations. Definition 4.7 (Double equivariant KG representations). Let A ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R be a KG following Defini- tion 4.1. A function Γ gra : ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R → ∪ ∞ R=1 ∪ ∞ N =2 R N ×R×N ×d , d ≥ 1 is double equivariant w.r.t. arbitrary node φ ∈ S N and relation τ ∈ S R permutations, if Γ gra (φ•τ •A) = φ•τ •Γ gra (A). Moreover, valid mappings of Γ gra must map a domain element to an image element with the same number of nodes N and relations R. Next, we connect Definitions 4.5 and 4.7. Theorem 4.8. For all A ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R , given an invariant triplet representation Γ tri we can construct a double equivariant representation as (Γ gra (A)) i,r,j,: N ) , and vice-versa. := Γ tri ((i, r, j), A), ∀(i, r, j) ∈ V (N ) × R (R) × V ( Section 5 will introduce a double equivariant neural architecture based on Theorem 4.8. However, first we want to discuss the consequences of invariant representations, and how it can benefit KG tasks. \n Consequences of invariant predictors in KGs We will now analyze two KG completion tasks that are effectively impossible for all standard KG completion methods (based on attributed multigraphs), which are relatively easy for predictors based on our invariant KG representations. Consider the knowledge base in Figure  2 , obtained from a fictional alien civilization with 3 KGs for training and one for test. Knowing nothing about alien language and costumes, we note that in training all KG relations are different. Minimally, we could predict the missing relation in red in test data is not "≮". Note, however, that because all edge attributes are unique, assuming the KG is an attributed (multi)graph does not allow us to automatically infer this obvious logical rule, since whatever rules are learned for one relation are not directly applicable to others. Now let\'s consider sending our aliens a KG with a the biological human relationships in Figure  1(a) . Given a set of biological male relations as training data in Figure  1(a) , under what assumptions could the alien be able to predict (without knowledge of our language or physiology) the relation "(Ellie, Grand ∧ Mother, Hanna)" between Ellie and Hanna in the (hold-out) test data of Figure  1 (b)? Thankfully, the tasks in Figures  1 and 2  can both be solved under our definition of KG (Definition 4.1). Due to required invariance, any triplet representation (i, r, j) ∈ V × R × V in either train or test (Definition 4.5) can only pay attention to the structural relations between nodes and their relations, not their absolute ids (node id and relation id). In the KG of Figure  2 , any representation invariant to both permutations in training can only encode that any relation is unlike any other relation, that is, a self-supervised trained predictor created by removing the triplet ( , , ) and trying to predict it back must predict a uniform distribution over the remaining relations R train \\{ }. If all train KGs are treated as a single (disconnected) KG, the uniform prediction is over R train \\{∆, †, , , }. In test, this predictor would predict the relation "?" uniformly over the set R test \\{≮}, which is really all we know about the aliens. In the task of Figure  1   \n Connection to Learning Logical Rules We now define universally quantified entity and relation Horn clauses for our tasks, and show that any predictor that can be learned from the invariant triplet representation in Definition 4.2 has an equivalent predictor as a conjunction of such Horn clauses.   ∀C 1 ∈ R, (∀C r ∈ R \\ {C 1 , . . . , C r−1 }) K r=2 , ∀E 1 ∈ V, (∀E i ∈ V \\ {E 1 , . . . , E i−1 }) M i=2 , i,j=1,...,M, r=1,...,K, Bi,r,j =1 (E i , C r , E j ) =⇒ (E 1 , C 1 , E h ), (1) M m=3 K r =1 B m,r ,a + B a,r ,m ≥ 1. Note that our definition of UQER Horn clauses (Definition 4.9) is a generalization of the first order logic (FOL) clauses in  (Yang et al., 2017; Meilicke et al., 2018; Sadeghian et al., 2019; Teru et al., 2020)  such that the relations in the Horn clauses are also universally quantified rather than predefined constants. Note that our Horn clauses need not to form a path in the KG, since some relevant associations between relations could be in disconnected subgraphs. Figure  3  exemplifies the connection between Definition 4.9 and our KG definition (Definition 4.1). In the training KG, we can see that (x1, couple of, x2) ∧ (x2, lives in, x3) =⇒ (x1, lives in, x3). According to that, we may simply learn that, in a KG, for any two different relations in R and any three different entities in V, if they form a logic chain of length 2 with distinct relations, then the second relation on the chain also exists between the source and destination entities of the chain. Using Equation (1) we would write this as ∀C 1 ∈ R, ∀C 2 ∈ R \\ {C 1 }, ∀E 1 ∈ V, ∀E 2 ∈ V \\ {E 1 }, ∀E 3 ∈ V \\ {E 1 , E 2 }, (E 1 , C 1 , E 3 ) ∧ (E 3 , C 2 , E 2 ) =⇒ (E 1 , C 2 , E 2 ). Then, on test KG in Figure  3 , we will apply the above UQER Horn clause learned from training to predict all missing positive triplets. For instance, an arbitrary variable allocation, "classmate of", "studies in" and entities y4, y5, y6, allows all conjunctive conditions of our Horn clause to be satisfied, thus predicting (y4, studies in, y5) as a positive triplet. Two other triplets can similarly be predicted in dashed blue in Figure  3 . We now connect our double-invariant triplet representations in Definition 4.5 with the UQER Horn clauses in Definition 4.9. Theorem 4.10. Given an arbitrary triplet predictor η : ) is a positive triplet, there exists a set of UQER Horn clauses (Definition 4.9) that predicts the same positive triplets for all R d → {0, 1} that takes the triplet representation Γ tri ((i, r, j), A) in Definition 4.5 as input, A ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R , and predicts if (i, r, j) ∈ ∪ ∞ R=1 ∪ ∞ N =2 V (N ) × R (R) × V (N A ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R and (i, r, j) ∈ ∪ ∞ R=1 ∪ ∞ N =2 V (N ) × R (R) × V (N ) . The full proof in Appendix A shows how the universal quantification in Definition 4.9 implies a double-invariant predictor, where we can construct a set of Horn clauses and for each Horn clause, the left side are observed facts in KG and the right side is triplet predicted to be positive. By adding the UQER property, as a permutation equivariance of nodes and relations, the set of Horn clauses still hold, where the predictor η based on our invariant triplet representations Γ tri (Definition 4.5) also gives the same predictions. \n Inductive Double-Exchangeable Neural Architecture for KGs In this section we propose a model to learn invariant triplet representation for KGs. By Theorem 4.8, one way to obtain invariant triplet representation is to learn a double equivariant function (Definition 4.7). So we propose an inductive structural doubly-exchangeable architecture to learn double equivariant functions over KG. We start by looking at Definition 4.7 from another point of view. Consider A (N,R) given by Definition 4.1. Denote A (r) as the matrix A (r) i,j = (A (N,R) ) i,r,j , r ∈ R (R) . Note that the KG can be written as A (N,R) = (A (1) , . . . , A (R) ). Since the actions of the two permutation groups S N and S R commute, the double equivariance in Definition 4.7 over A (N,R) can be described as a φ ∈ S N (graph) equivariance over A (r) , r = 1, . . . , R, and a τ ∈ S R (set) equivariance (over the set of homogeneous graphs). Hence, our double equivariance can make use of the general framework proposed by  Maron et al. (2020) ;  Bevilacqua et al. (2021) . We start with a linear double-equivariant layer composed by a Siamese layer to define the k-th linear double-equivariant layer L (t) : ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R → ∪ ∞ R=1 ∪ ∞ N =2 R N ×R×N ×dt as follows, for each r = 1, ..., R: (L (t) (A (N,R) )) :,r = L (t) 1 (A (r) ) + L (t) 2 r ∈R\\{r} A (r ) , (2) where t = 1, . . . , T , T ≥ 2, L (t) 1 , L (t) 2 : ∪ ∞ N =2 A N 2 1 → ∪ ∞ N =2 R N ×N ×dt can be any GNN layers that outputs pairwise representations. The sum r ∈R\\{r} A (r ) can also be replaced by other set aggregators such as mean, max, etc.. Our implementation uses the max aggregator, where max {A (r ) } r ∈R\\{r} only cares if a pair of nodes is connected (no matter the edge attribute). Note that the proposed layer is similar to the H-equivariant layers proposed by  Bevilacqua et al. (2021)  for increasing the expressiveness of GNN using sets of subgraphs (a markedly different task than ours). We now can define our (double-equivariant) neural network for KGs: Definition 5.1 (Double-equivariant neural network). The double-equivariant network Γ gra : ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R → ∪ ∞ R=1 ∪ ∞ N =2 R N ×R×N ×d is defined by several linear double equivariant layers described in Equation (  2 ) interleaved with non-polynomial activation functions, Γ gra (A) = L (T ) (• • • σ(L (2) (σ(L (1) (A)))) • • • ), ( 3 ) where σ is the non-polynomial activation function (our implementations uses ReLU). \n Implementation considerations Most-expressive pairwise representations for L (k) 1 , L (k) 2 are computationally expensive. Moreover, even less expressive pairwise GNN layers in Equation (  2    2022 ), are still expensive (computationally and memory-wise). Thus, we propose inductive structural doubly-exchangeable architecture (IS-DEA), and implementation of Equation (  3 ) that trade-offs expressivity for speed by using equivariant GNN layers  (Kipf & Welling, 2017; Hamilton et al., 2017; Veličković et al., 2018)  for node representation. Specifically, for a KG A (N,R) , IS-DEA performs vertex message passing through two learnable functions, such as MLPs, recursively over T layers {L (t) } T t=1 . At each iteration t ∈ {1, 2, ..., T }, all vertices i ∈ V (N )  are associated with a learned vector h (t) i ∈ R R×dt , d t ≥ 1. Since we do not assume our KGs have node attributes, we consider initializing h (0) i = 1. Then we recursively compute the update, ∀i ∈ V (N ) , ∀r ∈ R (R) , h (t+1) i,r = MLP (t) 1 h (t) i,r , j∈Nr(i) h (t) j,r + MLP (t) 2 R r =r h (t) i,r , j∈∪ r =r N r (i) R r =r h (t) j,r , (4 ) where MLP (t) 1 and MLP (t) 2 denotes two multi-layer perceptron for the Siamese and aggregation function, N r (i) denotes the neighborhood set of i with relation r in the graph, N r (i) = {j|(i, r, j) ∈ S or (j, r, i) ∈ S} (with S as the KG triplets encoded by A (N,R) ) and ∪ j =r N r (i) denotes the neighborhood set of i in the graph defined by A (r) . In our implementation, we use GIN  (Xu et al., 2019a)  as our GNN architecture, which satisfies Equation (4). At the final layers, we use standard MLPs (which does not take neighborhood information as input) to output a final prediction. As shown by  Srinivasan & Ribeiro (2020); You et al. (2019) , structural node representations are not most-expressive for link prediction task in homogeneous graphs. The same issue happens for KGs. To ameliorate the issue, we concatenate i and j (double-equivariant) node representations with the distance between i and j in our triplet representation (appending distances is also adopted in the representations of  Teru et al. (2020) ;  Galkin et al. (2021) ). Finally, we obtain the triplet representation ∀(i, r, j) ∈ V (N ) × R (R) × V (N )   Γ IS-DEA ((i, r, j), A) = (h (T ) i,r h (T ) j,r d(i, j) d(j, i)), (5) where we denote d(i, j) as the length of the non-trivial shortest path from i to j without direct connection between i and j in the KG, || as the concatenation operation. Since the KG is directed, we concatenate distance on both directions. Lemma 5.2. The triplet representation in Equation (  5 ) is an invariant triplet representation as per Definition 4.5. Finally, as in previous KG works  (Yang et al., 2015; Schlichtkrull et al., 2018; Zhu et al., 2021) , we use negative sampling in our training procedure, where for each training triplet (i, r, j) ∈ S, we randomly corrupt either the subject or object n times to generate the negative example. Following  Schlichtkrull et al. (2018) , we use cross-entropy loss for model optimization to obtain predictions that will score positive examples higher than negative examples: L = − (i,r,j)∈S log(Γ tri ((i, r, j), A)) − 1 n n p=1 log(1 − Γ tri ((i p , r, j p ), A)) , (6) where (i p , r, j p ) are the p-th negative examples. \n Experiments We evaluate IS-DEA on two synthetic tasks (that we propose to test the generalization capabilities of our method), FD-1 and FD-2; and on two inductive knowledge graph completion datasets, WN18RR-v1 and NELL995-v1, which are widely-used small-scale inductive knowledge graph completion benchmarks in literature  (Teru et al., 2020; Zhu et al., 2021) . WN18RR-v1 has an easy task  (Toutanova & Chen, 2015) . We use these small-scale graphs since our approach has a similar scalability to GraIL  (Teru et al., 2020) , where one pre-processes the graph into egonets for each triplet in the minibatch. In all results, we report mean performance over 5 runs, and all variances are omitted since they are quite small, which is consistent with the reporting of  \n Synthetic Experiments In the synthetic experiments, we propose two challenging family tree completion tasks in order to verify the theoretical benefits of our model: FD-1 is used to show that our model is insensitive to relation identity; FD-2 is used to show that our model can automatically generalize to new nodes and relations. These tasks are described next.   2  expresses that none of our baselines can perform this task (since, as they assume an attributed multigraph as input, they all need to learn parameters for each relation). Since IS-DEA does not learn parameters specific to relations, it is the only method that can inductively infer over a KG with new and more relations in test, and achieving very good performance on FD-2 as shown in Table  2 . \n Family Diagram 1 (FD-1). A simplified version of \n Real-world Knowledge graphs As far as we know, there are no real-world benchmarks where training and test KGs have distinct nodes and relations. Therefore, our real-world evaluation of inductive knowledge graph completion is limited to tasks that existing methods can also perform. Unfortunately, due to the complexity of the pre-processing step (similar to GraIL  (Teru et al., 2020) ) and training cost of the proposed IS-DEA, our experiments are currently limited to small-scale KGs. Thus, we select the smallest two benchmarks, WN18RR-v1 and NELL995-v1 with at most 7,000 fact triplets and 14 relations to test our proposal. In order to highlight the relation-invariance property of our proposal, we also perform a task where all relation IDs are randomly shuffled only in test. Our results for WN18RR-v1 and NELL995-v1 are reported in Table  3 . We can see that IS-DEA results are always invariant to the permutation of relations in test, while all baselines become worse at least on WN18RR-v1 if relations are permuted in test. Besides, IS-DEA obtains a perfect score on the key metric Hits@10, and particularly, is the best of all metrics for WN18RR-v1. We note in passing that we had to rerun all baselines. For nearly all baselines, we were able to improve their original results on the same benchmarks by better hyperparameter search (except NBFNet in WN18RR-v1). Table  5  reproduces Table  3  with baseline performances taken from original papers. Our conclusions remain the same, as expected. We also note that WN18RR-v1 and NELL995-v1 are easy tasks that do not test the full capabilities of IS-DEA. Hopefully our work will inspire future benchmark datasets with harder tasks that cannot be performed by existing methods. While double exchangeability may not be the right assumption for all KGs, it is clearly beneficial for some KGs. Our experiments treat all relations as exchangeable. Further research is needed to better understand which relations are exchangeable and which are not for a given KG. We also believe that using true pairwise representation can improve the performance of IS-DEA. Ablation Study. We also perform an ablation in Table  6  (Appendix) that verifies that, in practice, the shortest path distance features are not essential for IS-DEA in real-world datasets (since real-world KGs are likely asymmetric, where structural node embeddings have similar expressivity to structural pairwise embeddings). IS-DEA is still the bestperforming method even without shortest path distances. Limitations. IS-DEA excels both in synthetic and realworld benchmarks. However, the simplification from pairwise to node embeddings in IS-DEA limits its expressivity. In Appendix D.4, we give a synthetic counterexample how this could be an issue in some KGs. Moreover, IS-DEA has the same poor pre-processing scalability as GraIL. We leave these limitations as future works (see Appendix E). Besides, we do not see any negative social impact of our proposal. \n Conclusions In this work we introduced the concept of double exchangeable attributed graphs as a formal model for KGs, challenging the view that KGs are attributed graphs (with exchangeable node ids). We showed that, similar to how node id symmetries impose learning structural node embeddings in homogeneous graphs, double symmetries (node and relation ids) impose structural rule learning in KGs. We then introduced a blueprint for double equivariant neural network architectures for KGs, which adapts permutation-equivariance to both KG entities and relations. We showed this architecture can learn logical rules that standard KG methods cannot. Finally, experiments showed that even a simple double exchangeable architecture (IS-DEA) achieves promising results in inductive KG completion tasks, a significant improvement over baselines.  \n B. Related Work Factorization-based method for KG A widely popular way to tackle KG completion tasks is through latent representations of entities and relations. The basic principle is that the embedding should capture their relative information in the KG. Traditionally, factorization-based methods  (Sutskever et al., 2009; Nickel et al., 2011; Bordes et al., 2013; Wang et al., 2014; Yang et al., 2015; Trouillon et al., 2016; Nickel et al., 2016; Trouillon et al., 2017; Dettmers et al., 2018; Sun et al., 2019)  have been proposed to obtain latent embedding of entities and relations. These models try to score all combinations of relations and entities with embedding as factors, similar as tensor-factorization. Although excellence in transductive tasks, it is not possible to apply on inductive tasks on unseen entities without extensive retraining. GNN-based model for KG In recent years, with the advancement of graph neural networks (GNNs)  (Defferrard et al., 2016; Kipf & Welling, 2017; Hamilton et al., 2017; Veličković et al., 2017; Bronstein et al., 2017; Murphy et al., 2019c) , in graph machine learning fields, various works has applied the idea of GNN in relational prediction to ensure the inductive capability of the model, including RGCN  (Schlichtkrull et al., 2018) , GraIL  (Teru et al., 2020) , NodePiece  (Galkin et al., 2021) ,  NBFNet (Zhu et al., 2021) , ReFactorGNNs  (Chen et al., 2022)  etc.. These models can be used to infer on unseen entities at test time without extensive retraining as the factorization-based methods, while most of the GNN performance are worse than FM-based methods  (Ruffinelli et al., 2020; Chen et al., 2022) . Specifically,  Teru et al. (2020)     2021 ) extends the Bellman-Ford algorithm which learns pairwise representations by all the path representations betweneen nodes.  Barcelo et al. (2022)  tries to understand KG-GNNs expressiveness by connecting it with the Weisfeiler-Leman test in KG. Qian  Huang & Leskovec (2022)  aims to perform inductive reasoning over new relations, sharing the same interest as our work. However, the difference is that they frame it as a few-shot learning problem with few examples of new relations given, while we do not have access to any new relations. \n Logical Induction The relation prediction problem in knowledge graph can also be considered as the problem of learning first-order logical Horn clauses  (Yang et al., 2015; 2017; Sadeghian et al., 2019; Teru et al., 2020)  from the knowledge graph, where one aims to extract logical rules on binary predicates.  Barceló et al. (2020)  discusses the connection between the expressiveness of GNNs and first-order logical induction, but only on node GNN representation and logical node classifier. In our paper, we try to build connection between triplet representation and logical Horn clauses. Traditionally, logical rules are learned through statistically enumerating patterns observed in KG  (Lao & Cohen, 2010; Galárraga et al., 2013) . Neural LP  (Yang et al., 2017)  and DRUM  (Sadeghian et al., 2019)  learns logical rules in an end-to-end differentiable manner using the set of logic paths between two entities with sequence models.  Cheng et al. (2022)  follows a similar manner which breaks a big sequential model into small atomic models in a recursive way.  Galkin et al. (2022)  aims to inductively extract logical rules by devising NodePiece  (Galkin et al., 2021) and NBFNet (Zhu et al., 2021) . Knowledge graph alignment Knowledge graph alignment tasks  (Sun et al., 2018; 2020; Yan et al., 2021)  are very common in heterogeneous, cross-lingual, and domain-specific knowledge graphs, where the task aim to align entities among different domains. For example, matching entities with there counterparts in different languages  (Wang et al., 2018; Xu et al., 2019b) . It is intrinsically different than our task where we aim to inductively apply on completely new entities and relations, possibly with no clear alignments between them. \n C. Detailed Comparison between Different Methods In this section, we give a detailed comparison between different factorization-based methods, GNN and logical induction methods. Suppose for a focusing triplet (i, r, j), we are provided with (i, j)\'s enclosed subgraph G (i,j)  (Zhang & Chen, 2018; Teru et al., 2020)  which is a subgraph contains only nodes N (T ) (i) ∩ N (T ) (j) where N (T ) (i) are all neighbors within T -hop of node i, and direct connections between i and j inside the subgraph are removed for self-supervision. T is an arbitrary number which should be the same for all methods for comparison fairness. Given enclosed subgraph G (i,j) as the input, difference between all considered methods for inductive KG completion is how to achieve representation of (i, r, j) from the enclosed subgraph. For tensor factorization methods including RotatE  (Sun et al., 2019) , pRotatE  (Sun et al., 2019) , TransE  (Bordes et al., 2013) , ComplEx  (Trouillon et al., 2016 ), DistMult (Yang et al., 2015) , we will have two learnable embedding matrices H (V) and H (R) , the the representation of (i, r, j) is f (H (V) i , H (R) r , H (V) j ) where f is a distance measurement between H (V) i and H (V) j given H (R) r . The difference of methods in this category is the selection of f . For RGCN and ReFactorGNN, it is an extension of tensor factorization method with GNNs. First, it will fill H (V) and H (R) to corresponding nodes and edges in enclosed subgraph G (i,j) to generate an attributed enclosed subgraph G (i,j) , then an attributed graph neural network is applied on G (i,j) . We denote the final representation of node u given by GNN as GNN G (i,j) u . Then, the representation of (i, r, j) is achieved from f GNN G (i,j) i , GNN G (i,j) j . RGCN is sensitive to values of H (V) , while RefactorGNN is specially designed to be insensitive to that on inference. For NodePiece, it is a variant of former category. The only difference is that the final node representation GNN G (i,j) u of each node u is augmented with shortest distances to several anchors node in the training graph which are selected by arbitrary strategy, e.g., uniform sampling. For Neural LP and DRUM, they will first extract all different paths from i to j within G (i,j) . We denote the collection of all random paths as set W (i,j) , then the representation for (i, r, j) is READOUT({RNN(w)|∀w ∈ W (i,j) }) where READOUT is arbitrary aggregation function, e.g., sum, and RNN is arbitrary recurrent neural network  (Rumelhart et al., 1986; Hochreiter & Schmidhuber, 1997; Schuster & Paliwal, 1997) . The difference between these two methods is the RNN architecture. For GraIL and NBFNet, they will first assign node attributes for enclosed subgraph G (i,j) by strategy like DRNL, ZO  (Zhang et al., 2021; Chamberlain et al., 2022) , then fill H (R) to corresponding edge as edge attributes. This will result in an attributed enclosed subgraph G (i,j) . Next, an attributed graph neural network is applied on G (i,j) , and the representation for (i, r, j) is READOUT GNN G (i,j) where READOUT is arbitrary aggregation function, e.g., average over all nodes. NBFNet has optimized on data batching for better efficiency. For our proposed IS-DEA, the representation for (i, r, j) is f DSSGNN G (i,j) i , DSSGNN G (i,j) j , d i,j , d j,i where d i,j is the shortest distance from i to j in G (i,j) and DSS-GNN is as Equation (4). \n D. Experiments \n D.1. Dataset Generation Family Diagram 1 (FD-1). The generation of FD-1 is simply based on two logic chain rules as shown in Figure  5 : In training, we only have (X, Parent, Z) ∧ (Z, Father, Y ) =⇒ (X, Grand ∧ Father, Y ), while in test, we only have (X, Parent, Z) ∧ (Z, Mother, Y ) =⇒ (X, Grand ∧ Mother, Y ). Here (X, Parent, Z) means either (X, Father, Z) or (X, Mother, Z). For both training and test scenario, all direct father and mother relations (arcs) are provided as facts. A simplified generation process is illustrated in Figure  4 . The only difference in experiment is that true dataset has a complete binary tree of depth 6 while simplified version only has depth 2. The relation-independent Horn clauses sharing between training and test are quite simple: ∀r 1 , ∀r 2 , ∀r 3 , r 1 = r 2 = r 3 , there are two Horn clauses, (X, r 1 , Z) ∧ (Z, r 2 , Y ) =⇒ (X, r 3 , Y ) and (X, r 1 , Z) ∧ (Z, r 2 , Y ) =⇒ (X, r 2 , Y ). Since only father and mother appear in facts (the right side of imply symbol) in each Horn clauses, in the first Horn clause, r 3 will always be "grand". Family Diagram 2 (FD-2). The generation of FD-2 is an extension of Figure  3  except that we break connections like (y3, y6) for the easy of generation. Detailed generation steps are as following description: First, we select the same sharing Horn clause as used in the explanation of Theorem 4.10 and Figure  3  across training and test: ∀r 1 , ∀r 2 , r 1 = r 2 , (X, r 1 , Z) ∧ (Z, r 2 , Y ) → (X, r 2 , Y ). Then, we generate training facts from a complete binary tree of depth 6 with only relations "a" and "b" just as we did in FD-1, and apply the only relation-independent formerly defined to collect training queries (missing positive triplets) with only relation "b". Next, we repeat the same process on another two binary trees, one of whose relations are "1" and "2", the other of whose relations are "3" and "4". Finally, we merge all facts and queries from these two binary trees together as test dataset.   2021 ); For evaluation, we will not augment KG by inversions, and sample 50 negative triplets per positive to compute common metrics such as Mean Reciprocal Rank (MRR) and Hits@k as all baselines. For each positive sample, its negative samples are generated by uniformly corrupt either its subject or object by a random entity. We will filter out negative samples that collide with any positive triplets in facts and sample them again until there is no collision. Besides, we only corrupt objects in training, since corruption of subject can be achieved from corrupting object of inverse triplets  (Sun et al., 2019; Zhu et al., 2021) . Hyperparameters. We follow the same configuration as  Teru et al. (2020)  such that hidden layer is of 32 neurons, use Adam optimizer with learning rate 0.01, and weight decay 5e-4. For all datasets, we train our model 50 epochs with batch size 256. If the model is not improving for 15 epochs, we early stop the training. For all methods, number of hops and number of layers are 2 on FD-1 and FD-2, and are 3 on real-world inductive KG completion to ensure fair comparison. Complexity. For each layer of our method, it can be treated as running 2 homogeneous GNN |R| times on the KG, thus time cost is roughly 2|R| times of adopted GNN. In our experiment, we use GIN  (Xu et al., 2019a)  as our GNN architecture, thus the complexity is O(|R||S|d 3 ) where d is the maximum size of hidden layers, |R| is number of relations in the KG, and |S| is number of fact triplets (number of edges) in KG. Besides, our method requires the shortest distance between any two nodes without passing direct connection between two nodes for both positive and negative samples. Pay attention that this can not be simply achieved from Dijkstra or Floyd algorithm since the graph changes on computing each node pair, indeed computing such distance needs to traverse enclosed graph  (Zhang & Chen, 2018; Teru et al., 2020)  between each node pair once. Thus the complexity is the same as enclosed graph extraction which will be influenced by knowledge graph size and negative sampling rate. Roughly speaking, on popular transductive and inductive knowledge graph completion baselines, it takes days to months for extracting such information of a single run as a preprocessing step. \n D.3. More Result Explanation FD-1. Since FD-1 comes from an extremely simple generation process, we would expect our methods to achieve perfect performance on it (always rank positive triplets at rank 1 against all corresponding negative samples). However, it seems like that IS-DEA fails to achieve perfect performance (MRR and Hit@k all being 1.0) on this simple task. Indeed, there is no way to achieve such perfect performance on MRR, Hit@1 and Hit@2. The issue is that in the querying relations, for either father or mother relation prediction, there will be two equally good choices, e.g., (0, Father, X), X ∈ {3, 5};  for grand relation prediction, there will be four equally good choices, e.g., (0, Grand, X), X ∈ {3, 4, 5, 6}. But if we see Hit@4, IS-DEA achieves 100% accuracy. Another minor observation is that Neural LP and DRUM has exactly the same performance on FD-1. The reason is that Neural LP and DRUM has exactly the same framework except that the neural network architecture are slightly different. This observation is also found in  (Teru et al., 2020; Zhu et al., 2021) , and this also happens in later real-world experiments. Real-world Datasets. One interesting observation is that besides our proposal, a lot of baselines are also insensitive to relation shuffling on NELL995. The reason is that NELL995-v1 is an extremely sparse dataset with average node degree roughly 1, thus for a single node, most of nodes in the graph is unrelated regardless of the relation, thus the learning can be simply reduced to a naive link prediction task where relation ID has barely no influence. To verify this guess, we pick NBFNet, which is the best and relation-insensitive (from performance) baseline on NELL995-v1, freeze its relation-dependent parameters to be all-one, and run the experiments again, we can see that the performance has nearly no influence in Table  4  which reflects that knowledge graph completion on NELL995-v1 is nearly equivalent to link prediction on NELL995. Thus, it is possible for those methods to learn a relation-insensitive model on NELL995. Compare with Results Reported in Original Paper. In our reproduction of baselines on WN18RR-v1 and NELL995-v1, we find that some methods become slightly worse, while some becomes better on either datasets. This may relates to hyperparameter settings, and randomness of each method, thus for the fairness, we also report the comparison of our results directly against baseline performance reported in original papers. Since Hits@10 is the only metrics that is reported by all baselines, we report only this metric in Table  5 . NBFNet on NELL995-v1 is missing since it is not reported in original paper. Indeed, compare with Hits@10 column in Table  3 , the rank of methods is not influenced, thus the conclusion will not change. Ablation Study. Since negative samplings are drawn by uniformly corrupting object (without loss of generality), it is very likely that corrupted objects are far way from subject while true object is close to subject. Under such scenario, shortest distance itself will be a powerful enough feature to achieve good ranking performance in knowledge graph completion, thus we want to know if shortest distance feature augmentation contributes to the performance gain. As shown in Table  6 , even if shortest distance is excluded from our model, it still performs quite well and is only slightly hurts on WN18RR-v1. Thus, we can say that double-equivariant representation itself is enough to provide good performance. Besides, we also show in Table  7  that shortest distance itself is not enough for knowledge graph completion. ?Figure 1 . 1 Figure 1. (Biological human KG) Illustrative knowledge graphs of biological human relations. Exemplar inductive task: learn on training KG (a) to inductively predict missing relation "?" over Test KG (b) with new nodes (potentially more), new relations (potentially more), and new node features (potentially more). \n r,j , where we define the action of a permutation π ∈ S R , as π • r = π r . The node and relation permutation actions on A are commutative, i.e., φ • τ • A = τ • φ • A. We now define isomorphic triplets based on the notion of KG isomorphism. Definition 4.4 (Isomorphic triplets in KGs) \n A H have the same graph sizes and relation sizes, and ∃φ \n Figure 2 . 2 Figure 2. (Alien KG) Illustrative inductive knowledge graph completion task of our alien KG. The task is to inductively predict the missing relation "?" in red. Note that relations are all unique. lon et al., 2016; Chen et al., 2022), the invariant triplet representation has additional invariance properties. \n (a), once we remove (Bob, Grand ∧ Father, Hans) for training (via self-supervision), any invariant triplet predictor for the pair (Bob, Hans) that can correctly predict back the triplet (Bob, Grand ∧ Father, Hans) based on (1-hop) neighbor information from Bob and Hans in training must also be able to predict (Ellie, Grand ∧ Mother, Hanna) in the test KG of Figure 1(b). This is because, restricted to their respective 1-hop neighborhoods, the triplet (Bob, Grand ∧ Father, Hans) in the training KG of Figure 1(a) is isomorphic (Definition 4.4) to the triplet (Ellie, Grand ∧ Mother, Hanna) in the test KG of Figure 1(b). \n Figure 3 . 3 Figure 3. Example of an inductive KG completion task with new relations that can be explained by our Horn clauses. \n ), such as Zhang & Chen (2018); Zhu et al. (2021); Zhang et al. (2021); Zhou et al. ( \n Figure 5 . 5 Figure 5. FD-1 Generation Process. Above we show generation result of depth 2 for FD-1 dataset. Below we illustrate sharing Horn clause applied in both training and test generation. Blue means "Father"; orange means "Mother"; red means "Grand ∧ Father"; and green means "Grand ∧ Mother". In both training and test, direct father and mother are given as fact, and only dashed triplets are used for training loss or evaluation. \n Table 1 . 1 Inductive performance on Family Diagram 1. Existing baselines clearly struggle to perform this task. Model MRR↑ Hits@1↑ Hits@2↑ Hits@4↑ Neural LP 0.502 0.339 0.415 0.651 DRUM 0.502 0.339 0.415 0.651 GraIL 0.422 0.181 0.416 0.740 NBFNet 0.159 0.168 0.360 0.595 IS-DEA 0.832 0.700 0.903 1.000 Model MRR↑ Hits@1↑ Hits@2↑ Hits@4↑ Neural LP N/A N/A N/A N/A DRUM N/A N/A N/A N/A GraIL N/A N/A N/A N/A NBFNet N/A N/A N/A N/A IS-DEA 0.915 0.839 0.974 1.000 Teru et al. (2020);  Zhu et al. (2021). More experiment details including baselines, implementation details and ablation studies can be found in Appendix D. \n Table 2 . 2 Inductive Performance on Family Diagram 2. Only our method (IS-DEA) is able to perform this task. \n Yan, Y.,Liu, L., Ban, Y., Jing, B., and  Tong, H. Dynamic knowledge graph alignment. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 4564-4572, 2021. Yang, B., Yih, S. W.-t., He, X., Gao, J., and Deng, L. Em- bedding entities and relations for learning and inference in knowledge bases. In Proceedings of the International Conference on Learning Representations (ICLR) 2015, 2015. Yang, F., Yang, Z., and Cohen, W. W. Differentiable learning of logical rules for knowledge base reasoning. Advances in neural information processing systems, 30, 2017. You, J., Ying, R., and Leskovec, J. Position-aware graph neural networks. In International conference on machine learning, pp. 7134-7143. PMLR, 2019. Zhang, M. and Chen, Y. Link prediction based on graph neu- ral networks. Advances in neural information processing systems, 31, 2018. Zhang, M., Li, P., Xia, Y., Wang, K., and Jin, L. Label- ing trick: A theory of using graph neural networks for multi-node representation learning. Advances in Neural Information Processing Systems, 34:9061-9073, 2021. Zhou, Y., Kutyniok, G., and Ribeiro, B. OOD link prediction generalization capabilities of message-passing GNNs in larger test graphs. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. Zhu, Z., Zhang, Z., Xhonneux, L.-P., and Tang, J. Neural bellman-ford networks: A general graph neural network framework for link prediction. Advances in Neural Infor- mation Processing Systems, 34:29476-29490, 2021. \n extends the idea from Zhang & Chen (2018) to use local subgraph representations for KG link prediction. Chen et al. (2022) aims to build the connection between FM and GNNs, where they propose an architecture to cast FMs as GNNs. Galkin et al. (2021) uses anchor-nodes for parameter-efficient architecture for KG completion. Zhu et al. ( \n D.2. Experiment SetupBaselines and Implementation Details. In all experiments, we compare with four inductive and differentiable knowledge graph completion baselines, Neural LP(Yang et al., 2017), DRUM (Sadeghian et al., 2019) , GraIL (Teru et al., 2020), and   NBFNet (Zhu et al., 2021). Neural LP and DRUM treat KG as logical rule conjunctions, while GraIL and NBFNet treat KG as attributed graph. Same as Teru et al. (2020);  Zhu et al. (2021), we run each method 5 times on each dataset, and collect mean performance whose standard deviations are omitted since they are all small. For training of each single run, we augment each triplet (i, r, j) by its inversion (i, r −1 , j), and sample 2 negative triplets (i , r, j ) per positive in training as Sun et al. (2019);  Zhu et al. ( \n Table 4 . 4 Performance without/with Frozen Relation Embeddings. Freezing relation-dependent has barely no influence to NBFNet performance on NELL995-v1 which means that NELL995-v1 can be approximately solved as a link prediction task. NBFNet Configuration MRR↑ Hits@10↑ Original 0.442 0.998 With Frozen Relation Embeddings 0.442 0.996 Model Hits@10↑ WN18RR-v1 NELL995-v1 Neural LP 0.744 0.408 DRUM 0.744 0.194 GraIL 0.825 0.595 NBFNet 0.948 N/A IS-DEA 1.000 1.000 \n Table 5 . 5 Original Results of Baselines. NBFNet is not tested on NELL995, thus only its original performance is missing. In our reproduction of Table3, Neural LP and DRUM achieves better result than the original paper; GraIL is slightly worse on WN18RR, but is better on NELL995; NBFNet is slightly worse on WN18RR. Despite of those performance gap in reproduction, the rank of those methods are not changed comparing with Table3. \n\t\t\t Refer to the theory in(Srinivasan & Ribeiro,   \n\t\t\t ) for the sufficiency of invariances. \n\t\t\t We use A to denote arbitrary KG of any size instead of A(N,R)  , where N and R can be automatically inferred from A.', acknowledgement=None, annex='A. Proofs Theorem 4.8. For all A ∈ ∪ ∞ R=1 ∪ ∞ N =2 A N 2 R , given an invariant triplet representation Γ tri we can construct a double equivariant representation as (Γ gra (A)) i,r,j,: := Γ tri ((i, r, j), A), ∀(i, r, j) ∈ V (N ) × R (R) × V (N ) , and vice-versa. \n Proof. (⇒) For any KG ≥ 1 is an invariant triplet representation as in Definition 4.5. Using the invariant triplet representation, we can define a function Γ gra : N ) , (Γ gra (A)) i,r,j,: = Γ tri ((i, r, j), A). Then ∀φ ∈ S N , ∀τ ∈ S R , (Γ gra (φ•τ •A)) φ•i,τ •r,φ•j,: = Γ tri ((φ•i, τ •r, φ•j), φ•τ •A). We know Γ tri ((i, r, j), A) = Γ tri ((φ • i, τ • r, φ • j), φ • τ • A). Thus we conclude, ∀φ ∈ S N , ∀τ ∈ S R , ∀(i, r, j) ∈ V (N ) ×R (R) ×V (N ) , (φ R N ×R×N ×d is a double equivariant representation as Definition 4.7. Since Γ gra (φ Thus we show Γ tri is an invariant triplet representation as in Definition 4.5. Theorem 4.10. Given an arbitrary triplet predictor η : R d → {0, 1} that takes the triplet representation Γ tri  ((i, r, j)  ) is a positive triplet, there exists a set of UQER Horn clauses (Definition 4.9) that predicts the same positive triplets for all Proof. We show the proof by constructing a set of Horn clauses and prove that they have the desired properties. For any with N nodes and R relations. We first consider triplets (i + , r + , j + ) ∈ V (N ) × R (R) × V (N ) that predicted to be positive by the model, i.e., η(Γ tri ((i + , r + , j + ), A)) = 1. For each of such triplets, we can construct a Horn clause where (i + , r + , j + ) is on the right side, and all KG facts involving all relations and entities are in the left. For each Horn clause with (i + , r + , j + ) on the right side, we add the node and relation permutation equivariance property by defining R = R (R) , V = V (N ) , and is also still a valid implication of the UQER Horn clauses based on the permuted KG φ • τ • A facts, by definition of the UQER Horn clauses (Definition 4.9). Thus, for all triplets predicted to be positive for any KG, there exists a set of UQER Horn clauses that imply the triplets given the same KG facts. N ) that predicted to be negative by the model, i.e., η(Γ tri ((i − , r − , j − ), A)) = 0. Suppose there exists a UQER Horn clause in the set of UQER Horn clauses constructed in the above paragraph that implies (i − , r − , j − ) in the right side, for some \n We then consider triplets Thus, there are no UQER Horn clauses that imply these triplets for all triplets predicted to be negative (i.e., not predicted as positive). Lemma 5.2. The triplet representation in Equation (  5 ) is an invariant triplet representation as per Definition 4.5. Proof. From our model architecture (Equation (  5 )), Γ ISDEA ((i, r, j), A) = (h Using DSS layers, we can guarantee the node representations h (T ) i,r we learn achive invariance under the node and relation permutations, where h It is also clear that distance function is invariant to node and relation permutations, i.e. ∀i, j ∈ V, d(i, j) in A is the same as d(φ \n Dataset Hits@10↑ w/ Distance w/o Distance WN18RR-v1 1.000 0.962 NELL995-v1 1.000 1.000  \n D.4. Expressivity Limitation of Doubly Exchangeable Representation In Figure  6 , we denote all four relations by numbers such that "father" (blue) is 0, "mother" (orange) is 1, "grand diff" (red) is 2 and "grand same" (green) is 3. We are going to show that IS-DEA is incapable to distinguish triplets of relation 2 and triplets of relation 3 in Figure  6 . We denote the node representation given by IS-DEA as H v,r where v ∈ [0, 6] and r ∈ [0, 3]. Given only the fact tripelts (relation 0 and 1, or color blue and orange), we can easily see that node 3 and node 6 are symmetric, and node 4 and node 5 are symmetric by simply flip the father and mother relation IDs. Thus, based on the invariance as Definition 4.5, we should get The representation of "grand diff" and "grand same" on each node is always the same because there is no facts involving these two relations, thus IS-DEA can not see their difference, thus can not distinguish them on representations. From the computation view as Equation (  4 ), the first function 1 always receive an empty graph, while the second function always receive the full unattributed graph (only facts) for relation "grand diff" and "grand same". The representation of four triplets to be queried will be We omit the shortest distances in the representation since they are all 2, thus has no influence when compare with each other. Blue means "Father"; orange means "Mother"; red means "Grand diff" that we have two different relations on the chain path; green means "Grand same" that we have two same relations on the chain path. Queries are all four "Grand diff" and "Grand same" triplets. Based on Equation (  7 ), we can further notice that = Γ tri ((0, 5, 2), A) = Γ tri ((0, 5, 2), A) = Γ tri ((0, 4, 3), A) Suppose the final MLP translating triplet representations into scores is f , and denote score s i,r,j = f (Γ tri (i, r, j), A), we will have s 0,3,3 = s 0,6,3 = s 0,6,2 = s 0,3,2 s 0,4,3 = s 0,5,3 = s 0,5,2 = s 0,4,2 Suppose our model can perform well on "Grand diff" (relation 2) completion, then it must ensure that negative cases has lower score than positive cases such that s 0,3,2 = s 0,6,2 < s 0,4,2 = s 0,5,2 . However, this also implies s 0,3,3 = s 0,6,3 < s 0,4,3 = s 0,5,3 , which shows that this model is performing poorly on "Grand same" (relation 3) completion, since it ranks node 4 and node 5 which is negative cases higher than node 3 and node 6 which is positive cases on "Grand same". We can see that in a case like Figure  6 , if IS-DEA performs perfect for one querying relation, it must perform poorly for the other relation, thus there is no way for IS-DEA to achieve perfect performance on such tasks which reflects its expressivity limitation. However, if we only want to perform transductive learning on such cases, a tensor factorization based can easily solve this task, thus this experssivity limitation can results in a failure for knowledge graph completion.  \n E. Future Work As addressed in the main paper, our implemented architecture (IS-DEA) has several limitations, which could be addressed in future work. First, IS-DEA has high pre-processing cost. This high time cost is introduced by using non-trivial shortest distance whose extraction is of the same complexity as enclosed subgraph. However, we show that non-trivial shortest distance is not fatal to our model in real-world tasks, thus it is possible that non-trivial shortest distance can be replaced by other heuristics that can be efficiently extracted. Second, IS-DEA has high training and inference costs, since it relies on repeating GNNs for each relation. Thus, complexity IS-DEA of scales linearly w.r.t. number of relations, which is often a large number in real-world knowledge base, e.g., Wikipedia. However, fully equivariance over all relations can be too strong, and we may only want partial equivariance (Definition 4.5, Quotient Group) which may reduce the cost. Third, IS-DEA has expressivity limitation. This limitation is related to former two cost issues since it is caused by compromising most-expressive pairwise representation to node-wise representation due to time cost. Thus if we can reduce the cost, we may be able to use more expressive graph encoder. Finally, although we show IS-DEA representations can be explained by UQER Horn clauses, there is no algorithm to create UQER Horn clauses from IS-DEA representations. This topic is known as "explainability" which is important in knowledge graph community. We leave such an algorithm as another future work other than optimization.')
